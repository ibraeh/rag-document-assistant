{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363e91c9",
   "metadata": {},
   "source": [
    "RAG Document Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9993ec4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrag-document-assistant/\\n├── backend/\\n│   ├── app/\\n│   │   ├── __init__.py\\n│   │   ├── main.py\\n│   │   ├── config.py\\n│   │   ├── models.py\\n│   │   ├── services/\\n│   │   │   ├── __init__.py\\n│   │   │   ├── document_processor.py\\n│   │   │   ├── embeddings.py\\n│   │   │   ├── vector_store.py\\n│   │   │   └── llm_service.py\\n│   │   └── utils/\\n│   │       ├── __init__.py\\n│   │       └── helpers.py\\n│   ├── tests/\\n│   │   ├── __init__.py\\n│   │   └── test_document_processor.py\\n│   ├── requirements.txt\\n│   ├── .env.example\\n│   └── Dockerfile\\n├── frontend/\\n│   ├── streamlit_app.py\\n│   ├── requirements.txt\\n│   └── .streamlit/\\n│       └── config.toml\\n├── data/\\n│   ├── uploads/\\n│   └── vector_db/\\n├── notebooks/\\n│   └── experimentation.ipynb\\n├── docker-compose.yml\\n├── .gitignore\\n├── README.md\\n└── setup.sh\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project Structure\n",
    "\"\"\"\n",
    "rag-document-assistant/\n",
    "├── backend/\n",
    "│   ├── app/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── main.py\n",
    "│   │   ├── config.py\n",
    "│   │   ├── models.py\n",
    "│   │   ├── services/\n",
    "│   │   │   ├── __init__.py\n",
    "│   │   │   ├── document_processor.py\n",
    "│   │   │   ├── embeddings.py\n",
    "│   │   │   ├── vector_store.py\n",
    "│   │   │   └── llm_service.py\n",
    "│   │   └── utils/\n",
    "│   │       ├── __init__.py\n",
    "│   │       └── helpers.py\n",
    "│   ├── tests/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── test_document_processor.py\n",
    "│   ├── requirements.txt\n",
    "│   ├── .env.example\n",
    "│   └── Dockerfile\n",
    "├── frontend/\n",
    "│   ├── streamlit_app.py\n",
    "│   ├── requirements.txt\n",
    "│   └── .streamlit/\n",
    "│       └── config.toml\n",
    "├── data/\n",
    "│   ├── uploads/\n",
    "│   └── vector_db/\n",
    "├── notebooks/\n",
    "│   └── experimentation.ipynb\n",
    "├── docker-compose.yml\n",
    "├── .gitignore\n",
    "├── README.md\n",
    "└── setup.sh\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fcba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: virtualenv in c:\\programdata\\anaconda3\\lib\\site-packages (20.35.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from virtualenv) (0.4.0)\n",
      "Requirement already satisfied: filelock<4,>=3.12.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from virtualenv) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.13.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from virtualenv) (4.15.0)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from virtualenv) (4.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install virtualenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4804e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file rag-document-assistant already exists.\n"
     ]
    }
   ],
   "source": [
    "mkdir rag-document-assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a68e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\n"
     ]
    }
   ],
   "source": [
    "cd rag-document-assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a3d829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PermissionError: [Errno 13] Permission denied: 'C:\\\\Users\\\\c2191409\\\\rag-document-assistant\\\\venv\\\\Scripts\\\\pythonw.exe'\n"
     ]
    }
   ],
   "source": [
    "!virtualenv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2885a58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p backend/app/utils backend/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68457f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create directories\n",
    "!mkdir frontend\\.streamlit data\\uploads data\\vector_db notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863780c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is BA1D-3E5A\n",
      "\n",
      " Directory of C:\\Users\\c2191409\\rag-document-assistant\n",
      "\n",
      "06/11/2025  22:58    <DIR>          .\n",
      "06/11/2025  22:58    <DIR>          ..\n",
      "07/11/2025  01:16    <DIR>          backend\n",
      "07/11/2025  01:23    <DIR>          data\n",
      "07/11/2025  01:23    <DIR>          frontend\n",
      "07/11/2025  01:23    <DIR>          notebooks\n",
      "07/11/2025  00:05    <DIR>          venv\n",
      "               0 File(s)              0 bytes\n",
      "               7 Dir(s)  1,242,238,541,824 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f03d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is BA1D-3E5A\n",
      "\n",
      " Directory of C:\\Users\\c2191409\\rag-document-assistant\\backend\\app\\services\n",
      "\n",
      "06/11/2025  22:58    <DIR>          .\n",
      "06/11/2025  22:58    <DIR>          ..\n",
      "07/11/2025  01:32                 0 __init__.py\n",
      "               1 File(s)              0 bytes\n",
      "               2 Dir(s)  1,242,236,903,424 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir backend\\app\\services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad973740",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type nul > backend/app/__init__.py \n",
    "!type nul >backend/app/services/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99c6b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!type nul > backend/app/utils/__init__.py \n",
    "!type nul > backend/tests/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9dd4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = \"\"\"fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "python-multipart==0.0.6\n",
    "python-dotenv==1.0.0\n",
    "pydantic==2.5.0\n",
    "pydantic-settings==2.1.0\n",
    "\n",
    "# LLM & Embeddings\n",
    "langchain==0.1.0\n",
    "langchain-community==0.0.10\n",
    "langchain-openai==0.0.2\n",
    "openai==1.6.1\n",
    "tiktoken==0.5.2\n",
    "\n",
    "# Vector Store\n",
    "chromadb==0.4.18\n",
    "sentence-transformers==2.2.2\n",
    "\n",
    "# Document Processing\n",
    "pypdf==3.17.4\n",
    "python-docx==1.1.0\n",
    "unstructured==0.11.6\n",
    "pillow==10.1.0\n",
    "\n",
    "# Utilities\n",
    "pandas==2.1.4\n",
    "numpy==1.26.2\n",
    "aiofiles==23.2.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"backend/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fc9c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastapi==0.104.1\n",
      "uvicorn[standard]==0.24.0\n",
      "python-multipart==0.0.6\n",
      "python-dotenv==1.0.0\n",
      "pydantic==2.5.0\n",
      "pydantic-settings==2.1.0\n",
      "\n",
      "# LLM & Embeddings\n",
      "langchain==0.1.0\n",
      "langchain-community==0.0.10\n",
      "langchain-openai==0.0.2\n",
      "openai==1.6.1\n",
      "tiktoken==0.5.2\n",
      "\n",
      "# Vector Store\n",
      "chromadb==0.4.18\n",
      "sentence-transformers==2.2.2\n",
      "\n",
      "# Document Processing\n",
      "pypdf==3.17.4\n",
      "python-docx==1.1.0\n",
      "unstructured==0.11.6\n",
      "pillow==10.1.0\n",
      "\n",
      "# Utilities\n",
      "pandas==2.1.4\n",
      "numpy==1.26.2\n",
      "aiofiles==23.2.1\n"
     ]
    }
   ],
   "source": [
    "!type backend\\requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70a46cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = \"\"\"streamlit==1.29.0\n",
    "requests==2.31.0\n",
    "python-dotenv==1.0.0\n",
    "plotly==5.18.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"frontend/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ce1a37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streamlit==1.29.0\n",
      "requests==2.31.0\n",
      "python-dotenv==1.0.0\n",
      "plotly==5.18.0\n"
     ]
    }
   ],
   "source": [
    "!type frontend\\requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80bc9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\\backend\n"
     ]
    }
   ],
   "source": [
    "cd backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f5ec2a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi==0.104.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\~il\\\\_imaging.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "     ---------------------------------------- 0.0/92.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 92.9/92.9 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting uvicorn[standard]==0.24.0\n",
      "  Downloading uvicorn-0.24.0-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 0.0/59.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.6/59.6 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting python-multipart==0.0.6\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.7/45.7 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting python-dotenv==1.0.0\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting pydantic==2.5.0\n",
      "  Downloading pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
      "     ---------------------------------------- 0.0/407.5 kB ? eta -:--:--\n",
      "     ------------------------ ------------- 266.2/407.5 kB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 407.5/407.5 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting pydantic-settings==2.1.0\n",
      "  Downloading pydantic_settings-2.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting langchain==0.1.0\n",
      "  Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "     ---------------------------------------- 0.0/798.0 kB ? eta -:--:--\n",
      "     -------------- ----------------------- 307.2/798.0 kB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 645.1/798.0 kB 8.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 798.0/798.0 kB 6.3 MB/s eta 0:00:00\n",
      "Collecting langchain-community==0.0.10\n",
      "  Downloading langchain_community-0.0.10-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 0.5/1.5 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 0.9/1.5 MB 9.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.5/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.8 MB/s eta 0:00:00\n",
      "Collecting langchain-openai==0.0.2\n",
      "  Downloading langchain_openai-0.0.2-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.6.1\n",
      "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
      "     ---------------------------------------- 0.0/225.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 225.4/225.4 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting tiktoken==0.5.2\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-win_amd64.whl (786 kB)\n",
      "     ---------------------------------------- 0.0/786.3 kB ? eta -:--:--\n",
      "     ----------------------------------- - 747.5/786.3 kB 15.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- 786.3/786.3 kB 12.5 MB/s eta 0:00:00\n",
      "Collecting chromadb==0.4.18\n",
      "  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
      "     ---------------------------------------- 0.0/502.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 502.4/502.4 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.0/86.0 kB 4.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pypdf==3.17.4\n",
      "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
      "     ---------------------------------------- 0.0/278.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 278.2/278.2 kB 8.6 MB/s eta 0:00:00\n",
      "Collecting python-docx==1.1.0\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "     ---------------------------------------- 0.0/239.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 239.6/239.6 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting unstructured==0.11.6\n",
      "  Downloading unstructured-0.11.6-py3-none-any.whl (1.8 MB)\n",
      "     ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "     -------------------------- ------------- 1.2/1.8 MB 24.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.8/1.8 MB 22.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.8/1.8 MB 16.0 MB/s eta 0:00:00\n",
      "Collecting pillow==10.1.0\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     --------------------- ------------------ 1.4/2.6 MB 29.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.6/2.6 MB 33.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 23.6 MB/s eta 0:00:00\n",
      "Collecting pandas==2.1.4\n",
      "  Downloading pandas-2.1.4-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "     ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 1.8/10.7 MB 38.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 3.6/10.7 MB 38.2 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 5.6/10.7 MB 39.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 7.9/10.7 MB 42.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.6/10.7 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.7/10.7 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.7/10.7 MB 36.4 MB/s eta 0:00:00\n",
      "Collecting numpy==1.26.2\n",
      "  Downloading numpy-1.26.2-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 2.3/15.8 MB 72.7 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.8/15.8 MB 51.1 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 7.4/15.8 MB 52.7 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 10.3/15.8 MB 54.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 13.0/15.8 MB 54.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.8/15.8 MB 54.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.8/15.8 MB 54.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.8/15.8 MB 38.6 MB/s eta 0:00:00\n",
      "Collecting aiofiles==23.2.1\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting anyio<4.0.0,>=3.7.1\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "     ---------------------------------------- 0.0/80.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 80.9/80.9 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi==0.104.1->-r requirements.txt (line 1)) (4.15.0)\n",
      "Collecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 0.0/67.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.0/67.0 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h11>=0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (8.0.4)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (6.0)\n",
      "Collecting httptools>=0.5.0\n",
      "  Downloading httptools-0.7.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "     ---------------------------------------- 0.0/86.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.2/86.2 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-15.0.1-cp310-cp310-win_amd64.whl (176 kB)\n",
      "     ---------------------------------------- 0.0/176.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 176.8/176.8 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting watchfiles>=0.13\n",
      "  Downloading watchfiles-1.1.1-cp310-cp310-win_amd64.whl (287 kB)\n",
      "     ---------------------------------------- 0.0/287.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 287.5/287.5 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting pydantic-core==2.14.1\n",
      "  Downloading pydantic_core-2.14.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.9/1.9 MB 60.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 30.3 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading sqlalchemy-2.0.44-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---------------------------------------  2.1/2.1 MB 68.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 34.2 MB/s eta 0:00:00\n",
      "Collecting langsmith<0.1.0,>=0.0.77\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.5/56.5 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain==0.1.0->-r requirements.txt (line 9)) (8.2.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain==0.1.0->-r requirements.txt (line 9)) (2.29.0)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7\n",
      "  Downloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
      "     ---------------------------------------- 0.0/303.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 303.1/303.1 kB 18.3 MB/s eta 0:00:00\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.13.2-cp310-cp310-win_amd64.whl (455 kB)\n",
      "     ---------------------------------------- 0.0/455.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 455.1/455.1 kB 14.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==1.6.1->-r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==1.6.1->-r requirements.txt (line 12)) (4.65.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==1.6.1->-r requirements.txt (line 12)) (0.23.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken==0.5.2->-r requirements.txt (line 13)) (2023.6.3)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from chromadb==0.4.18->-r requirements.txt (line 16)) (7.3.1)\n",
      "Collecting grpcio>=1.58.0\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-win_amd64.whl (4.7 MB)\n",
      "     ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "     ------------------- -------------------- 2.3/4.7 MB 70.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.7/4.7 MB 60.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.7/4.7 MB 42.7 MB/s eta 0:00:00\n",
      "Collecting chroma-hnswlib==0.7.3\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl (150 kB)\n",
      "     ---------------------------------------- 0.0/150.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 150.6/150.6 kB 8.8 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
      "Collecting tokenizers>=0.13.2\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------  2.7/2.7 MB 57.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.7/2.7 MB 34.2 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "     ---------------------------------------- 0.0/132.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 132.3/132.3 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.59b0-py3-none-any.whl (13 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 0.0/65.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 65.9/65.9 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting pypika>=0.48.9\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB 3.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting typer>=0.9.0\n",
      "  Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 0.0/47.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 47.0/47.0 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     ------------------------------------- -- 1.9/2.0 MB 60.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 32.2 MB/s eta 0:00:00\n",
      "Collecting posthog>=2.4.0\n",
      "  Downloading posthog-6.9.0-py3-none-any.whl (144 kB)\n",
      "     ---------------------------------------- 0.0/144.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 144.5/144.5 kB 8.4 MB/s eta 0:00:00\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting pulsar-client>=3.1.0\n",
      "  Downloading pulsar_client-3.8.0-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "     ---------------------- ----------------- 2.1/3.7 MB 67.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 2.7/3.7 MB 33.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 3.1/3.7 MB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.7/3.7 MB 21.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.7/3.7 MB 16.7 MB/s eta 0:00:00\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-win_amd64.whl (150 kB)\n",
      "     ---------------------------------------- 0.0/150.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 150.9/150.9 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.23.2-cp310-cp310-win_amd64.whl (13.5 MB)\n",
      "     ---------------------------------------- 0.0/13.5 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.3/13.5 MB 47.7 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.7/13.5 MB 49.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.6/13.5 MB 54.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.9/13.5 MB 53.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.8/13.5 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  13.2/13.5 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  13.2/13.5 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  13.3/13.5 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  13.4/13.5 MB 28.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  13.5/13.5 MB 28.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 13.5/13.5 MB 23.3 MB/s eta 0:00:00\n",
      "Collecting mmh3>=4.0.1\n",
      "  Downloading mmh3-5.2.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "     ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.2/12.0 MB 68.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.0/12.0 MB 63.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.5/12.0 MB 60.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 10.4/12.0 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.0/12.0 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.0/12.0 MB 43.7 MB/s eta 0:00:00\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-2.9.0-cp310-cp310-win_amd64.whl (109.3 MB)\n",
      "     ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "     - ------------------------------------- 2.9/109.3 MB 61.0 MB/s eta 0:00:02\n",
      "     -- ------------------------------------ 5.8/109.3 MB 61.8 MB/s eta 0:00:02\n",
      "     --- ----------------------------------- 8.6/109.3 MB 60.9 MB/s eta 0:00:02\n",
      "     --- ---------------------------------- 11.4/109.3 MB 65.6 MB/s eta 0:00:02\n",
      "     ---- --------------------------------- 14.1/109.3 MB 59.5 MB/s eta 0:00:02\n",
      "     ----- -------------------------------- 16.9/109.3 MB 59.5 MB/s eta 0:00:02\n",
      "     ------ ------------------------------- 19.7/109.3 MB 59.5 MB/s eta 0:00:02\n",
      "     ------- ------------------------------ 22.1/109.3 MB 54.7 MB/s eta 0:00:02\n",
      "     -------- ----------------------------- 24.6/109.3 MB 54.4 MB/s eta 0:00:02\n",
      "     --------- ---------------------------- 27.1/109.3 MB 54.4 MB/s eta 0:00:02\n",
      "     ---------- --------------------------- 29.9/109.3 MB 54.4 MB/s eta 0:00:02\n",
      "     ----------- -------------------------- 32.6/109.3 MB 54.4 MB/s eta 0:00:02\n",
      "     ------------ ------------------------- 35.3/109.3 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------- ------------------------ 37.8/109.3 MB 54.7 MB/s eta 0:00:02\n",
      "     -------------- ----------------------- 40.4/109.3 MB 54.7 MB/s eta 0:00:02\n",
      "     -------------- ----------------------- 43.1/109.3 MB 54.7 MB/s eta 0:00:02\n",
      "     --------------- ---------------------- 45.7/109.3 MB 54.4 MB/s eta 0:00:02\n",
      "     ---------------- --------------------- 48.4/109.3 MB 54.4 MB/s eta 0:00:02\n",
      "     ----------------- -------------------- 51.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 54.1/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------- ------------------ 56.8/109.3 MB 59.8 MB/s eta 0:00:01\n",
      "     -------------------- ----------------- 59.5/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------- ---------------- 62.0/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 64.5/109.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ----------------------- -------------- 67.0/109.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------ ------------- 69.5/109.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------ ------------- 71.9/109.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 74.3/109.3 MB 54.4 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 76.8/109.3 MB 54.7 MB/s eta 0:00:01\n",
      "     --------------------------- ---------- 79.4/109.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ---------------------------- --------- 82.0/109.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 84.5/109.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 87.2/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 89.7/109.3 MB 54.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 92.2/109.3 MB 54.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 94.9/109.3 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 97.1/109.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 99.6/109.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 102.3/109.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 105.2/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  108.1/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  109.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  109.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  109.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  109.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  109.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  109.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  109.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  109.3/109.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- 109.3/109.3 MB 15.6 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.0-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "     ----------------------- ---------------- 2.2/3.7 MB 45.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.7/3.7 MB 58.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.7/3.7 MB 39.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 17)) (1.1.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 17)) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 17)) (3.8.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.1/1.1 MB 32.6 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-1.1.2-py3-none-any.whl (514 kB)\n",
      "     ---------------------------------------- 0.0/515.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 515.0/515.0 kB 16.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx==1.1.0->-r requirements.txt (line 21)) (4.9.2)\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.14.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.5/1.5 MB 95.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 32.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (1.14.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (4.12.2)\n",
      "Collecting python-iso639\n",
      "  Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "     ---------------------------------------- 0.0/167.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 167.6/167.6 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting filetype\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "     ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 608.4/608.4 kB 19.3 MB/s eta 0:00:00\n",
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 981.5/981.5 kB 21.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting python-magic\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting unstructured-client\n",
      "  Downloading unstructured_client-0.42.3-py3-none-any.whl (207 kB)\n",
      "     ---------------------------------------- 0.0/207.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 207.8/207.8 kB 13.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: chardet in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (4.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.1.4->-r requirements.txt (line 26)) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.1.4->-r requirements.txt (line 26)) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.1.4->-r requirements.txt (line 26)) (2.8.2)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.8.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.4.1-cp310-cp310-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 0.0/41.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.6/41.6 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.7.0-cp310-cp310-win_amd64.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r requirements.txt (line 9)) (22.1.0)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.22.0-cp310-cp310-win_amd64.whl (86 kB)\n",
      "     ---------------------------------------- 0.0/86.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.9/86.9 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: exceptiongroup in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.1->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.1->-r requirements.txt (line 1)) (3.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.6.1->-r requirements.txt (line 12)) (2023.5.7)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.6.1->-r requirements.txt (line 12)) (1.4.0)\n",
      "Requirement already satisfied: httpcore<0.16.0,>=0.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.6.1->-r requirements.txt (line 12)) (0.15.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "     ---------------------------------------- 0.0/201.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 201.0/201.0 kB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (23.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "     ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "     ------------------------------ --------- 2.2/2.9 MB 46.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.9/2.9 MB 36.7 MB/s eta 0:00:00\n",
      "Collecting shellingham\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting typer-slim\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 47.1/47.1 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.0->-r requirements.txt (line 9)) (2.1)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "     ---------------------------------------- 0.0/223.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 223.1/223.1 kB 13.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18->-r requirements.txt (line 16)) (1.16.0)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18->-r requirements.txt (line 16)) (1.26.15)\n",
      "Collecting durationpy>=0.7\n",
      "  Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18->-r requirements.txt (line 16)) (1.5.2)\n",
      "Collecting langchain-core<0.2,>=0.1.7\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "     ---------------------------------------- 0.0/302.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 302.9/302.9 kB 9.4 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.51-py3-none-any.whl (302 kB)\n",
      "     ---------------------------------------- 0.0/302.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 302.9/302.9 kB 9.4 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
      "     ---------------------------------------- 0.0/302.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 302.8/302.8 kB 18.3 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.49-py3-none-any.whl (303 kB)\n",
      "     ---------------------------------------- 0.0/303.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 303.0/303.0 kB 9.4 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.48-py3-none-any.whl (302 kB)\n",
      "     ---------------------------------------- 0.0/302.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 302.9/302.9 kB 18.3 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.47-py3-none-any.whl (302 kB)\n",
      "     ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 302.2/302.2 kB 9.1 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
      "     ---------------------------------------- 0.0/299.3 kB ? eta -:--:--\n",
      "     ------------------------------------- 299.3/299.3 kB 19.3 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "     ---------------------------------------- 0.0/291.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 291.3/291.3 kB 9.1 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.44-py3-none-any.whl (290 kB)\n",
      "     ---------------------------------------- 0.0/290.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 290.2/290.2 kB 17.5 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.43-py3-none-any.whl (289 kB)\n",
      "     ---------------------------------------- 0.0/289.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 289.1/289.1 kB 17.4 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.42-py3-none-any.whl (287 kB)\n",
      "     ---------------------------------------- 0.0/287.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 287.5/287.5 kB 8.9 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.41-py3-none-any.whl (278 kB)\n",
      "     ---------------------------------------- 0.0/278.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 278.4/278.4 kB 16.8 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
      "     ---------------------------------------- 0.0/276.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 276.8/276.8 kB 8.3 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.39-py3-none-any.whl (276 kB)\n",
      "     ---------------------------------------- 0.0/276.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 276.6/276.6 kB 17.8 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.38-py3-none-any.whl (279 kB)\n",
      "     ---------------------------------------- 0.0/279.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 279.2/279.2 kB 17.9 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.37-py3-none-any.whl (274 kB)\n",
      "     ---------------------------------------- 0.0/274.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 274.6/274.6 kB 16.5 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.36-py3-none-any.whl (273 kB)\n",
      "     ---------------------------------------- 0.0/273.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 273.9/273.9 kB 16.5 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.35-py3-none-any.whl (273 kB)\n",
      "     ---------------------------------------- 0.0/273.0 kB ? eta -:--:--\n",
      "     -------------------------------------  266.2/273.0 kB 8.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 273.0/273.0 kB 3.3 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.34-py3-none-any.whl (271 kB)\n",
      "     ---------------------------------------- 0.0/271.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 271.6/271.6 kB 16.3 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n",
      "     ---------------------------------------- 0.0/269.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 269.1/269.1 kB 8.4 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
      "     ---------------------------------------- 0.0/260.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 260.9/260.9 kB 15.7 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.31-py3-none-any.whl (258 kB)\n",
      "     ---------------------------------------- 0.0/258.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 258.8/258.8 kB 7.8 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
      "     ---------------------------------------- 0.0/256.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 256.9/256.9 kB 16.4 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.29-py3-none-any.whl (252 kB)\n",
      "     ---------------------------------------- 0.0/252.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 252.6/252.6 kB 15.1 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.28-py3-none-any.whl (252 kB)\n",
      "     ---------------------------------------- 0.0/252.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 252.4/252.4 kB 15.1 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.27-py3-none-any.whl (250 kB)\n",
      "     ---------------------------------------- 0.0/250.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 250.8/250.8 kB 7.8 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
      "     ---------------------------------------- 0.0/246.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 246.4/246.4 kB 15.7 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.25-py3-none-any.whl (242 kB)\n",
      "     ---------------------------------------- 0.0/242.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 242.1/242.1 kB 14.5 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.24-py3-none-any.whl (241 kB)\n",
      "     ---------------------------------------- 0.0/241.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 241.3/241.3 kB 7.2 MB/s eta 0:00:00\n",
      "  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "     ---------------------------------------- 0.0/241.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 241.2/241.2 kB 15.4 MB/s eta 0:00:00\n",
      "Collecting langsmith<0.1.0,>=0.0.77\n",
      "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.4/55.4 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.9\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 0.0/53.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.0/53.0 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.33.0-cp310-abi3-win_amd64.whl (436 kB)\n",
      "     ---------------------------------------- 0.0/436.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 436.9/436.9 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "     ------------- -------------------------- 2.1/6.3 MB 67.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 4.9/6.3 MB 63.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.3/6.3 MB 57.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.3/6.3 MB 36.6 MB/s eta 0:00:00\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.18->-r requirements.txt (line 16)) (6.0.0)\n",
      "Collecting opentelemetry-proto==1.38.0\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "     ---------------------------------------- 0.0/72.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 72.5/72.5 kB 3.9 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos~=1.57\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "     ---------------------------------------- 0.0/297.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 297.5/297.5 kB 18.0 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "     ---------------------------------------- 0.0/208.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 208.0/208.0 kB 12.4 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation==0.59b0\n",
      "  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl (33 kB)\n",
      "Collecting opentelemetry-util-http==0.59b0\n",
      "  Downloading opentelemetry_util_http-0.59b0-py3-none-any.whl (7.6 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.59b0\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.59b0-py3-none-any.whl (16 kB)\n",
      "Collecting asgiref~=3.0\n",
      "  Downloading asgiref-3.10.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.0->-r requirements.txt (line 9)) (2.0.4)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.4-cp310-cp310-win_amd64.whl (298 kB)\n",
      "     ---------------------------------------- 0.0/298.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 298.7/298.7 kB 18.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (3.1.2)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "     ---------------------------------------- 0.0/320.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 320.2/320.2 kB 19.4 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "     ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 566.1/566.1 kB 17.9 MB/s eta 0:00:00\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "     ---------------------------------------- 0.0/243.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 243.4/243.4 kB 15.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->unstructured==0.11.6->-r requirements.txt (line 22)) (2.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (2.2.0)\n",
      "Collecting unstructured-client\n",
      "  Downloading unstructured_client-0.42.2-py3-none-any.whl (207 kB)\n",
      "     ---------------------------------------- 0.0/207.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 207.6/207.6 kB 13.2 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.42.1-py3-none-any.whl (207 kB)\n",
      "     ---------------------------------------- 0.0/207.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 207.2/207.2 kB 13.1 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.42.0-py3-none-any.whl (207 kB)\n",
      "     ---------------------------------------- 0.0/207.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 207.1/207.1 kB 6.4 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.41.0-py3-none-any.whl (211 kB)\n",
      "     ---------------------------------------- 0.0/211.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 211.0/211.0 kB 6.3 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.40.0-py3-none-any.whl (212 kB)\n",
      "     ---------------------------------------- 0.0/212.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 212.5/212.5 kB 12.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.39.1-py3-none-any.whl (212 kB)\n",
      "     ---------------------------------------- 0.0/212.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 212.6/212.6 kB 6.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.38.1-py3-none-any.whl (212 kB)\n",
      "     ---------------------------------------- 0.0/212.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 212.6/212.6 kB 12.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.37.4-py3-none-any.whl (211 kB)\n",
      "     ---------------------------------------- 0.0/211.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 211.2/211.2 kB 6.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.37.2-py3-none-any.whl (210 kB)\n",
      "     ---------------------------------------- 0.0/210.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 210.7/210.7 kB 13.4 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.37.1-py3-none-any.whl (210 kB)\n",
      "     ---------------------------------------- 0.0/210.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 210.4/210.4 kB 12.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.36.0-py3-none-any.whl (195 kB)\n",
      "     ---------------------------------------- 0.0/195.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 195.8/195.8 kB 11.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.35.0-py3-none-any.whl (192 kB)\n",
      "     ---------------------------------------- 0.0/192.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 192.1/192.1 kB 11.4 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.34.0-py3-none-any.whl (189 kB)\n",
      "     ---------------------------------------- 0.0/189.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 189.4/189.4 kB 11.2 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.33.1-py3-none-any.whl (189 kB)\n",
      "     ---------------------------------------- 0.0/189.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 189.2/189.2 kB 5.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.33.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.4/181.4 kB 11.4 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.32.4-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.5/181.5 kB 11.4 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.32.3-py3-none-any.whl (180 kB)\n",
      "     ---------------------------------------- 0.0/180.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 180.6/180.6 kB 10.6 MB/s eta 0:00:00\n",
      "Collecting eval-type-backport>=0.2.0\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting requests-toolbelt>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 54.5/54.5 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting unstructured-client\n",
      "  Downloading unstructured_client-0.32.2-py3-none-any.whl (180 kB)\n",
      "     ---------------------------------------- 0.0/180.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 180.6/180.6 kB 10.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.32.1-py3-none-any.whl (180 kB)\n",
      "     ---------------------------------------- 0.0/180.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 180.5/180.5 kB 10.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.32.0-py3-none-any.whl (178 kB)\n",
      "     ---------------------------------------- 0.0/178.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 178.2/178.2 kB 10.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.31.6-py3-none-any.whl (177 kB)\n",
      "     ---------------------------------------- 0.0/177.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 177.5/177.5 kB 10.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.31.5-py3-none-any.whl (177 kB)\n",
      "     ---------------------------------------- 0.0/177.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 177.5/177.5 kB 10.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.31.4-py3-none-any.whl (177 kB)\n",
      "     ---------------------------------------- 0.0/177.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 177.5/177.5 kB 10.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.31.3-py3-none-any.whl (175 kB)\n",
      "     ---------------------------------------- 0.0/175.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 175.8/175.8 kB 10.3 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.31.2-py3-none-any.whl (172 kB)\n",
      "     ---------------------------------------- 0.0/172.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 172.6/172.6 kB 10.8 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.31.1-py3-none-any.whl (166 kB)\n",
      "     ---------------------------------------- 0.0/166.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 166.8/166.8 kB 10.4 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.31.0-py3-none-any.whl (166 kB)\n",
      "     ---------------------------------------- 0.0/166.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 166.5/166.5 kB 9.8 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.30.6-py3-none-any.whl (166 kB)\n",
      "     ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 166.4/166.4 kB 9.8 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.30.5-py3-none-any.whl (165 kB)\n",
      "     ---------------------------------------- 0.0/165.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 165.8/165.8 kB 9.7 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.30.4-py3-none-any.whl (164 kB)\n",
      "     ---------------------------------------- 0.0/164.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 164.8/164.8 kB 9.7 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.30.3-py3-none-any.whl (113 kB)\n",
      "     ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 113.6/113.6 kB 6.9 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.30.2-py3-none-any.whl (113 kB)\n",
      "     ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 113.6/113.6 kB 6.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.30.1-py3-none-any.whl (112 kB)\n",
      "     ---------------------------------------- 0.0/112.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 112.5/112.5 kB 6.8 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.30.0-py3-none-any.whl (112 kB)\n",
      "     ---------------------------------------- 0.0/112.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 112.5/112.5 kB 3.3 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.29.0-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 0.0/63.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 63.6/63.6 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Collecting unstructured-client\n",
      "  Downloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 0.0/62.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.9/62.9 kB 3.5 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.28.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.6/62.6 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.27.0-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 0.0/59.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.9/59.9 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.26.2-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 0.0/60.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.0/60.0 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.26.1-py3-none-any.whl (60 kB)\n",
      "     ---------------------------------------- 0.0/60.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.2/60.2 kB 3.1 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.26.0-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 0.0/59.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.7/59.7 kB 3.3 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.9-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.3/45.3 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting urllib3<2.4.0,>=1.24.2\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "     ---------------------------------------- 0.0/144.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 144.2/144.2 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting deepdiff>=6.0\n",
      "  Downloading deepdiff-8.6.1-py3-none-any.whl (91 kB)\n",
      "     ---------------------------------------- 0.0/91.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 91.4/91.4 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting certifi\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "     ---------------------------------------- 0.0/163.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 163.3/163.3 kB 9.6 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl (107 kB)\n",
      "     ---------------------------------------- 0.0/107.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 107.2/107.2 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "     ---------------------------------------- 0.0/73.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 73.5/73.5 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting unstructured-client\n",
      "  Downloading unstructured_client-0.25.8-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.3/45.3 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.7-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.2/45.2 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.6-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.1/45.1 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.5-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 0.0/43.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.9/43.9 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.4-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 2.2 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.3-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 1.1 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.2-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 0.0/43.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.7/43.7 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.1-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 0.0/43.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.5/43.5 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.25.0-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 0.0/42.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.4/42.4 kB 2.0 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.24.1-py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 0.0/41.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.9/41.9 kB 2.1 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.24.0-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/41.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.0/41.0 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.23.9-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/41.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.0/41.0 kB 2.0 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.23.8-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/41.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.0/41.0 kB 2.0 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.23.7-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/41.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.0/41.0 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.23.5-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/41.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.0/41.0 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.23.3-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/40.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.3/40.3 kB 1.9 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.23.2-py3-none-any.whl (39 kB)\n",
      "  Downloading unstructured_client-0.23.1-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/40.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.1/40.1 kB 2.0 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.23.0-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/40.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 40.1/40.1 kB 2.0 MB/s eta 0:00:00\n",
      "  Downloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
      "Collecting requests<3,>=2\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 0.0/64.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.7/64.7 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting unstructured-client\n",
      "  Downloading unstructured_client-0.21.1-py3-none-any.whl (28 kB)\n",
      "  Downloading unstructured_client-0.21.0-py3-none-any.whl (24 kB)\n",
      "Collecting mypy-extensions>=1.0.0\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.3 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.3/181.3 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0\n",
      "  Downloading cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.18->-r requirements.txt (line 16)) (3.11.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18->-r requirements.txt (line 16)) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18->-r requirements.txt (line 16)) (2.2.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 536.2/536.2 kB 17.0 MB/s eta 0:00:00\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.8/86.8 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (2.1.1)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "     ---------------------------------------- 0.0/160.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 160.1/160.1 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 0.0/83.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 83.2/83.2 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18->-r requirements.txt (line 16)) (0.1.2)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 0.0/83.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 83.1/83.1 kB 4.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sentence-transformers, pypika, langdetect\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125960 sha256=14d1498b30f24da65e85e5d05e483bf5b352ec003fe75781d2fd16c7d208a41f\n",
      "  Stored in directory: c:\\users\\c2191409\\appdata\\local\\pip\\cache\\wheels\\62\\f2\\10\\1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=22b4244dfc557b5abe65008ca02ac9f9072145676e7ed9a62a25edf3d11bde6f\n",
      "  Stored in directory: c:\\users\\c2191409\\appdata\\local\\pip\\cache\\wheels\\e1\\26\\51\\d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=196e2deac80680352655058c374557a964025a33448425cf563b4d7bc2f5e3e8\n",
      "  Stored in directory: c:\\users\\c2191409\\appdata\\local\\pip\\cache\\wheels\\95\\03\\7d\\59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "Successfully built sentence-transformers pypika langdetect\n",
      "Installing collected packages: pypika, mpmath, flatbuffers, filetype, durationpy, websockets, urllib3, tenacity, tabulate, sympy, shellingham, sentencepiece, safetensors, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, python-docx, pyreadline3, pypdf, pydantic-core, pyasn1, protobuf, propcache, pillow, packaging, opentelemetry-util-http, oauthlib, numpy, mypy-extensions, multidict, mmh3, langdetect, jsonpath-python, jsonpatch, importlib-resources, httptools, grpcio, greenlet, fsspec, frozenlist, emoji, distro, charset-normalizer, certifi, cachetools, bcrypt, backoff, async-timeout, asgiref, anyio, annotated-types, aiohappyeyeballs, aiofiles, yarl, watchfiles, uvicorn, typing-inspect, torch, starlette, SQLAlchemy, rsa, rich, requests, pydantic, pyasn1-modules, pulsar-client, pandas, opentelemetry-proto, opentelemetry-api, marshmallow, humanfriendly, googleapis-common-protos, chroma-hnswlib, aiosignal, typer, torchvision, tiktoken, requests-oauthlib, pydantic-settings, posthog, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, langsmith, huggingface-hub, google-auth, fastapi, dataclasses-json, coloredlogs, aiohttp, unstructured-client, tokenizers, opentelemetry-sdk, opentelemetry-instrumentation, openai, onnxruntime, langchain-core, kubernetes, unstructured, transformers, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-openai, langchain-community, sentence-transformers, opentelemetry-instrumentation-fastapi, langchain, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.4.0\n",
      "    Uninstalling Pillow-9.4.0:\n",
      "      Successfully uninstalled Pillow-9.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7667defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\n",
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is BA1D-3E5A\n",
      "\n",
      " Directory of C:\\Users\\c2191409\\rag-document-assistant\n",
      "\n",
      "06/11/2025  22:58    <DIR>          .\n",
      "06/11/2025  22:58    <DIR>          ..\n",
      "07/11/2025  01:23    <DIR>          notebooks\n",
      "07/11/2025  01:23    <DIR>          data\n",
      "07/11/2025  01:45    <DIR>          frontend\n",
      "07/11/2025  01:42    <DIR>          backend\n",
      "07/11/2025  00:05    <DIR>          venv\n",
      "               0 File(s)              0 bytes\n",
      "               7 Dir(s)  1,242,226,335,744 bytes free\n"
     ]
    }
   ],
   "source": [
    "!chdir\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43d76b24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi==0.104.1\n",
      "  Using cached fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "Collecting uvicorn[standard]==0.24.0\n",
      "  Using cached uvicorn-0.24.0-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: python-multipart==0.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (0.0.6)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.0)\n",
      "Collecting pydantic==2.5.0\n",
      "  Using cached pydantic-2.5.0-py3-none-any.whl (407 kB)\n",
      "Collecting pydantic-settings==2.1.0\n",
      "  Using cached pydantic_settings-2.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting langchain==0.1.0\n",
      "  Using cached langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "Collecting langchain-community==0.0.10\n",
      "  Using cached langchain_community-0.0.10-py3-none-any.whl (1.5 MB)\n",
      "Collecting langchain-openai==0.0.2\n",
      "  Using cached langchain_openai-0.0.2-py3-none-any.whl (28 kB)\n",
      "Collecting openai==1.6.1\n",
      "  Using cached openai-1.6.1-py3-none-any.whl (225 kB)\n",
      "Collecting tiktoken==0.5.2\n",
      "  Using cached tiktoken-0.5.2-cp310-cp310-win_amd64.whl (786 kB)\n",
      "Collecting chromadb==0.4.18\n",
      "  Using cached chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: pypdf==3.17.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 20)) (3.17.4)\n",
      "Requirement already satisfied: python-docx==1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 21)) (1.1.0)\n",
      "Collecting unstructured==0.11.6\n",
      "  Using cached unstructured-0.11.6-py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: pillow==10.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 23)) (10.1.0)\n",
      "Collecting pandas==2.1.4\n",
      "  Using cached pandas-2.1.4-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "Collecting numpy==1.26.2\n",
      "  Using cached numpy-1.26.2-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Collecting aiofiles==23.2.1\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting anyio<4.0.0,>=3.7.1\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi==0.104.1->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (8.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (15.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]==0.24.0->-r requirements.txt (line 2)) (6.0)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.1-cp310-cp310-win_amd64.whl (287 kB)\n",
      "Collecting httptools>=0.5.0\n",
      "  Using cached httptools-0.7.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pydantic-core==2.14.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic==2.5.0->-r requirements.txt (line 5)) (2.14.1)\n",
      "Collecting langsmith<0.1.0,>=0.0.77\n",
      "  Using cached langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain==0.1.0->-r requirements.txt (line 9)) (8.5.0)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.13.2-cp310-cp310-win_amd64.whl (455 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7\n",
      "  Using cached langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain==0.1.0->-r requirements.txt (line 9)) (2.29.0)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached sqlalchemy-2.0.44-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==1.6.1->-r requirements.txt (line 12)) (4.65.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==1.6.1->-r requirements.txt (line 12)) (0.23.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==1.6.1->-r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken==0.5.2->-r requirements.txt (line 13)) (2023.6.3)\n",
      "Collecting tokenizers>=0.13.2\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Collecting typer>=0.9.0\n",
      "  Using cached typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Using cached mmh3-5.2.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from chromadb==0.4.18->-r requirements.txt (line 16)) (7.3.1)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Using cached onnxruntime-1.23.2-cp310-cp310-win_amd64.whl (13.5 MB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Using cached posthog-6.9.0-py3-none-any.whl (144 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Using cached opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from chromadb==0.4.18->-r requirements.txt (line 16)) (0.48.9)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Using cached bcrypt-5.0.0-cp39-abi3-win_amd64.whl (150 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
      "Collecting pulsar-client>=3.1.0\n",
      "  Using cached pulsar_client-3.8.0-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.59b0-py3-none-any.whl (13 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Using cached kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "Collecting chroma-hnswlib==0.7.3\n",
      "  Using cached chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl (150 kB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting grpcio>=1.58.0\n",
      "  Using cached grpcio-1.76.0-cp310-cp310-win_amd64.whl (4.7 MB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 17)) (1.10.1)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-2.9.0-cp310-cp310-win_amd64.whl (109.3 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.24.0-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 17)) (1.1.3)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-1.1.2-py3-none-any.whl (514 kB)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 17)) (0.2.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 17)) (3.8.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx==1.1.0->-r requirements.txt (line 21)) (4.9.2)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (1.14.1)\n",
      "Requirement already satisfied: rapidfuzz in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (3.14.3)\n",
      "Collecting emoji\n",
      "  Using cached emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "Requirement already satisfied: python-iso639 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (2025.2.18)\n",
      "Requirement already satisfied: chardet in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (4.0.0)\n",
      "Requirement already satisfied: filetype in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (1.2.0)\n",
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (0.9.0)\n",
      "Collecting unstructured-client\n",
      "  Using cached unstructured_client-0.42.3-py3-none-any.whl (207 kB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: python-magic in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (0.4.27)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured==0.11.6->-r requirements.txt (line 22)) (4.12.2)\n",
      "Collecting backoff\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.1.4->-r requirements.txt (line 26)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.1.4->-r requirements.txt (line 26)) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.1.4->-r requirements.txt (line 26)) (2023.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r requirements.txt (line 9)) (0.4.1)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.7.0-cp310-cp310-win_amd64.whl (46 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0->-r requirements.txt (line 9)) (22.1.0)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.22.0-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.1->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.1->-r requirements.txt (line 1)) (1.1.1)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.6.1->-r requirements.txt (line 12)) (2023.5.7)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.6.1->-r requirements.txt (line 12)) (1.4.0)\n",
      "Requirement already satisfied: httpcore<0.16.0,>=0.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.6.1->-r requirements.txt (line 12)) (0.15.0)\n",
      "Requirement already satisfied: shellingham in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (1.5.4)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "Collecting typer-slim\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (23.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (3.20.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.0->-r requirements.txt (line 9)) (2.1)\n",
      "Collecting requests-oauthlib\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Using cached google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18->-r requirements.txt (line 16)) (1.26.20)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18->-r requirements.txt (line 16)) (0.10)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18->-r requirements.txt (line 16)) (1.5.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.18->-r requirements.txt (line 16)) (1.16.0)\n",
      "Collecting packaging>=20.9\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "  Using cached langchain_core-0.1.51-py3-none-any.whl (302 kB)\n",
      "  Using cached langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
      "  Using cached langchain_core-0.1.49-py3-none-any.whl (303 kB)\n",
      "  Using cached langchain_core-0.1.48-py3-none-any.whl (302 kB)\n",
      "  Using cached langchain_core-0.1.47-py3-none-any.whl (302 kB)\n",
      "  Using cached langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
      "  Using cached langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "  Using cached langchain_core-0.1.44-py3-none-any.whl (290 kB)\n",
      "  Using cached langchain_core-0.1.43-py3-none-any.whl (289 kB)\n",
      "  Using cached langchain_core-0.1.42-py3-none-any.whl (287 kB)\n",
      "  Using cached langchain_core-0.1.41-py3-none-any.whl (278 kB)\n",
      "  Using cached langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
      "  Using cached langchain_core-0.1.39-py3-none-any.whl (276 kB)\n",
      "  Using cached langchain_core-0.1.38-py3-none-any.whl (279 kB)\n",
      "  Using cached langchain_core-0.1.37-py3-none-any.whl (274 kB)\n",
      "  Using cached langchain_core-0.1.36-py3-none-any.whl (273 kB)\n",
      "  Using cached langchain_core-0.1.35-py3-none-any.whl (273 kB)\n",
      "  Using cached langchain_core-0.1.34-py3-none-any.whl (271 kB)\n",
      "  Using cached langchain_core-0.1.33-py3-none-any.whl (269 kB)\n",
      "  Using cached langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
      "  Using cached langchain_core-0.1.31-py3-none-any.whl (258 kB)\n",
      "  Using cached langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
      "  Using cached langchain_core-0.1.29-py3-none-any.whl (252 kB)\n",
      "  Using cached langchain_core-0.1.28-py3-none-any.whl (252 kB)\n",
      "  Using cached langchain_core-0.1.27-py3-none-any.whl (250 kB)\n",
      "  Using cached langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
      "  Using cached langchain_core-0.1.25-py3-none-any.whl (242 kB)\n",
      "  Using cached langchain_core-0.1.24-py3-none-any.whl (241 kB)\n",
      "  Using cached langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77\n",
      "  Using cached langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18->-r requirements.txt (line 16)) (1.14.0)\n",
      "Requirement already satisfied: protobuf in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18->-r requirements.txt (line 16)) (6.33.0)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.18->-r requirements.txt (line 16)) (25.9.23)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.18->-r requirements.txt (line 16)) (6.0.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.38.0\n",
      "  Using cached opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "Collecting googleapis-common-protos~=1.57\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.59b0\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.59b0-py3-none-any.whl (16 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Collecting opentelemetry-instrumentation==0.59b0\n",
      "  Using cached opentelemetry_instrumentation-0.59b0-py3-none-any.whl (33 kB)\n",
      "Collecting opentelemetry-util-http==0.59b0\n",
      "  Using cached opentelemetry_util_http-0.59b0-py3-none-any.whl (7.6 kB)\n",
      "Collecting asgiref~=3.0\n",
      "  Using cached asgiref-3.10.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.0->-r requirements.txt (line 9)) (2.0.4)\n",
      "Collecting greenlet>=1\n",
      "  Using cached greenlet-3.2.4-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (3.1.2)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (2.8.4)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (0.6.2)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->unstructured==0.11.6->-r requirements.txt (line 22)) (2.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (2.2.0)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured==0.11.6->-r requirements.txt (line 22)) (39.0.1)\n",
      "Collecting unstructured-client\n",
      "  Using cached unstructured_client-0.42.2-py3-none-any.whl (207 kB)\n",
      "  Using cached unstructured_client-0.42.1-py3-none-any.whl (207 kB)\n",
      "  Using cached unstructured_client-0.42.0-py3-none-any.whl (207 kB)\n",
      "  Using cached unstructured_client-0.41.0-py3-none-any.whl (211 kB)\n",
      "Collecting nest-asyncio>=1.6.0\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting unstructured-client\n",
      "  Using cached unstructured_client-0.40.0-py3-none-any.whl (212 kB)\n",
      "  Using cached unstructured_client-0.39.1-py3-none-any.whl (212 kB)\n",
      "  Using cached unstructured_client-0.38.1-py3-none-any.whl (212 kB)\n",
      "  Using cached unstructured_client-0.37.4-py3-none-any.whl (211 kB)\n",
      "  Using cached unstructured_client-0.37.2-py3-none-any.whl (210 kB)\n",
      "  Using cached unstructured_client-0.37.1-py3-none-any.whl (210 kB)\n",
      "  Using cached unstructured_client-0.36.0-py3-none-any.whl (195 kB)\n",
      "  Using cached unstructured_client-0.35.0-py3-none-any.whl (192 kB)\n",
      "  Using cached unstructured_client-0.34.0-py3-none-any.whl (189 kB)\n",
      "  Using cached unstructured_client-0.33.1-py3-none-any.whl (189 kB)\n",
      "  Using cached unstructured_client-0.33.0-py3-none-any.whl (181 kB)\n",
      "  Using cached unstructured_client-0.32.4-py3-none-any.whl (181 kB)\n",
      "  Using cached unstructured_client-0.32.3-py3-none-any.whl (180 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Collecting eval-type-backport>=0.2.0\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting requests-toolbelt>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Collecting unstructured-client\n",
      "  Using cached unstructured_client-0.32.2-py3-none-any.whl (180 kB)\n",
      "  Using cached unstructured_client-0.32.1-py3-none-any.whl (180 kB)\n",
      "  Using cached unstructured_client-0.32.0-py3-none-any.whl (178 kB)\n",
      "  Using cached unstructured_client-0.31.6-py3-none-any.whl (177 kB)\n",
      "  Using cached unstructured_client-0.31.5-py3-none-any.whl (177 kB)\n",
      "  Using cached unstructured_client-0.31.4-py3-none-any.whl (177 kB)\n",
      "  Using cached unstructured_client-0.31.3-py3-none-any.whl (175 kB)\n",
      "  Using cached unstructured_client-0.31.2-py3-none-any.whl (172 kB)\n",
      "  Using cached unstructured_client-0.31.1-py3-none-any.whl (166 kB)\n",
      "  Using cached unstructured_client-0.31.0-py3-none-any.whl (166 kB)\n",
      "  Using cached unstructured_client-0.30.6-py3-none-any.whl (166 kB)\n",
      "  Using cached unstructured_client-0.30.5-py3-none-any.whl (165 kB)\n",
      "  Using cached unstructured_client-0.30.4-py3-none-any.whl (164 kB)\n",
      "  Using cached unstructured_client-0.30.3-py3-none-any.whl (113 kB)\n",
      "  Using cached unstructured_client-0.30.2-py3-none-any.whl (113 kB)\n",
      "  Using cached unstructured_client-0.30.1-py3-none-any.whl (112 kB)\n",
      "  Using cached unstructured_client-0.30.0-py3-none-any.whl (112 kB)\n",
      "  Using cached unstructured_client-0.29.0-py3-none-any.whl (63 kB)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6\n",
      "  Using cached jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Collecting unstructured-client\n",
      "  Using cached unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
      "  Using cached unstructured_client-0.28.0-py3-none-any.whl (62 kB)\n",
      "  Using cached unstructured_client-0.27.0-py3-none-any.whl (59 kB)\n",
      "  Using cached unstructured_client-0.26.2-py3-none-any.whl (59 kB)\n",
      "  Using cached unstructured_client-0.26.1-py3-none-any.whl (60 kB)\n",
      "  Using cached unstructured_client-0.26.0-py3-none-any.whl (59 kB)\n",
      "  Using cached unstructured_client-0.25.9-py3-none-any.whl (45 kB)\n",
      "Collecting requests<3,>=2\n",
      "  Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Collecting unstructured-client\n",
      "  Using cached unstructured_client-0.25.8-py3-none-any.whl (45 kB)\n",
      "  Using cached unstructured_client-0.25.7-py3-none-any.whl (45 kB)\n",
      "  Using cached unstructured_client-0.25.6-py3-none-any.whl (45 kB)\n",
      "  Using cached unstructured_client-0.25.5-py3-none-any.whl (43 kB)\n",
      "  Using cached unstructured_client-0.25.4-py3-none-any.whl (43 kB)\n",
      "  Using cached unstructured_client-0.25.3-py3-none-any.whl (43 kB)\n",
      "  Using cached unstructured_client-0.25.2-py3-none-any.whl (43 kB)\n",
      "  Using cached unstructured_client-0.25.1-py3-none-any.whl (43 kB)\n",
      "  Using cached unstructured_client-0.25.0-py3-none-any.whl (42 kB)\n",
      "  Using cached unstructured_client-0.24.1-py3-none-any.whl (41 kB)\n",
      "  Using cached unstructured_client-0.24.0-py3-none-any.whl (40 kB)\n",
      "  Using cached unstructured_client-0.23.9-py3-none-any.whl (40 kB)\n",
      "  Using cached unstructured_client-0.23.8-py3-none-any.whl (40 kB)\n",
      "  Using cached unstructured_client-0.23.7-py3-none-any.whl (40 kB)\n",
      "  Using cached unstructured_client-0.23.5-py3-none-any.whl (40 kB)\n",
      "  Using cached unstructured_client-0.23.3-py3-none-any.whl (40 kB)\n",
      "  Using cached unstructured_client-0.23.2-py3-none-any.whl (39 kB)\n",
      "  Using cached unstructured_client-0.23.1-py3-none-any.whl (40 kB)\n",
      "  Using cached unstructured_client-0.23.0-py3-none-any.whl (40 kB)\n",
      "  Using cached unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
      "Collecting mypy-extensions>=1.0.0\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Collecting deepdiff>=6.0\n",
      "  Using cached deepdiff-8.6.1-py3-none-any.whl (91 kB)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Collecting unstructured-client\n",
      "  Using cached unstructured_client-0.21.1-py3-none-any.whl (28 kB)\n",
      "  Using cached unstructured_client-0.21.0-py3-none-any.whl (24 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0\n",
      "  Using cached cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.18->-r requirements.txt (line 16)) (3.11.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18->-r requirements.txt (line 16)) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18->-r requirements.txt (line 16)) (2.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.18->-r requirements.txt (line 16)) (1.3.0)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 17)) (2.1.1)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Requirement already satisfied: pyreadline3 in c:\\programdata\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.18->-r requirements.txt (line 16)) (3.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.18->-r requirements.txt (line 16)) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.18->-r requirements.txt (line 16)) (0.6.1)\n",
      "Installing collected packages: rsa, pyasn1-modules, packaging, opentelemetry-util-http, opentelemetry-proto, oauthlib, numpy, mypy-extensions, multidict, mmh3, langdetect, jsonpath-python, jsonpatch, importlib-resources, humanfriendly, httptools, grpcio, greenlet, googleapis-common-protos, fsspec, frozenlist, emoji, distro, charset-normalizer, certifi, cachetools, bcrypt, backoff, async-timeout, asgiref, anyio, annotated-types, aiohappyeyeballs, aiofiles, yarl, watchfiles, uvicorn, typing-inspect, torch, starlette, SQLAlchemy, rich, requests, pydantic, pulsar-client, pandas, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, marshmallow, google-auth, coloredlogs, chroma-hnswlib, aiosignal, typer, torchvision, tiktoken, requests-oauthlib, pydantic-settings, posthog, opentelemetry-semantic-conventions, onnxruntime, langsmith, huggingface-hub, fastapi, dataclasses-json, aiohttp, unstructured-client, tokenizers, opentelemetry-sdk, opentelemetry-instrumentation, openai, langchain-core, kubernetes, unstructured, transformers, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-openai, langchain-community, sentence-transformers, opentelemetry-instrumentation-fastapi, langchain, chromadb\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!cd ../frontend\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41a6bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8664dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\n"
     ]
    }
   ],
   "source": [
    "!chdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1241a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_content = \"\"\"# OpenAI Configuration\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "\n",
    "# Alternative: Use local embeddings (free)\n",
    "USE_LOCAL_EMBEDDINGS=true\n",
    "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# Vector Store\n",
    "VECTOR_DB_PATH=../data/vector_db\n",
    "COLLECTION_NAME=documents\n",
    "\n",
    "# File Upload\n",
    "UPLOAD_DIR=../data/uploads\n",
    "MAX_FILE_SIZE=10485760  # 10MB\n",
    "\n",
    "# LLM Settings\n",
    "LLM_MODEL=gpt-3.5-turbo\n",
    "LLM_TEMPERATURE=0.7\n",
    "MAX_TOKENS=500\n",
    "\"\"\"\n",
    "\n",
    "with open(\"backend/.env.example\", \"w\") as f:\n",
    "    f.write(env_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75133396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec70837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# OpenAI Configuration\n",
      "OPENAI_API_KEY=your_openai_api_key_here\n",
      "\n",
      "# Alternative: Use local embeddings (free)\n",
      "USE_LOCAL_EMBEDDINGS=true\n",
      "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      "# Vector Store\n",
      "VECTOR_DB_PATH=../data/vector_db\n",
      "COLLECTION_NAME=documents\n",
      "\n",
      "# File Upload\n",
      "UPLOAD_DIR=../data/uploads\n",
      "MAX_FILE_SIZE=10485760  # 10MB\n",
      "\n",
      "# LLM Settings\n",
      "LLM_MODEL=gpt-3.5-turbo\n",
      "LLM_TEMPERATURE=0.7\n",
      "MAX_TOKENS=500\n"
     ]
    }
   ],
   "source": [
    "!type backend\\.env.example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c3400b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) copied.\n"
     ]
    }
   ],
   "source": [
    "!copy backend\\.env.example backend\\.env\n",
    "# Edit .env with your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66ce8962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# OpenAI Configuration\n",
      "OPENAI_API_KEY=your_openai_api_key_here\n",
      "\n",
      "# Alternative: Use local embeddings (free)\n",
      "USE_LOCAL_EMBEDDINGS=true\n",
      "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      "# Vector Store\n",
      "VECTOR_DB_PATH=../data/vector_db\n",
      "COLLECTION_NAME=documents\n",
      "\n",
      "# File Upload\n",
      "UPLOAD_DIR=../data/uploads\n",
      "MAX_FILE_SIZE=10485760  # 10MB\n",
      "\n",
      "# LLM Settings\n",
      "LLM_MODEL=gpt-3.5-turbo\n",
      "LLM_TEMPERATURE=0.7\n",
      "MAX_TOKENS=500\n"
     ]
    }
   ],
   "source": [
    "!type backend\\.env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f493306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal 1: Start Backend\n",
    "!cd backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddcbb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\n"
     ]
    }
   ],
   "source": [
    "!chdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a20f292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: uvicorn\n",
      "Version: 0.38.0\n",
      "Summary: The lightning-fast ASGI server.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Tom Christie <tom@tomchristie.com>\n",
      "License: \n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: click, h11, typing-extensions\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# check installation\n",
    "!pip show uvicorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ba8f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_py = \"\"\"\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Backend API\",\n",
    "    description=\"A starter FastAPI backend\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Hello from FastAPI backend!\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\"status\": \"ok\"}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"backend/app/main.py\", \"w\") as f:\n",
    "    f.write(main_py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7a1de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from fastapi import FastAPI\n",
      "\n",
      "app = FastAPI(\n",
      "    title=\"Backend API\",\n",
      "    description=\"A starter FastAPI backend\",\n",
      "    version=\"1.0.0\"\n",
      ")\n",
      "\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    return {\"message\": \"Hello from FastAPI backend!\"}\n",
      "\n",
      "@app.get(\"/health\")\n",
      "def health_check():\n",
      "    return {\"status\": \"ok\"}\n"
     ]
    }
   ],
   "source": [
    "!type backend\\app\\main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9228e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.121.0-py3-none-any.whl (109 kB)\n",
      "     ---------------------------------------- 0.0/109.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 109.2/109.2 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: uvicorn[standard] in c:\\programdata\\anaconda3\\lib\\site-packages (0.38.0)\n",
      "Collecting annotated-doc>=0.0.2\n",
      "  Downloading annotated_doc-0.0.3-py3-none-any.whl (5.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi) (4.15.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "     ---------------------------------------- 0.0/463.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 463.4/463.4 kB 9.6 MB/s eta 0:00:00\n",
      "Collecting starlette<0.50.0,>=0.40.0\n",
      "  Downloading starlette-0.49.3-py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 0.0/74.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 74.3/74.3 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h11>=0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]) (0.12.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]) (8.0.4)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]) (0.4.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]) (6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]) (1.0.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]) (15.0.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn[standard]) (0.7.1)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.1-cp310-cp310-win_amd64.whl (287 kB)\n",
      "Collecting typing-inspection>=0.4.2\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.41.5\n",
      "  Downloading pydantic_core-2.41.5-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     -------------------- ------------------- 1.0/2.0 MB 32.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.0/2.0 MB 31.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 18.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from starlette<0.50.0,>=0.40.0->fastapi) (3.7.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (1.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (3.4)\n",
      "Installing collected packages: typing-inspection, pydantic-core, annotated-types, annotated-doc, watchfiles, starlette, pydantic, fastapi\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.1\n",
      "    Uninstalling pydantic_core-2.14.1:\n",
      "      Successfully uninstalled pydantic_core-2.14.1\n",
      "Successfully installed annotated-doc-0.0.3 annotated-types-0.7.0 fastapi-0.121.0 pydantic-2.12.4 pydantic-core-2.41.5 starlette-0.49.3 typing-inspection-0.4.2 watchfiles-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn[standard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "96120ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d07f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\n"
     ]
    }
   ],
   "source": [
    "!chdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7c9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_code = \"\"\"\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load .env\n",
    "load_dotenv(\".env\")\n",
    "backend_url = os.getenv(\"BACKEND_URL\") \n",
    "# print(backend_url)\n",
    "\n",
    "# Page config\n",
    "# st.set_page_config(page_title=\"Frontend App\", layout=\"wide\")\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"RAG Document Assistant\",\n",
    "    page_icon=\"📚\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS\n",
    "st.markdown(\n",
    "'''\n",
    "    <style>\n",
    "        .main-header {\n",
    "            font-size: 2.5rem;\n",
    "            font-weight: bold;\n",
    "            color: #1f77b4;\n",
    "            text-align: center;\n",
    "            margin-bottom: 2rem;\n",
    "        }\n",
    "        .source-box {\n",
    "            background-color: #f0f2f6;\n",
    "            padding: 1rem;\n",
    "            border-radius: 0.5rem;\n",
    "            margin: 0.5rem 0;\n",
    "            border-left: 3px solid #1f77b4;\n",
    "        }\n",
    "        .metric-card {\n",
    "            background-color: #ffffff;\n",
    "            padding: 1rem;\n",
    "            border-radius: 0.5rem;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }\n",
    "    </style>\n",
    "''', unsafe_allow_html=True)\n",
    "\n",
    "# Upload Document\n",
    "def upload_document(file):\n",
    "    '''Upload a document to the backend and return the response.'''\n",
    "    try:\n",
    "        with st.spinner(\"Uploading document...\"):\n",
    "            files = {\"file\": (file.name, file, file.type)}\n",
    "            response = requests.post(f\"{backend_url}/upload\", files=files)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        st.error(f\"HTTP error: {http_err.response.status_code} - {http_err.response.text}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        st.error(\"Connection error: Unable to reach the backend.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Unexpected error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "# Ask Question\n",
    "def ask_question(question, document_ids=None, top_k=4):\n",
    "    '''Send a question to the backend and return the response.'''\n",
    "    try:\n",
    "        payload = {\n",
    "            \"question\": question,\n",
    "            \"document_ids\": document_ids,\n",
    "            \"top_k\": top_k,\n",
    "            \"include_sources\": True\n",
    "        }\n",
    "        with st.spinner(\"Asking the backend...\"):\n",
    "            response = requests.post(f\"{backend_url}/ask\", json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        st.error(f\"HTTP error: {http_err.response.status_code} - {http_err.response.text}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        st.error(\"Connection error: Unable to reach the backend.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Unexpected error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "# Get Documents\n",
    "def get_documents():\n",
    "    '''Fetch the list of available documents from the backend.'''\n",
    "    try:\n",
    "        with st.spinner(\"Fetching documents...\"):\n",
    "            response = requests.get(f\"{backend_url}/documents\")\n",
    "            if response.status_code == 404:\n",
    "                return []  # Treat 404 as empty list\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        st.error(f\"HTTP error: {http_err.response.status_code} - {http_err.response.text}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        st.error(\"Connection error: Unable to reach the backend.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Unexpected error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "# Delete document(s)\n",
    "def delete_document(document_id):\n",
    "    '''Delete a document by ID from the backend.'''\n",
    "    try:\n",
    "        with st.spinner(f\"Deleting document: {document_id}\"):\n",
    "            response = requests.delete(f\"{backend_url}/documents/{document_id}\")\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        st.error(f\"HTTP error: {http_err.response.status_code} - {http_err.response.text}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        st.error(\"Connection error: Unable to reach the backend.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Unexpected error: {str(e)}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_backend_health():\n",
    "    '''Check if backend is running'''\n",
    "    try:\n",
    "        response = requests.get(f\"{backend_url}/health\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Header\n",
    "    st.markdown('<h1 class=\"main-header\">📚 RAG Document Assistant</h1>', unsafe_allow_html=True)\n",
    "    \n",
    "    # Check backend health\n",
    "    if not check_backend_health():\n",
    "        st.error(\"⚠️ Backend server is not running. Please start it with: `uvicorn app.main:app --reload`\")\n",
    "        st.stop()\n",
    "    \n",
    "    # Sidebar\n",
    "    with st.sidebar:\n",
    "        st.header(\"⚙️ Settings\")\n",
    "        \n",
    "        # Top-K slider\n",
    "        top_k = st.slider(\n",
    "            \"Number of sources to retrieve\",\n",
    "            min_value=1,\n",
    "            max_value=10,\n",
    "            value=4,\n",
    "            help=\"Higher values provide more context but may include less relevant information\"\n",
    "        )\n",
    "        \n",
    "        st.divider()\n",
    "        \n",
    "        # Document management\n",
    "        st.header(\"📄 Documents\")\n",
    "        \n",
    "        docs_response = get_documents()\n",
    "        if docs_response and docs_response.get('documents'):\n",
    "            documents = docs_response['documents']\n",
    "            st.success(f\"✅ {len(documents)} document(s) indexed\")\n",
    "            \n",
    "            # Document selection\n",
    "            selected_docs = st.multiselect(\n",
    "                \"Filter by documents\",\n",
    "                options=[doc['document_id'] for doc in documents],\n",
    "                format_func=lambda x: next(\n",
    "                    (doc['filename'] for doc in documents if doc['document_id'] == x),\n",
    "                    x\n",
    "                ),\n",
    "                help=\"Leave empty to search all documents\"\n",
    "            )\n",
    "            \n",
    "            # Show document details\n",
    "            with st.expander(\"View document details\"):\n",
    "                for doc in documents:\n",
    "                    col1, col2 = st.columns([3, 1])\n",
    "                    with col1:\n",
    "                        st.text(f\"📄 {doc['filename']}\")\n",
    "                        st.caption(f\"Pages: {doc['pages']} | Chunks: {doc['chunks']}\")\n",
    "                    with col2:\n",
    "                        if st.button(\"🗑️\", key=f\"delete_{doc['document_id']}\"):\n",
    "                            if delete_document(doc['document_id']):\n",
    "                                st.success(\"Deleted!\")\n",
    "                                st.rerun()\n",
    "        else:\n",
    "            st.info(\"No documents uploaded yet\")\n",
    "            selected_docs = None\n",
    "        \n",
    "        st.divider()\n",
    "        \n",
    "        # About\n",
    "        st.header(\"ℹ️ About\")\n",
    "        st.markdown('''\n",
    "        This is a RAG (Retrieval-Augmented Generation) system that:\n",
    "        - 📤 Accepts PDF, DOCX, and TXT files\n",
    "        - 🔍 Finds relevant information\n",
    "        - 💬 Answers questions with citations\n",
    "        - 🎯 Provides source references\n",
    "        ''')\n",
    "    \n",
    "    # Main content area - Two tabs\n",
    "    tab1, tab2 = st.tabs([\"💬 Ask Questions\", \"📤 Upload Documents\"])\n",
    "    \n",
    "    # Tab 1: Question Answering\n",
    "    with tab1:\n",
    "        st.header(\"Ask Questions About Your Document(s)\")\n",
    "        \n",
    "        # Check if documents exist\n",
    "        if not docs_response or not docs_response.get('documents'):\n",
    "            st.warning(\"⚠️ Please upload document(s) using the 'Upload Documents' tab\")\n",
    "        else:\n",
    "            # Question input\n",
    "            question = st.text_input(\n",
    "                \"Enter your question:\",\n",
    "                placeholder=\"What is the main topic discussed in the document?\",\n",
    "                key=\"question_input\"\n",
    "            )\n",
    "            \n",
    "            col1, col2, col3 = st.columns([2, 1, 1])\n",
    "            with col1:\n",
    "                ask_button = st.button(\"🔍 Ask Question\", type=\"primary\", use_container_width=True)\n",
    "            with col2:\n",
    "                if st.button(\"🔄 Clear\", use_container_width=True):\n",
    "                    st.session_state.question_input = \"\"\n",
    "                    st.rerun()\n",
    "            \n",
    "            # Process question\n",
    "            if ask_button and question:\n",
    "                with st.spinner(\"🤔 Thinking...\"):\n",
    "                    result = ask_question(\n",
    "                        question,\n",
    "                        document_ids=selected_docs if selected_docs else None,\n",
    "                        top_k=top_k\n",
    "                    )\n",
    "                \n",
    "                if result:\n",
    "                    # Display answer\n",
    "                    st.success(\"✅ Answer Generated\")\n",
    "                    \n",
    "                    # Answer box\n",
    "                    st.markdown(\"### 💡 Answer\")\n",
    "                    st.markdown(f'''\n",
    "                    <div style='background-color: #e8f4f8; padding: 1.5rem; border-radius: 0.5rem; border-left: 4px solid #1f77b4;'>\n",
    "                        {result['answer']}\n",
    "                    </div>\n",
    "                    ''', unsafe_allow_html=True)\n",
    "                    \n",
    "                    # Metadata\n",
    "                    col1, col2, col3 = st.columns(3)\n",
    "                    with col1:\n",
    "                        st.metric(\"⏱️ Processing Time\", f\"{result['processing_time']:.2f}s\")\n",
    "                    with col2:\n",
    "                        st.metric(\"🤖 Model\", result['model_used'])\n",
    "                    with col3:\n",
    "                        st.metric(\"📚 Sources Used\", len(result['sources']))\n",
    "                    \n",
    "                    # Sources\n",
    "                    if result.get('sources'):\n",
    "                        st.markdown(\"### 📖 Sources\")\n",
    "                        for i, source in enumerate(result['sources'], 1):\n",
    "                            with st.expander(\n",
    "                                f\"Source {i}: {source['document_name']} \"\n",
    "                                f\"(Page {source['page_number']}) - \"\n",
    "                                f\"Relevance: {source['relevance_score']:.1%}\"\n",
    "                            ):\n",
    "                                st.text(source['chunk_text'])\n",
    "            elif ask_button:\n",
    "                st.warning(\"Please enter a question\")\n",
    "    \n",
    "    # Tab 2: Document Upload\n",
    "    with tab2:\n",
    "        st.header(\"Upload New Documents\")\n",
    "        \n",
    "        # Upload interface\n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"Choose a file\",\n",
    "            type=['pdf', 'docx', 'txt'],\n",
    "            help=\"Supported formats: PDF, DOCX, TXT (Max 10MB)\"\n",
    "        )\n",
    "        \n",
    "        if uploaded_file:\n",
    "            # Display file info\n",
    "            col1, col2 = st.columns(2)\n",
    "            with col1:\n",
    "                st.info(f\"📄 **File:** {uploaded_file.name}\")\n",
    "            with col2:\n",
    "                file_size_mb = uploaded_file.size / (1024 * 1024)\n",
    "                st.info(f\"💾 **Size:** {file_size_mb:.2f} MB\")\n",
    "            \n",
    "            # Upload button\n",
    "            if st.button(\"📤 Upload and Process\", type=\"primary\", use_container_width=True):\n",
    "                with st.spinner(\"🔄 Processing document...\"):\n",
    "                    # Show progress\n",
    "                    progress_bar = st.progress(0)\n",
    "                    status_text = st.empty()\n",
    "                    \n",
    "                    status_text.text(\"📄 Uploading document...\")\n",
    "                    progress_bar.progress(25)\n",
    "                    \n",
    "                    result = upload_document(uploaded_file)\n",
    "                    \n",
    "                    if result:\n",
    "                        status_text.text(\"✂️ Chunking text...\")\n",
    "                        progress_bar.progress(50)\n",
    "                        time.sleep(0.5)\n",
    "                        \n",
    "                        status_text.text(\"🧮 Generating embeddings...\")\n",
    "                        progress_bar.progress(75)\n",
    "                        time.sleep(0.5)\n",
    "                        \n",
    "                        status_text.text(\"💾 Storing in vector database...\")\n",
    "                        progress_bar.progress(100)\n",
    "                        \n",
    "                        # Success message\n",
    "                        st.success(\"✅ Document processed successfully!\")\n",
    "                        \n",
    "                        # Display results\n",
    "                        col1, col2, col3 = st.columns(3)\n",
    "                        with col1:\n",
    "                            st.metric(\"📄 Pages\", result['pages'])\n",
    "                        with col2:\n",
    "                            st.metric(\"✂️ Chunks\", result['chunks'])\n",
    "                        with col3:\n",
    "                            st.metric(\"🆔 ID\", result['document_id'][:8] + \"...\")\n",
    "                        \n",
    "                        st.info(\"💡 You can now ask questions about this document in the 'Ask Questions' tab\")\n",
    "                        \n",
    "                        # Clear progress\n",
    "                        time.sleep(1)\n",
    "                        progress_bar.empty()\n",
    "                        status_text.empty()\n",
    "        else:\n",
    "            # Instructions\n",
    "            st.markdown('''\n",
    "            ### 📝 Instructions\n",
    "            \n",
    "            1. **Click** the file uploader above\n",
    "            2. **Select** a PDF, DOCX, or TXT file (max 10MB)\n",
    "            3. **Click** 'Upload and Process'\n",
    "            4. **Wait** for processing to complete\n",
    "            5. **Go** to 'Ask Questions' tab to query your document\n",
    "            \n",
    "            ### 💡 Tips\n",
    "            - Documents are automatically chunked for optimal retrieval\n",
    "            - Each chunk includes page number for source tracking\n",
    "            - Multiple documents can be uploaded and queried together\n",
    "            - Use the sidebar to filter which documents to search\n",
    "            ''')\n",
    "    \n",
    "    # Footer\n",
    "    st.divider()\n",
    "    st.markdown('''\n",
    "    <div style='text-align: center; color: #666; padding: 1rem;'>\n",
    "        Built with ❤️ using Streamlit, FastAPI, LangChain, and ChromaDB.<br>\n",
    "        RAG Document Assistant v1.0.0\n",
    "    </div>\n",
    "    ''', unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "with open(\"frontend/streamlit_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(streamlit_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c04b5a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import streamlit as st\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import requests\n",
      "from dotenv import load_dotenv\n",
      "import os\n",
      "\n",
      "# load .env\n",
      "load_dotenv(\".env\")\n",
      "backend_url = os.getenv(\"BACKEND_URL\") + \"/health\"\n",
      "# print(backend_url)\n",
      "\n",
      "# Page config\n",
      "# st.set_page_config(page_title=\"Frontend App\", layout=\"wide\")\n",
      "\n",
      "# Page config\n",
      "st.set_page_config(\n",
      "    page_title=\"RAG Document Assistant\",\n",
      "    page_icon=\"📚\",\n",
      "    layout=\"wide\",\n",
      "    initial_sidebar_state=\"expanded\"\n",
      ")\n",
      "\n",
      "\n",
      "st.title(\"RAG Document Assistant Frontend\")\n",
      "st.write(\"Hello!, this is application using Streamlit for frontend and FastAPI for backend.\")\n",
      "\n",
      "# Example: simple dataframe\n",
      "df = pd.DataFrame({\n",
      "    \"Category\": [\"A\", \"B\", \"C\", \"D\"],\n",
      "    \"Values\": [10, 23, 17, 8]\n",
      "})\n",
      "\n",
      "# Plotly chart\n",
      "fig = px.bar(df, x=\"Category\", y=\"Values\", title=\"Sample Bar Chart\")\n",
      "st.plotly_chart(fig)\n",
      "\n",
      "# Example input\n",
      "name = st.text_input(\"Enter your name:\")\n",
      "if name:\n",
      "    st.success(f\"Welcome, {name}!\")\n",
      "\n",
      "# Backend URL\n",
      "# backend_url = \"http://127.0.0.1:8000/health\"\n",
      "\n",
      "# Button to trigger request\n",
      "if st.button(\"Check Backend Health\"):\n",
      "    st.write(f\"Connecting to: {backend_url}\")\n",
      "    try:\n",
      "        response = requests.get(backend_url)\n",
      "        # print(response)\n",
      "        # print(response.status_code)\n",
      "        if response.status_code == 200:            \n",
      "            data = response.json()\n",
      "            st.success(f\"Backend says: {data['status']}\")\n",
      "        else:\n",
      "            st.error(f\"Error: {response.status_code}\")\n",
      "    except Exception as e:\n",
      "        st.error(f\"Failed to connect: {e}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!type frontend\\streamlit_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "570edfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the environment variable to .env\n",
    "env_text = \"BACKEND_URL=http://127.0.0.1:8000\\n\"\n",
    "\n",
    "with open(\"frontend/.env\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(env_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a651b7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKEND_URL=http://127.0.0.1:8000\n"
     ]
    }
   ],
   "source": [
    "!type frontend\\.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285bad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker setup\n",
    "compose_yaml = \"\"\"version: '3.8'\n",
    "\n",
    "services:\n",
    "  backend:\n",
    "    build: ./backend\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    volumes:\n",
    "      - ./data:/app/data\n",
    "    env_file:\n",
    "      - ./backend/.env\n",
    "    restart: unless-stopped\n",
    "\n",
    "  frontend:\n",
    "    build: ./frontend\n",
    "    ports:\n",
    "      - \"8501:8501\"\n",
    "    depends_on:\n",
    "      - backend\n",
    "    environment:\n",
    "      - BACKEND_URL=http://backend:8000\n",
    "    restart: unless-stopped\n",
    "\n",
    "volumes:\n",
    "  data:\n",
    "\"\"\"\n",
    "\n",
    "with open(\"docker-compose.yml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(compose_yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999d16ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: '3.8'\n",
      "\n",
      "services:\n",
      "  backend:\n",
      "    build: ./backend\n",
      "    ports:\n",
      "      - \"8000:8000\"\n",
      "    volumes:\n",
      "      - ./data:/app/data\n",
      "    env_file:\n",
      "      - ./backend/.env\n",
      "    restart: unless-stopped\n",
      "\n",
      "  frontend:\n",
      "    build: ./frontend\n",
      "    ports:\n",
      "      - \"8501:8501\"\n",
      "    depends_on:\n",
      "      - backend\n",
      "    environment:\n",
      "      - BACKEND_URL=http://backend:8000\n",
      "    restart: unless-stopped\n",
      "\n",
      "volumes:\n",
      "  data:\n"
     ]
    }
   ],
   "source": [
    "!type docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c987889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c2191409\\rag-document-assistant\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a104eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_dockerfile = \"\"\"FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY . .\n",
    "\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"backend/Dockerfile\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(backend_dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcab61af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM python:3.11-slim\n",
      "\n",
      "WORKDIR /app\n",
      "\n",
      "COPY . .\n",
      "\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
     ]
    }
   ],
   "source": [
    "!type backend\\Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e4da9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frontend_dockerfile = \"\"\"FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY . .\n",
    "\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "CMD [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"frontend/Dockerfile\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(frontend_dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599e81de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM python:3.11-slim\n",
      "\n",
      "WORKDIR /app\n",
      "\n",
      "COPY . .\n",
      "\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "CMD [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n"
     ]
    }
   ],
   "source": [
    "!type frontend\\Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbcd3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "gitignore = \"\"\"__pycache__/\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".env\n",
    ".env.*\n",
    "*.sqlite3\n",
    "*.db\n",
    "*.log\n",
    "*.DS_Store\n",
    ".vscode/\n",
    ".idea/\n",
    "*.egg-info/\n",
    "dist/\n",
    "build/\n",
    "data/\n",
    "\"\"\"\n",
    "\n",
    "with open(\".gitignore\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(gitignore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c32d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config files\n",
    "# \n",
    "\n",
    "config_code = '''\"\"\"\n",
    "Configuration management for RAG Document Assistant\n",
    "\"\"\"\n",
    "from pydantic_settings import BaseSettings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Set\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    \"\"\"Application settings loaded from environment variables or defaults.\"\"\"\n",
    "\n",
    "    # 🔐 OpenAI Configuration\n",
    "    openai_api_key: Optional[str] = None\n",
    "\n",
    "    # 🧠 Embedding Configuration\n",
    "    use_local_embeddings: bool = True\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_dimension: int = 384\n",
    "\n",
    "    # 🗃️ Vector Store Configuration\n",
    "    vector_db_path: str = \"../data/vector_db\"\n",
    "    collection_name: str = \"documents\"\n",
    "\n",
    "    # 📤 File Upload Configuration\n",
    "    upload_dir: str = \"../data/uploads\"\n",
    "    max_file_size: int = 10 * 1024 * 1024  # 10MB\n",
    "    allowed_extensions: Set[str] = {\".pdf\", \".docx\", \".txt\"}\n",
    "\n",
    "    # 🤖 LLM Configuration\n",
    "    llm_model: str = \"gpt-3.5-turbo\"\n",
    "    llm_temperature: float = 0.7\n",
    "    max_tokens: int = 500\n",
    "\n",
    "    # 📄 Document Processing\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "\n",
    "    # 🔍 Retrieval Configuration\n",
    "    top_k_results: int = 4\n",
    "    similarity_threshold: float = 0.7\n",
    "\n",
    "    # ⚙️ Application Settings\n",
    "    app_name: str = \"RAG Document Assistant\"\n",
    "    debug: bool = False\n",
    "\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "        case_sensitive = False\n",
    "\n",
    "\n",
    "# Create settings instance\n",
    "settings = Settings()\n",
    "\n",
    "# Ensure required directories exist\n",
    "for path in [settings.upload_dir, settings.vector_db_path]:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open(\"backend/app/config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(config_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b555ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Configuration management for RAG Document Assistant\n",
      "\"\"\"\n",
      "from pydantic_settings import BaseSettings\n",
      "from pathlib import Path\n",
      "from typing import Optional, Set\n",
      "\n",
      "\n",
      "class Settings(BaseSettings):\n",
      "    \"\"\"Application settings loaded from environment variables or defaults.\"\"\"\n",
      "\n",
      "    # 🔐 OpenAI Configuration\n",
      "    openai_api_key: Optional[str] = None\n",
      "\n",
      "    # 🧠 Embedding Configuration\n",
      "    use_local_embeddings: bool = True\n",
      "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
      "    embedding_dimension: int = 384\n",
      "\n",
      "    # 🗃️ Vector Store Configuration\n",
      "    vector_db_path: str = \"../data/vector_db\"\n",
      "    collection_name: str = \"documents\"\n",
      "\n",
      "    # 📤 File Upload Configuration\n",
      "    upload_dir: str = \"../data/uploads\"\n",
      "    max_file_size: int = 10 * 1024 * 1024  # 10MB\n",
      "    allowed_extensions: Set[str] = {\".pdf\", \".docx\", \".txt\"}\n",
      "\n",
      "    # 🤖 LLM Configuration\n",
      "    llm_model: str = \"gpt-3.5-turbo\"\n",
      "    llm_temperature: float = 0.7\n",
      "    max_tokens: int = 500\n",
      "\n",
      "    # 📄 Document Processing\n",
      "    chunk_size: int = 1000\n",
      "    chunk_overlap: int = 200\n",
      "\n",
      "    # 🔍 Retrieval Configuration\n",
      "    top_k_results: int = 4\n",
      "    similarity_threshold: float = 0.7\n",
      "\n",
      "    # ⚙️ Application Settings\n",
      "    app_name: str = \"RAG Document Assistant\"\n",
      "    debug: bool = False\n",
      "\n",
      "    class Config:\n",
      "        env_file = \".env\"\n",
      "        case_sensitive = False\n",
      "\n",
      "\n",
      "# Create settings instance\n",
      "settings = Settings()\n",
      "\n",
      "# Ensure required directories exist\n",
      "for path in [settings.upload_dir, settings.vector_db_path]:\n",
      "    Path(path).mkdir(parents=True, exist_ok=True)\n"
     ]
    }
   ],
   "source": [
    "!type backend\\app\\config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3909e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store\n",
    "\n",
    "vector_store_code = '''\"\"\"\n",
    "Vector store service using ChromaDB for semantic search\n",
    "\"\"\"\n",
    "import chromadb\n",
    "from chromadb.config import Settings as ChromaSettings\n",
    "from typing import List, Dict, Optional\n",
    "from app.config import settings\n",
    "from app.services.embeddings import embedding_service\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages vector storage and retrieval using ChromaDB\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_client()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Initializing ChromaDB client\")\n",
    "            self.client = chromadb.PersistentClient(\n",
    "                path=settings.vector_db_path,\n",
    "                settings=ChromaSettings(\n",
    "                    anonymized_telemetry=False,\n",
    "                    allow_reset=True\n",
    "                )\n",
    "            )\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=settings.collection_name,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"ChromaDB initialized. Collection: {settings.collection_name}, \"\n",
    "                f\"Documents: {self.collection.count()}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def reset_collection(self):\n",
    "        \"\"\"Reset the entire collection (delete all data)\"\"\"\n",
    "        try:\n",
    "            self.client.delete_collection(settings.collection_name)\n",
    "            self.collection = self.client.create_collection(\n",
    "                name=settings.collection_name,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "            logger.info(\"Collection reset successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error resetting collection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, chunks: List[Dict], document_id: str) -> int:\n",
    "        \"\"\"Add document chunks to vector store\"\"\"\n",
    "        try:\n",
    "            if not chunks:\n",
    "                raise ValueError(\"No chunks provided\")\n",
    "\n",
    "            ids, documents, metadatas = [], [], []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{document_id}_chunk_{i}\"\n",
    "                ids.append(chunk_id)\n",
    "                documents.append(chunk[\"text\"])\n",
    "                metadatas.append(chunk[\"metadata\"])\n",
    "\n",
    "            logger.info(f\"Generating embeddings for {len(documents)} chunks\")\n",
    "            embeddings = embedding_service.embed_texts(documents)\n",
    "\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas,\n",
    "                embeddings=embeddings\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Added {len(chunks)} chunks for document {document_id}\")\n",
    "            return len(chunks)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def search(self, query: str, top_k: int = 4, document_ids: Optional[List[str]] = None) -> List[Dict]:\n",
    "        \"\"\"Search for similar chunks\"\"\"\n",
    "        try:\n",
    "            query_embedding = embedding_service.embed_text(query)\n",
    "            where_filter = {\"document_id\": {\"$in\": document_ids}} if document_ids else None\n",
    "\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=top_k,\n",
    "                where=where_filter\n",
    "            )\n",
    "\n",
    "            formatted_results = []\n",
    "            if results and results.get(\"ids\"):\n",
    "                for i in range(len(results[\"ids\"][0])):\n",
    "                    formatted_results.append({\n",
    "                        \"id\": results[\"ids\"][0][i],\n",
    "                        \"text\": results[\"documents\"][0][i],\n",
    "                        \"metadata\": results[\"metadatas\"][0][i],\n",
    "                        \"score\": 1 - results[\"distances\"][0][i]\n",
    "                    })\n",
    "\n",
    "            logger.info(f\"Found {len(formatted_results)} results for query\")\n",
    "            return formatted_results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_document(self, document_id: str) -> int:\n",
    "        \"\"\"Delete all chunks for a document\"\"\"\n",
    "        try:\n",
    "            results = self.collection.get(where={\"document_id\": document_id})\n",
    "            if results and results.get(\"ids\"):\n",
    "                self.collection.delete(ids=results[\"ids\"])\n",
    "                deleted_count = len(results[\"ids\"])\n",
    "                logger.info(f\"Deleted {deleted_count} chunks for document {document_id}\")\n",
    "                return deleted_count\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting document: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_document_count(self) -> int:\n",
    "        \"\"\"Get total number of unique documents\"\"\"\n",
    "        try:\n",
    "            results = self.collection.get()\n",
    "            if results and results.get(\"metadatas\"):\n",
    "                unique_docs = {meta[\"document_id\"] for meta in results[\"metadatas\"]}\n",
    "                return len(unique_docs)\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting document count: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def get_all_documents(self) -> List[Dict]:\n",
    "        \"\"\"Get metadata for all documents\"\"\"\n",
    "        try:\n",
    "            results = self.collection.get()\n",
    "            if not results or not results.get(\"metadatas\"):\n",
    "                return []\n",
    "\n",
    "            docs_dict = {}\n",
    "            for metadata in results[\"metadatas\"]:\n",
    "                doc_id = metadata[\"document_id\"]\n",
    "                if doc_id not in docs_dict:\n",
    "                    docs_dict[doc_id] = {\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"filename\": metadata.get(\"filename\", \"Unknown\"),\n",
    "                        \"file_type\": metadata.get(\"file_type\", \"Unknown\"),\n",
    "                        \"pages\": metadata.get(\"pages\", 0),\n",
    "                        \"chunks\": 0\n",
    "                    }\n",
    "                docs_dict[doc_id][\"chunks\"] += 1\n",
    "\n",
    "            return list(docs_dict.values())\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting all documents: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "vector_store = VectorStore()\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open(\"backend/app/services/vector_store.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(vector_store_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5978a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Vector store service using ChromaDB for semantic search\n",
      "\"\"\"\n",
      "import chromadb\n",
      "from chromadb.config import Settings as ChromaSettings\n",
      "from typing import List, Dict, Optional\n",
      "from app.config import settings\n",
      "from app.services.embeddings import embedding_service\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class VectorStore:\n",
      "    \"\"\"Manages vector storage and retrieval using ChromaDB\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.client = None\n",
      "        self.collection = None\n",
      "        self._initialize_client()\n",
      "\n",
      "    def _initialize_client(self):\n",
      "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
      "        try:\n",
      "            logger.info(\"Initializing ChromaDB client\")\n",
      "            self.client = chromadb.PersistentClient(\n",
      "                path=settings.vector_db_path,\n",
      "                settings=ChromaSettings(\n",
      "                    anonymized_telemetry=False,\n",
      "                    allow_reset=True\n",
      "                )\n",
      "            )\n",
      "            self.collection = self.client.get_or_create_collection(\n",
      "                name=settings.collection_name,\n",
      "                metadata={\"hnsw:space\": \"cosine\"}\n",
      "            )\n",
      "            logger.info(\n",
      "                f\"ChromaDB initialized. Collection: {settings.collection_name}, \"\n",
      "                f\"Documents: {self.collection.count()}\"\n",
      "            )\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error initializing ChromaDB: {e}\")\n",
      "            raise\n",
      "\n",
      "    def reset_collection(self):\n",
      "        \"\"\"Reset the entire collection (delete all data)\"\"\"\n",
      "        try:\n",
      "            self.client.delete_collection(settings.collection_name)\n",
      "            self.collection = self.client.create_collection(\n",
      "                name=settings.collection_name,\n",
      "                metadata={\"hnsw:space\": \"cosine\"}\n",
      "            )\n",
      "            logger.info(\"Collection reset successfully\")\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error resetting collection: {e}\")\n",
      "            raise\n",
      "\n",
      "    def add_documents(self, chunks: List[Dict], document_id: str) -> int:\n",
      "        \"\"\"Add document chunks to vector store\"\"\"\n",
      "        try:\n",
      "            if not chunks:\n",
      "                raise ValueError(\"No chunks provided\")\n",
      "\n",
      "            ids, documents, metadatas = [], [], []\n",
      "            for i, chunk in enumerate(chunks):\n",
      "                chunk_id = f\"{document_id}_chunk_{i}\"\n",
      "                ids.append(chunk_id)\n",
      "                documents.append(chunk[\"text\"])\n",
      "                metadatas.append(chunk[\"metadata\"])\n",
      "\n",
      "            logger.info(f\"Generating embeddings for {len(documents)} chunks\")\n",
      "            embeddings = embedding_service.embed_texts(documents)\n",
      "\n",
      "            self.collection.add(\n",
      "                ids=ids,\n",
      "                documents=documents,\n",
      "                metadatas=metadatas,\n",
      "                embeddings=embeddings\n",
      "            )\n",
      "\n",
      "            logger.info(f\"Added {len(chunks)} chunks for document {document_id}\")\n",
      "            return len(chunks)\n",
      "\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error adding documents: {e}\")\n",
      "            raise\n",
      "\n",
      "    def search(self, query: str, top_k: int = 4, document_ids: Optional[List[str]] = None) -> List[Dict]:\n",
      "        \"\"\"Search for similar chunks\"\"\"\n",
      "        try:\n",
      "            query_embedding = embedding_service.embed_text(query)\n",
      "            where_filter = {\"document_id\": {\"$in\": document_ids}} if document_ids else None\n",
      "\n",
      "            results = self.collection.query(\n",
      "                query_embeddings=[query_embedding],\n",
      "                n_results=top_k,\n",
      "                where=where_filter\n",
      "            )\n",
      "\n",
      "            formatted_results = []\n",
      "            if results and results.get(\"ids\"):\n",
      "                for i in range(len(results[\"ids\"][0])):\n",
      "                    formatted_results.append({\n",
      "                        \"id\": results[\"ids\"][0][i],\n",
      "                        \"text\": results[\"documents\"][0][i],\n",
      "                        \"metadata\": results[\"metadatas\"][0][i],\n",
      "                        \"score\": 1 - results[\"distances\"][0][i]\n",
      "                    })\n",
      "\n",
      "            logger.info(f\"Found {len(formatted_results)} results for query\")\n",
      "            return formatted_results\n",
      "\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error searching: {e}\")\n",
      "            raise\n",
      "\n",
      "    def delete_document(self, document_id: str) -> int:\n",
      "        \"\"\"Delete all chunks for a document\"\"\"\n",
      "        try:\n",
      "            results = self.collection.get(where={\"document_id\": document_id})\n",
      "            if results and results.get(\"ids\"):\n",
      "                self.collection.delete(ids=results[\"ids\"])\n",
      "                deleted_count = len(results[\"ids\"])\n",
      "                logger.info(f\"Deleted {deleted_count} chunks for document {document_id}\")\n",
      "                return deleted_count\n",
      "            return 0\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error deleting document: {e}\")\n",
      "            raise\n",
      "\n",
      "    def get_document_count(self) -> int:\n",
      "        \"\"\"Get total number of unique documents\"\"\"\n",
      "        try:\n",
      "            results = self.collection.get()\n",
      "            if results and results.get(\"metadatas\"):\n",
      "                unique_docs = {meta[\"document_id\"] for meta in results[\"metadatas\"]}\n",
      "                return len(unique_docs)\n",
      "            return 0\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error getting document count: {e}\")\n",
      "            return 0\n",
      "\n",
      "    def get_all_documents(self) -> List[Dict]:\n",
      "        \"\"\"Get metadata for all documents\"\"\"\n",
      "        try:\n",
      "            results = self.collection.get()\n",
      "            if not results or not results.get(\"metadatas\"):\n",
      "                return []\n",
      "\n",
      "            docs_dict = {}\n",
      "            for metadata in results[\"metadatas\"]:\n",
      "                doc_id = metadata[\"document_id\"]\n",
      "                if doc_id not in docs_dict:\n",
      "                    docs_dict[doc_id] = {\n",
      "                        \"document_id\": doc_id,\n",
      "                        \"filename\": metadata.get(\"filename\", \"Unknown\"),\n",
      "                        \"file_type\": metadata.get(\"file_type\", \"Unknown\"),\n",
      "                        \"pages\": metadata.get(\"pages\", 0),\n",
      "                        \"chunks\": 0\n",
      "                    }\n",
      "                docs_dict[doc_id][\"chunks\"] += 1\n",
      "\n",
      "            return list(docs_dict.values())\n",
      "\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error getting all documents: {e}\")\n",
      "            return []\n",
      "\n",
      "\n",
      "# Singleton instance\n",
      "vector_store = VectorStore()\n"
     ]
    }
   ],
   "source": [
    "!type backend\\app\\services\\vector_store.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f48b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document processor\n",
    "\n",
    "test_code = '''\"\"\"\n",
    "Unit tests for DocumentProcessor service\n",
    "\"\"\"\n",
    "import pytest\n",
    "from pathlib import Path\n",
    "from app.services.document_processor import DocumentProcessor\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def processor():\n",
    "    \"\"\"Fixture to create a DocumentProcessor instance\"\"\"\n",
    "    return DocumentProcessor()\n",
    "\n",
    "\n",
    "def test_valid_pdf_file_passes(processor):\n",
    "    \"\"\"Valid PDF file under size limit should pass\"\"\"\n",
    "    processor.validate_file(\"test.pdf\", 5 * 1024 * 1024)  # 5MB\n",
    "\n",
    "\n",
    "def test_invalid_extension_raises_error(processor):\n",
    "    \"\"\"Unsupported file extension should raise ValueError\"\"\"\n",
    "    with pytest.raises(ValueError, match=\"File type not allowed\"):\n",
    "        processor.validate_file(\"test.exe\", 5 * 1024 * 1024)\n",
    "\n",
    "\n",
    "def test_file_too_large_raises_error(processor):\n",
    "    \"\"\"File exceeding size limit should raise ValueError\"\"\"\n",
    "    with pytest.raises(ValueError, match=\"File too large\"):\n",
    "        processor.validate_file(\"test.pdf\", 20 * 1024 * 1024)  # 20MB\n",
    "\n",
    "\n",
    "def test_generate_document_id_is_consistent(processor):\n",
    "    \"\"\"Document ID should be deterministic and space-free\"\"\"\n",
    "    filename = \"test document.pdf\"\n",
    "    content = b\"test content\"\n",
    "    doc_id = processor.generate_document_id(filename, content)\n",
    "\n",
    "    assert isinstance(doc_id, str)\n",
    "    assert len(doc_id) > 0\n",
    "    assert \" \" not in doc_id\n",
    "\n",
    "    # Same input should yield same ID\n",
    "    doc_id2 = processor.generate_document_id(filename, content)\n",
    "    assert doc_id == doc_id2\n",
    "\n",
    "\n",
    "def test_chunk_text_returns_chunks(processor):\n",
    "    \"\"\"Chunking long text should return structured chunks\"\"\"\n",
    "    text = \"This is a test. \" * 200\n",
    "    metadata = {\"document_id\": \"test123\", \"filename\": \"test.txt\"}\n",
    "\n",
    "    chunks = processor.chunk_text(text, metadata)\n",
    "\n",
    "    assert len(chunks) > 0\n",
    "    for chunk in chunks:\n",
    "        assert isinstance(chunk, dict)\n",
    "        assert \"text\" in chunk\n",
    "        assert \"metadata\" in chunk\n",
    "        assert chunk[\"metadata\"][\"document_id\"] == \"test123\"\n",
    "\n",
    "\n",
    "def test_extract_page_number_with_marker(processor):\n",
    "    \"\"\"Should extract page number from marker\"\"\"\n",
    "    text = \"[Page 5]\\nSome content here\"\n",
    "    page_num = processor._extract_page_number(text)\n",
    "    assert page_num == 5\n",
    "\n",
    "\n",
    "def test_extract_page_number_without_marker(processor):\n",
    "    \"\"\"Should default to page 1 if no marker\"\"\"\n",
    "    text = \"Some content without page marker\"\n",
    "    page_num = processor._extract_page_number(text)\n",
    "    assert page_num == 1\n",
    "\n",
    "\n",
    "def test_chunk_text_empty_string_raises_error(processor):\n",
    "    \"\"\"Empty text should raise ValueError\"\"\"\n",
    "    metadata = {\"document_id\": \"test123\"}\n",
    "    with pytest.raises(ValueError, match=\"Text is empty\"):\n",
    "        processor.chunk_text(\"\", metadata)\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open(\"backend/tests/test_document_processor.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(test_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "203277cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Unit tests for DocumentProcessor service\n",
      "\"\"\"\n",
      "import pytest\n",
      "from pathlib import Path\n",
      "from app.services.document_processor import DocumentProcessor\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def processor():\n",
      "    \"\"\"Fixture to create a DocumentProcessor instance\"\"\"\n",
      "    return DocumentProcessor()\n",
      "\n",
      "\n",
      "def test_valid_pdf_file_passes(processor):\n",
      "    \"\"\"Valid PDF file under size limit should pass\"\"\"\n",
      "    processor.validate_file(\"test.pdf\", 5 * 1024 * 1024)  # 5MB\n",
      "\n",
      "\n",
      "def test_invalid_extension_raises_error(processor):\n",
      "    \"\"\"Unsupported file extension should raise ValueError\"\"\"\n",
      "    with pytest.raises(ValueError, match=\"File type not allowed\"):\n",
      "        processor.validate_file(\"test.exe\", 5 * 1024 * 1024)\n",
      "\n",
      "\n",
      "def test_file_too_large_raises_error(processor):\n",
      "    \"\"\"File exceeding size limit should raise ValueError\"\"\"\n",
      "    with pytest.raises(ValueError, match=\"File too large\"):\n",
      "        processor.validate_file(\"test.pdf\", 20 * 1024 * 1024)  # 20MB\n",
      "\n",
      "\n",
      "def test_generate_document_id_is_consistent(processor):\n",
      "    \"\"\"Document ID should be deterministic and space-free\"\"\"\n",
      "    filename = \"test document.pdf\"\n",
      "    content = b\"test content\"\n",
      "    doc_id = processor.generate_document_id(filename, content)\n",
      "\n",
      "    assert isinstance(doc_id, str)\n",
      "    assert len(doc_id) > 0\n",
      "    assert \" \" not in doc_id\n",
      "\n",
      "    # Same input should yield same ID\n",
      "    doc_id2 = processor.generate_document_id(filename, content)\n",
      "    assert doc_id == doc_id2\n",
      "\n",
      "\n",
      "def test_chunk_text_returns_chunks(processor):\n",
      "    \"\"\"Chunking long text should return structured chunks\"\"\"\n",
      "    text = \"This is a test. \" * 200\n",
      "    metadata = {\"document_id\": \"test123\", \"filename\": \"test.txt\"}\n",
      "\n",
      "    chunks = processor.chunk_text(text, metadata)\n",
      "\n",
      "    assert len(chunks) > 0\n",
      "    for chunk in chunks:\n",
      "        assert isinstance(chunk, dict)\n",
      "        assert \"text\" in chunk\n",
      "        assert \"metadata\" in chunk\n",
      "        assert chunk[\"metadata\"][\"document_id\"] == \"test123\"\n",
      "\n",
      "\n",
      "def test_extract_page_number_with_marker(processor):\n",
      "    \"\"\"Should extract page number from marker\"\"\"\n",
      "    text = \"[Page 5]\n",
      "Some content here\"\n",
      "    page_num = processor._extract_page_number(text)\n",
      "    assert page_num == 5\n",
      "\n",
      "\n",
      "def test_extract_page_number_without_marker(processor):\n",
      "    \"\"\"Should default to page 1 if no marker\"\"\"\n",
      "    text = \"Some content without page marker\"\n",
      "    page_num = processor._extract_page_number(text)\n",
      "    assert page_num == 1\n",
      "\n",
      "\n",
      "def test_chunk_text_empty_string_raises_error(processor):\n",
      "    \"\"\"Empty text should raise ValueError\"\"\"\n",
      "    metadata = {\"document_id\": \"test123\"}\n",
      "    with pytest.raises(ValueError, match=\"Text is empty\"):\n",
      "        processor.chunk_text(\"\", metadata)\n"
     ]
    }
   ],
   "source": [
    "!type backend\\tests\\test_document_processor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11e76e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "\n",
    "embedding_code = '''\n",
    "\"\"\"\n",
    "Embedding service for converting text to vectors\n",
    "RAG Document Assistant - Embeddings with caching and similarity\n",
    "\"\"\"\n",
    "from typing import List, Optional, Union\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from app.config import settings\n",
    "import logging\n",
    "import time\n",
    "import hashlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"Simple file-based cache for embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, cache_dir: str = \"data/embedding_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.enabled = True\n",
    "\n",
    "    def _get_cache_key(self, text: str, model_name: str) -> str:\n",
    "        content = f\"{model_name}:{text}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "\n",
    "    def get(self, text: str, model_name: str) -> Optional[np.ndarray]:\n",
    "        if not self.enabled:\n",
    "            return None\n",
    "        try:\n",
    "            cache_file = self.cache_dir / f\"{self._get_cache_key(text, model_name)}.pkl\"\n",
    "            if cache_file.exists():\n",
    "                with open(cache_file, \"rb\") as f:\n",
    "                    return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Cache read error: {e}\")\n",
    "        return None\n",
    "\n",
    "    def set(self, text: str, model_name: str, embedding: np.ndarray):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        try:\n",
    "            cache_file = self.cache_dir / f\"{self._get_cache_key(text, model_name)}.pkl\"\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(embedding, f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Cache write error: {e}\")\n",
    "\n",
    "    def clear(self):\n",
    "        try:\n",
    "            for f in self.cache_dir.glob(\"*.pkl\"):\n",
    "                f.unlink()\n",
    "            logger.info(\"Embedding cache cleared\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cache clear error: {e}\")\n",
    "\n",
    "    def get_cache_stats(self) -> dict:\n",
    "        files = list(self.cache_dir.glob(\"*.pkl\"))\n",
    "        total_size = sum(f.stat().st_size for f in files)\n",
    "        return {\n",
    "            \"cached_items\": len(files),\n",
    "            \"total_size_mb\": round(total_size / (1024 * 1024), 2),\n",
    "            \"cache_enabled\": self.enabled\n",
    "        }\n",
    "\n",
    "\n",
    "class EmbeddingService:\n",
    "    \"\"\"Handles embedding generation, caching, and similarity\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_name = settings.embedding_model\n",
    "        self.cache = EmbeddingCache()\n",
    "        self.model = None\n",
    "        self.embedding_dimension = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            logger.info(f\"Loading model: {self.model_name}\")\n",
    "            start = time.time()\n",
    "            self.model = SentenceTransformer(self.model_name, device=\"cpu\")\n",
    "            self.embedding_dimension = self.model.get_sentence_embedding_dimension()\n",
    "            logger.info(f\"Model loaded in {time.time() - start:.2f}s\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model load error: {e}\")\n",
    "            raise RuntimeError(f\"Failed to load model: {e}\")\n",
    "\n",
    "    def embed_text(self, text: str, normalize: bool = True, use_cache: bool = True) -> List[float]:\n",
    "        if not text or not text.strip():\n",
    "            raise ValueError(\"Empty text for embedding\")\n",
    "        text = text.strip()\n",
    "        if use_cache:\n",
    "            cached = self.cache.get(text, self.model_name)\n",
    "            if cached is not None:\n",
    "                return cached.tolist()\n",
    "        try:\n",
    "            embedding = self.model.encode(\n",
    "                text,\n",
    "                normalize_embeddings=normalize,\n",
    "                show_progress_bar=False,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            if use_cache:\n",
    "                self.cache.set(text, self.model_name, embedding)\n",
    "            return embedding.tolist()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Embedding error: {e}\")\n",
    "            raise RuntimeError(f\"Embedding failed: {e}\")\n",
    "\n",
    "    def embed_texts(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        batch_size: int = 32,\n",
    "        normalize: bool = True,\n",
    "        show_progress: bool = True,\n",
    "        use_cache: bool = True\n",
    "    ) -> List[List[float]]:\n",
    "        if not texts:\n",
    "            raise ValueError(\"Empty text list\")\n",
    "        valid_texts = [t.strip() for t in texts if t and t.strip()]\n",
    "        if not valid_texts:\n",
    "            raise ValueError(\"No valid texts\")\n",
    "\n",
    "        embeddings = [None] * len(valid_texts)\n",
    "        to_embed, indices = [], []\n",
    "\n",
    "        for i, text in enumerate(valid_texts):\n",
    "            if use_cache:\n",
    "                cached = self.cache.get(text, self.model_name)\n",
    "                if cached is not None:\n",
    "                    embeddings[i] = cached\n",
    "                else:\n",
    "                    to_embed.append(text)\n",
    "                    indices.append(i)\n",
    "            else:\n",
    "                to_embed.append(text)\n",
    "                indices.append(i)\n",
    "\n",
    "        if to_embed:\n",
    "            try:\n",
    "                start = time.time()\n",
    "                new_embeddings = self.model.encode(\n",
    "                    to_embed,\n",
    "                    batch_size=batch_size,\n",
    "                    normalize_embeddings=normalize,\n",
    "                    show_progress_bar=show_progress and len(to_embed) > 50,\n",
    "                    convert_to_numpy=True\n",
    "                )\n",
    "                for i, emb in zip(indices, new_embeddings):\n",
    "                    embeddings[i] = emb\n",
    "                    if use_cache:\n",
    "                        self.cache.set(valid_texts[i], self.model_name, emb)\n",
    "                logger.info(f\"Embedded {len(to_embed)} texts in {time.time() - start:.2f}s\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch embedding error: {e}\")\n",
    "                raise RuntimeError(f\"Batch embedding failed: {e}\")\n",
    "\n",
    "        return [emb.tolist() for emb in embeddings]\n",
    "\n",
    "    def compute_similarity(\n",
    "        self,\n",
    "        embedding1: Union[List[float], np.ndarray],\n",
    "        embedding2: Union[List[float], np.ndarray],\n",
    "        metric: str = \"cosine\"\n",
    "    ) -> float:\n",
    "        emb1 = np.array(embedding1)\n",
    "        emb2 = np.array(embedding2)\n",
    "        try:\n",
    "            if metric == \"cosine\":\n",
    "                return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))\n",
    "            elif metric == \"dot\":\n",
    "                return float(np.dot(emb1, emb2))\n",
    "            elif metric == \"euclidean\":\n",
    "                return float(1.0 / (1.0 + np.linalg.norm(emb1 - emb2)))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Similarity error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        return self.embedding_dimension\n",
    "\n",
    "    def get_model_info(self) -> dict:\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"embedding_dimension\": self.embedding_dimension,\n",
    "            \"max_sequence_length\": self.model.max_seq_length,\n",
    "            \"device\": str(self.model.device),\n",
    "            \"cache_stats\": self.cache.get_cache_stats()\n",
    "        }\n",
    "\n",
    "    def benchmark(self, num_texts: int = 100, text_length: int = 200) -> dict:\n",
    "        import random, string\n",
    "        test_texts = [\n",
    "            ''.join(random.choices(string.ascii_letters + \" \", k=text_length))\n",
    "            for _ in range(num_texts)\n",
    "        ]\n",
    "        try:\n",
    "            single_start = time.time()\n",
    "            for text in test_texts[:10]:\n",
    "                self.embed_text(text, use_cache=False)\n",
    "            single_rate = 10 / (time.time() - single_start)\n",
    "\n",
    "            batch_start = time.time()\n",
    "            self.embed_texts(test_texts, batch_size=32, show_progress=False, use_cache=False)\n",
    "            batch_rate = num_texts / (time.time() - batch_start)\n",
    "\n",
    "            return {\n",
    "                \"single_embedding_rate\": round(single_rate, 2),\n",
    "                \"batch_embedding_rate\": round(batch_rate, 2),\n",
    "                \"speedup_factor\": round(batch_rate / single_rate, 2),\n",
    "                \"total_texts\": num_texts\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Benchmark error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def reload_model(self, model_name: Optional[str] = None):\n",
    "        try:\n",
    "            if model_name:\n",
    "                self.model_name = model_name\n",
    "                logger.info(f\"Reloading model: {model_name}\")\n",
    "            if self.model:\n",
    "                del self.model\n",
    "            self._load_model()\n",
    "            if model_name:\n",
    "                self.cache.clear()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Reload error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache.clear()\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.model:\n",
    "            del self.model\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "embedding_service = EmbeddingService()\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def embed_query(query: str) -> List[float]:\n",
    "    return embedding_service.embed_text(query)\n",
    "\n",
    "def embed_documents(documents: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "    return embedding_service.embed_texts(documents, batch_size=batch_size)\n",
    "\n",
    "def get_embedding_info() -> dict:\n",
    "    return embedding_service.get_model_info()\n",
    "\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open(\"backend/app/services/embeddings.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(embedding_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6311d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\"\"\n",
      "Embedding service for converting text to vectors\n",
      "RAG Document Assistant - Embeddings with caching and similarity\n",
      "\"\"\"\n",
      "from typing import List, Optional, Union\n",
      "from sentence_transformers import SentenceTransformer\n",
      "import numpy as np\n",
      "from app.config import settings\n",
      "import logging\n",
      "import time\n",
      "import hashlib\n",
      "import pickle\n",
      "from pathlib import Path\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class EmbeddingCache:\n",
      "    \"\"\"Simple file-based cache for embeddings\"\"\"\n",
      "\n",
      "    def __init__(self, cache_dir: str = \"data/embedding_cache\"):\n",
      "        self.cache_dir = Path(cache_dir)\n",
      "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
      "        self.enabled = True\n",
      "\n",
      "    def _get_cache_key(self, text: str, model_name: str) -> str:\n",
      "        content = f\"{model_name}:{text}\"\n",
      "        return hashlib.md5(content.encode()).hexdigest()\n",
      "\n",
      "    def get(self, text: str, model_name: str) -> Optional[np.ndarray]:\n",
      "        if not self.enabled:\n",
      "            return None\n",
      "        try:\n",
      "            cache_file = self.cache_dir / f\"{self._get_cache_key(text, model_name)}.pkl\"\n",
      "            if cache_file.exists():\n",
      "                with open(cache_file, \"rb\") as f:\n",
      "                    return pickle.load(f)\n",
      "        except Exception as e:\n",
      "            logger.warning(f\"Cache read error: {e}\")\n",
      "        return None\n",
      "\n",
      "    def set(self, text: str, model_name: str, embedding: np.ndarray):\n",
      "        if not self.enabled:\n",
      "            return\n",
      "        try:\n",
      "            cache_file = self.cache_dir / f\"{self._get_cache_key(text, model_name)}.pkl\"\n",
      "            with open(cache_file, \"wb\") as f:\n",
      "                pickle.dump(embedding, f)\n",
      "        except Exception as e:\n",
      "            logger.warning(f\"Cache write error: {e}\")\n",
      "\n",
      "    def clear(self):\n",
      "        try:\n",
      "            for f in self.cache_dir.glob(\"*.pkl\"):\n",
      "                f.unlink()\n",
      "            logger.info(\"Embedding cache cleared\")\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Cache clear error: {e}\")\n",
      "\n",
      "    def get_cache_stats(self) -> dict:\n",
      "        files = list(self.cache_dir.glob(\"*.pkl\"))\n",
      "        total_size = sum(f.stat().st_size for f in files)\n",
      "        return {\n",
      "            \"cached_items\": len(files),\n",
      "            \"total_size_mb\": round(total_size / (1024 * 1024), 2),\n",
      "            \"cache_enabled\": self.enabled\n",
      "        }\n",
      "\n",
      "\n",
      "class EmbeddingService:\n",
      "    \"\"\"Handles embedding generation, caching, and similarity\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.model_name = settings.embedding_model\n",
      "        self.cache = EmbeddingCache()\n",
      "        self.model = None\n",
      "        self.embedding_dimension = None\n",
      "        self._load_model()\n",
      "\n",
      "    def _load_model(self):\n",
      "        try:\n",
      "            logger.info(f\"Loading model: {self.model_name}\")\n",
      "            start = time.time()\n",
      "            self.model = SentenceTransformer(self.model_name, device=\"cpu\")\n",
      "            self.embedding_dimension = self.model.get_sentence_embedding_dimension()\n",
      "            logger.info(f\"Model loaded in {time.time() - start:.2f}s\")\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Model load error: {e}\")\n",
      "            raise RuntimeError(f\"Failed to load model: {e}\")\n",
      "\n",
      "    def embed_text(self, text: str, normalize: bool = True, use_cache: bool = True) -> List[float]:\n",
      "        if not text or not text.strip():\n",
      "            raise ValueError(\"Empty text for embedding\")\n",
      "        text = text.strip()\n",
      "        if use_cache:\n",
      "            cached = self.cache.get(text, self.model_name)\n",
      "            if cached is not None:\n",
      "                return cached.tolist()\n",
      "        try:\n",
      "            embedding = self.model.encode(\n",
      "                text,\n",
      "                normalize_embeddings=normalize,\n",
      "                show_progress_bar=False,\n",
      "                convert_to_numpy=True\n",
      "            )\n",
      "            if use_cache:\n",
      "                self.cache.set(text, self.model_name, embedding)\n",
      "            return embedding.tolist()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Embedding error: {e}\")\n",
      "            raise RuntimeError(f\"Embedding failed: {e}\")\n",
      "\n",
      "    def embed_texts(\n",
      "        self,\n",
      "        texts: List[str],\n",
      "        batch_size: int = 32,\n",
      "        normalize: bool = True,\n",
      "        show_progress: bool = True,\n",
      "        use_cache: bool = True\n",
      "    ) -> List[List[float]]:\n",
      "        if not texts:\n",
      "            raise ValueError(\"Empty text list\")\n",
      "        valid_texts = [t.strip() for t in texts if t and t.strip()]\n",
      "        if not valid_texts:\n",
      "            raise ValueError(\"No valid texts\")\n",
      "\n",
      "        embeddings = [None] * len(valid_texts)\n",
      "        to_embed, indices = [], []\n",
      "\n",
      "        for i, text in enumerate(valid_texts):\n",
      "            if use_cache:\n",
      "                cached = self.cache.get(text, self.model_name)\n",
      "                if cached is not None:\n",
      "                    embeddings[i] = cached\n",
      "                else:\n",
      "                    to_embed.append(text)\n",
      "                    indices.append(i)\n",
      "            else:\n",
      "                to_embed.append(text)\n",
      "                indices.append(i)\n",
      "\n",
      "        if to_embed:\n",
      "            try:\n",
      "                start = time.time()\n",
      "                new_embeddings = self.model.encode(\n",
      "                    to_embed,\n",
      "                    batch_size=batch_size,\n",
      "                    normalize_embeddings=normalize,\n",
      "                    show_progress_bar=show_progress and len(to_embed) > 50,\n",
      "                    convert_to_numpy=True\n",
      "                )\n",
      "                for i, emb in zip(indices, new_embeddings):\n",
      "                    embeddings[i] = emb\n",
      "                    if use_cache:\n",
      "                        self.cache.set(valid_texts[i], self.model_name, emb)\n",
      "                logger.info(f\"Embedded {len(to_embed)} texts in {time.time() - start:.2f}s\")\n",
      "            except Exception as e:\n",
      "                logger.error(f\"Batch embedding error: {e}\")\n",
      "                raise RuntimeError(f\"Batch embedding failed: {e}\")\n",
      "\n",
      "        return [emb.tolist() for emb in embeddings]\n",
      "\n",
      "    def compute_similarity(\n",
      "        self,\n",
      "        embedding1: Union[List[float], np.ndarray],\n",
      "        embedding2: Union[List[float], np.ndarray],\n",
      "        metric: str = \"cosine\"\n",
      "    ) -> float:\n",
      "        emb1 = np.array(embedding1)\n",
      "        emb2 = np.array(embedding2)\n",
      "        try:\n",
      "            if metric == \"cosine\":\n",
      "                return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))\n",
      "            elif metric == \"dot\":\n",
      "                return float(np.dot(emb1, emb2))\n",
      "            elif metric == \"euclidean\":\n",
      "                return float(1.0 / (1.0 + np.linalg.norm(emb1 - emb2)))\n",
      "            else:\n",
      "                raise ValueError(f\"Unknown metric: {metric}\")\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Similarity error: {e}\")\n",
      "            raise\n",
      "\n",
      "    def get_embedding_dimension(self) -> int:\n",
      "        return self.embedding_dimension\n",
      "\n",
      "    def get_model_info(self) -> dict:\n",
      "        return {\n",
      "            \"model_name\": self.model_name,\n",
      "            \"embedding_dimension\": self.embedding_dimension,\n",
      "            \"max_sequence_length\": self.model.max_seq_length,\n",
      "            \"device\": str(self.model.device),\n",
      "            \"cache_stats\": self.cache.get_cache_stats()\n",
      "        }\n",
      "\n",
      "    def benchmark(self, num_texts: int = 100, text_length: int = 200) -> dict:\n",
      "        import random, string\n",
      "        test_texts = [\n",
      "            ''.join(random.choices(string.ascii_letters + \" \", k=text_length))\n",
      "            for _ in range(num_texts)\n",
      "        ]\n",
      "        try:\n",
      "            single_start = time.time()\n",
      "            for text in test_texts[:10]:\n",
      "                self.embed_text(text, use_cache=False)\n",
      "            single_rate = 10 / (time.time() - single_start)\n",
      "\n",
      "            batch_start = time.time()\n",
      "            self.embed_texts(test_texts, batch_size=32, show_progress=False, use_cache=False)\n",
      "            batch_rate = num_texts / (time.time() - batch_start)\n",
      "\n",
      "            return {\n",
      "                \"single_embedding_rate\": round(single_rate, 2),\n",
      "                \"batch_embedding_rate\": round(batch_rate, 2),\n",
      "                \"speedup_factor\": round(batch_rate / single_rate, 2),\n",
      "                \"total_texts\": num_texts\n",
      "            }\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Benchmark error: {e}\")\n",
      "            raise\n",
      "\n",
      "    def reload_model(self, model_name: Optional[str] = None):\n",
      "        try:\n",
      "            if model_name:\n",
      "                self.model_name = model_name\n",
      "                logger.info(f\"Reloading model: {model_name}\")\n",
      "            if self.model:\n",
      "                del self.model\n",
      "            self._load_model()\n",
      "            if model_name:\n",
      "                self.cache.clear()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Reload error: {e}\")\n",
      "            raise\n",
      "\n",
      "    def clear_cache(self):\n",
      "        self.cache.clear()\n",
      "\n",
      "    def __del__(self):\n",
      "        if self.model:\n",
      "            del self.model\n",
      "\n",
      "\n",
      "# Singleton instance\n",
      "embedding_service = EmbeddingService()\n",
      "\n",
      "\n",
      "# Utility functions\n",
      "def embed_query(query: str) -> List[float]:\n",
      "    return embedding_service.embed_text(query)\n",
      "\n",
      "def embed_documents(documents: List[str], batch_size: int = 32) -> List[List[float]]:\n",
      "    return embedding_service.embed_texts(documents, batch_size=batch_size)\n",
      "\n",
      "def get_embedding_info() -> dict:\n",
      "    return embedding_service.get_model_info()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!type backend\\app\\services\\embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9882dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM service\n",
    "\n",
    "llm_service_code = '''\n",
    "\"\"\"\n",
    "LLM service for question answering\n",
    "RAG Document Assistant - Complete LLM Service\n",
    "\"\"\"\n",
    "from typing import List, Dict, Optional, Iterator\n",
    "from openai import OpenAI\n",
    "from app.config import settings\n",
    "import logging\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PromptTemplate:\n",
    "    \"\"\"Collection of prompt templates for different tasks\"\"\"\n",
    "\n",
    "    SYSTEM_QA = \"\"\"You are a helpful AI assistant that answers questions based on provided documents.\n",
    "\n",
    "Instructions:\n",
    "1. Answer using ONLY the provided sources\n",
    "2. If not found, say \"I cannot find this information in the provided documents\"\n",
    "3. Cite sources (e.g., \"According to Source 1...\")\n",
    "4. Be concise but complete\n",
    "5. Synthesize multiple sources if needed\n",
    "6. Do not infer or fabricate\n",
    "7. Maintain a professional tone\"\"\"\n",
    "\n",
    "    SYSTEM_SUMMARIZATION = \"\"\"You are a helpful AI assistant that creates concise summaries of documents.\n",
    "\n",
    "Instructions:\n",
    "1. Summarize the provided text\n",
    "2. Focus on key points\n",
    "3. Be objective and accurate\n",
    "4. Limit to 3–5 sentences\n",
    "5. Use bullet points if appropriate\"\"\"\n",
    "\n",
    "    SYSTEM_COMPARISON = \"\"\"You are a helpful AI assistant that compares information across multiple documents.\n",
    "\n",
    "Instructions:\n",
    "1. Identify similarities and differences\n",
    "2. Reference each source clearly\n",
    "3. Highlight agreements and disagreements\n",
    "4. Be objective and organized\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_qa_prompt(question: str, context_chunks: List[Dict]) -> tuple:\n",
    "        context = \"\\n\".join(\n",
    "            f\"[Source {i+1}: {chunk.get('metadata', {}).get('filename', 'Unknown')}, Page {chunk.get('metadata', {}).get('page_number', '?')}]\\n{chunk['text']}\"\n",
    "            for i, chunk in enumerate(context_chunks)\n",
    "        )\n",
    "        user_prompt = f\"\"\"Context from documents:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer the question based on the context provided above. Remember to cite your sources.\"\"\"\n",
    "        return PromptTemplate.SYSTEM_QA, user_prompt\n",
    "\n",
    "    @staticmethod\n",
    "    def create_summarization_prompt(text: str, max_sentences: int = 5) -> tuple:\n",
    "        user_prompt = f\"Please summarize the following text in {max_sentences} sentences or less:\\n\\n{text}\\n\\nSummary:\"\n",
    "        return PromptTemplate.SYSTEM_SUMMARIZATION, user_prompt\n",
    "\n",
    "    @staticmethod\n",
    "    def create_comparison_prompt(question: str, sources_by_doc: Dict[str, List[Dict]]) -> tuple:\n",
    "        context = \"\"\n",
    "        for doc_id, chunks in sources_by_doc.items():\n",
    "            if chunks:\n",
    "                doc_name = chunks[0].get('metadata', {}).get('filename', doc_id)\n",
    "                context += f\"\\n=== From {doc_name} ===\\n\" + \"\\n\".join(chunk['text'] for chunk in chunks)\n",
    "        user_prompt = f\"Compare the information from different documents regarding this question:\\n\\nQuestion: {question}\\n\\nDocuments:\\n{context}\\n\\nProvide a comparative analysis:\"\n",
    "        return PromptTemplate.SYSTEM_COMPARISON, user_prompt\n",
    "\n",
    "\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"Simple in-memory cache for LLM responses\"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "\n",
    "    def _generate_key(self, question: str, context_ids: List[str], model: str) -> str:\n",
    "        content = f\"{model}:{question}:{','.join(sorted(context_ids))}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "\n",
    "    def get(self, question: str, context_chunks: List[Dict], model: str) -> Optional[str]:\n",
    "        try:\n",
    "            context_ids = [chunk.get('id', str(i)) for i, chunk in enumerate(context_chunks)]\n",
    "            key = self._generate_key(question, context_ids, model)\n",
    "            if key in self.cache:\n",
    "                self.hits += 1\n",
    "                return self.cache[key]\n",
    "            self.misses += 1\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Cache retrieval error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def set(self, question: str, context_chunks: List[Dict], model: str, response: str):\n",
    "        try:\n",
    "            context_ids = [chunk.get('id', str(i)) for i, chunk in enumerate(context_chunks)]\n",
    "            key = self._generate_key(question, context_ids, model)\n",
    "            if len(self.cache) >= self.max_size:\n",
    "                self.cache.pop(next(iter(self.cache)))\n",
    "            self.cache[key] = response\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Cache store error: {e}\")\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
    "        return {\n",
    "            \"cached_items\": len(self.cache),\n",
    "            \"max_size\": self.max_size,\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"hit_rate_percent\": round(hit_rate, 2)\n",
    "        }\n",
    "\n",
    "    def clear(self):\n",
    "        self.cache.clear()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "\n",
    "\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"Handles LLM-based question answering\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.cache = ResponseCache()\n",
    "        self.total_tokens_used = 0\n",
    "        self.total_requests = 0\n",
    "        self._initialize_client()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        if settings.openai_api_key:\n",
    "            try:\n",
    "                self.client = OpenAI(api_key=settings.openai_api_key)\n",
    "                logger.info(\"OpenAI client initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"OpenAI init error: {e}\")\n",
    "                self.client = None\n",
    "        else:\n",
    "            logger.warning(\"OpenAI API key missing\")\n",
    "\n",
    "    def generate_answer(self, question: str, context_chunks: List[Dict], model=None, temperature=None, max_tokens=None, use_cache=True) -> str:\n",
    "        if not question.strip():\n",
    "            raise ValueError(\"Question cannot be empty\")\n",
    "        if not context_chunks:\n",
    "            return \"No relevant information available. Please upload documents first.\"\n",
    "\n",
    "        model = model or settings.llm_model\n",
    "        temperature = temperature if temperature is not None else settings.llm_temperature\n",
    "        max_tokens = max_tokens or settings.max_tokens\n",
    "\n",
    "        if use_cache:\n",
    "            cached = self.cache.get(question, context_chunks, model)\n",
    "            if cached:\n",
    "                return cached\n",
    "\n",
    "        if not self.client:\n",
    "            return self.generate_fallback_answer(question, context_chunks)\n",
    "\n",
    "        system_prompt, user_prompt = PromptTemplate.create_qa_prompt(question, context_chunks)\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            self.total_requests += 1\n",
    "            if hasattr(response, \"usage\"):\n",
    "                self.total_tokens_used += response.usage.total_tokens\n",
    "            if use_cache:\n",
    "                self.cache.set(question, context_chunks, model, answer)\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"LLM error: {e}\")\n",
    "            return self.generate_fallback_answer(question, context_chunks)\n",
    "\n",
    "    def generate_fallback_answer(self, question: str, context_chunks: List[Dict]) -> str:\n",
    "        if not context_chunks:\n",
    "            return \"I couldn't find any relevant information to answer your question.\"\n",
    "        parts = [\"Based on the documents, here's what I found:\\n\"]\n",
    "        for i, chunk in enumerate(context_chunks[:3], 1):\n",
    "            meta = chunk.get(\"metadata\", {})\n",
    "            doc = meta.get(\"filename\", \"Unknown\")\n",
    "            page = meta.get(\"page_number\", \"?\")\n",
    "            text = chunk[\"text\"][:400]\n",
    "            parts.append(f\"\\n**From {doc} (Page {page}):**\\n{text}...\")\n",
    "        parts.append(\"\\n\\n*Note: This is a fallback response. For better answers, configure an OpenAI API key.*\")\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    def summarize_document(self, text: str, max_sentences: int = 5, model: Optional[str] = None) -> str:\n",
    "        if not self.client:\n",
    "            return \"Summary generation requires OpenAI API key.\"\n",
    "        model = model or settings.llm_model\n",
    "        system_prompt, user_prompt = PromptTemplate.create_summarization_prompt(text, max_sentences)\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Summary error: {e}\")\n",
    "            return f\"Error generating summary: {str(e)}        except Exception as e:\n",
    "            \n",
    "\n",
    "    def compare_documents(self, question: str, sources_by_doc: Dict[str, List[Dict]], model: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate comparative analysis across multiple documents.\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            return \"Comparison requires OpenAI API key.\"\n",
    "\n",
    "        model = model or settings.llm_model\n",
    "        system_prompt, user_prompt = PromptTemplate.create_comparison_prompt(question, sources_by_doc)\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=800\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating comparison: {e}\")\n",
    "            return f\"Error generating comparison: {str(e)}\"\n",
    "\n",
    "    def get_usage_stats(self) -> dict:\n",
    "        \"\"\"Get usage statistics.\"\"\"\n",
    "        cache_stats = self.cache.get_stats()\n",
    "        return {\n",
    "            \"total_requests\": self.total_requests,\n",
    "            \"total_tokens_used\": self.total_tokens_used,\n",
    "            \"average_tokens_per_request\": round(self.total_tokens_used / self.total_requests, 2) if self.total_requests > 0 else 0,\n",
    "            \"cache_stats\": cache_stats,\n",
    "            \"client_initialized\": self.client is not None\n",
    "        }\n",
    "\n",
    "    def estimate_cost(self, model: Optional[str] = None) -> dict:\n",
    "        \"\"\"Estimate API cost based on usage.\"\"\"\n",
    "        model = model or settings.llm_model\n",
    "        pricing = {\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.50, \"output\": 1.50},\n",
    "            \"gpt-4\": {\"input\": 30.00, \"output\": 60.00},\n",
    "            \"gpt-4-turbo\": {\"input\": 10.00, \"output\": 30.00},\n",
    "        }\n",
    "\n",
    "        if model not in pricing:\n",
    "            return {\"error\": f\"Pricing not available for model: {model}\"}\n",
    "\n",
    "        avg_price_per_1m = (pricing[model][\"input\"] + pricing[model][\"output\"]) / 2\n",
    "        estimated_cost = (self.total_tokens_used / 1_000_000) * avg_price_per_1m\n",
    "\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"total_tokens\": self.total_tokens_used,\n",
    "            \"estimated_cost_usd\": round(estimated_cost, 4),\n",
    "            \"pricing_per_1m_tokens\": pricing[model]\n",
    "        }\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the response cache.\"\"\"\n",
    "        self.cache.clear()\n",
    "\n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset usage statistics.\"\"\"\n",
    "        self.total_tokens_used = 0\n",
    "        self.total_requests = 0\n",
    "        logger.info(\"Usage statistics reset\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SINGLETON INSTANCE\n",
    "# ============================================================================\n",
    "\n",
    "llm_service = LLMService()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def ask_question(question: str, context_chunks: List[Dict], **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Convenience function to ask a question.\n",
    "    \"\"\"\n",
    "    return llm_service.generate_answer(question, context_chunks, **kwargs)\n",
    "\n",
    "\n",
    "def get_llm_stats() -> dict:\n",
    "    \"\"\"Get LLM service statistics.\"\"\"\n",
    "    return llm_service.get_usage_stats()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    example_chunks = [\n",
    "        {\n",
    "            \"text\": \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "            \"metadata\": {\"filename\": \"ml_intro.pdf\", \"page_number\": 1, \"document_id\": \"doc1\"}\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Deep learning uses neural networks with multiple layers to process complex patterns.\",\n",
    "            \"metadata\": {\"filename\": \"ml_intro.pdf\", \"page_number\": 2, \"document_id\": \"doc1\"}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"\\n=== Testing Question Answering ===\")\n",
    "    question = \"What is machine learning?\"\n",
    "    answer = llm_service.generate_answer(question, example_chunks)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    print(\"\\n=== Testing Cache ===\")\n",
    "    answer2 = llm_service.generate_answer(question, example_chunks)\n",
    "    print(\"Second request (should be cached):\")\n",
    "    print(answer2)\n",
    "\n",
    "\n",
    "    print(\"\\n=== Usage Statistics ===\")\n",
    "    stats = llm_service.get_usage_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\n=== Cost Estimate ===\")\n",
    "    cost = llm_service.estimate_cost()\n",
    "    for key, value in cost.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open(\"backend/app/services/llm_service.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(llm_service_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2642a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\"\"\n",
      "LLM service for question answering\n",
      "RAG Document Assistant - Complete LLM Service\n",
      "\"\"\"\n",
      "from typing import List, Dict, Optional, Iterator\n",
      "from openai import OpenAI\n",
      "from app.config import settings\n",
      "import logging\n",
      "import time\n",
      "import hashlib\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class PromptTemplate:\n",
      "    \"\"\"Collection of prompt templates for different tasks\"\"\"\n",
      "\n",
      "    SYSTEM_QA = \"\"\"You are a helpful AI assistant that answers questions based on provided documents.\n",
      "\n",
      "Instructions:\n",
      "1. Answer using ONLY the provided sources\n",
      "2. If not found, say \"I cannot find this information in the provided documents\"\n",
      "3. Cite sources (e.g., \"According to Source 1...\")\n",
      "4. Be concise but complete\n",
      "5. Synthesize multiple sources if needed\n",
      "6. Do not infer or fabricate\n",
      "7. Maintain a professional tone\"\"\"\n",
      "\n",
      "    SYSTEM_SUMMARIZATION = \"\"\"You are a helpful AI assistant that creates concise summaries of documents.\n",
      "\n",
      "Instructions:\n",
      "1. Summarize the provided text\n",
      "2. Focus on key points\n",
      "3. Be objective and accurate\n",
      "4. Limit to 3–5 sentences\n",
      "5. Use bullet points if appropriate\"\"\"\n",
      "\n",
      "    SYSTEM_COMPARISON = \"\"\"You are a helpful AI assistant that compares information across multiple documents.\n",
      "\n",
      "Instructions:\n",
      "1. Identify similarities and differences\n",
      "2. Reference each source clearly\n",
      "3. Highlight agreements and disagreements\n",
      "4. Be objective and organized\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def create_qa_prompt(question: str, context_chunks: List[Dict]) -> tuple:\n",
      "        context = \"\n",
      "\".join(\n",
      "            f\"[Source {i+1}: {chunk.get('metadata', {}).get('filename', 'Unknown')}, Page {chunk.get('metadata', {}).get('page_number', '?')}]\n",
      "{chunk['text']}\"\n",
      "            for i, chunk in enumerate(context_chunks)\n",
      "        )\n",
      "        user_prompt = f\"\"\"Context from documents:\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Answer the question based on the context provided above. Remember to cite your sources.\"\"\"\n",
      "        return PromptTemplate.SYSTEM_QA, user_prompt\n",
      "\n",
      "    @staticmethod\n",
      "    def create_summarization_prompt(text: str, max_sentences: int = 5) -> tuple:\n",
      "        user_prompt = f\"Please summarize the following text in {max_sentences} sentences or less:\n",
      "\n",
      "{text}\n",
      "\n",
      "Summary:\"\n",
      "        return PromptTemplate.SYSTEM_SUMMARIZATION, user_prompt\n",
      "\n",
      "    @staticmethod\n",
      "    def create_comparison_prompt(question: str, sources_by_doc: Dict[str, List[Dict]]) -> tuple:\n",
      "        context = \"\"\n",
      "        for doc_id, chunks in sources_by_doc.items():\n",
      "            if chunks:\n",
      "                doc_name = chunks[0].get('metadata', {}).get('filename', doc_id)\n",
      "                context += f\"\n",
      "=== From {doc_name} ===\n",
      "\" + \"\n",
      "\".join(chunk['text'] for chunk in chunks)\n",
      "        user_prompt = f\"Compare the information from different documents regarding this question:\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Documents:\n",
      "{context}\n",
      "\n",
      "Provide a comparative analysis:\"\n",
      "        return PromptTemplate.SYSTEM_COMPARISON, user_prompt\n",
      "\n",
      "\n",
      "\n",
      "class ResponseCache:\n",
      "    \"\"\"Simple in-memory cache for LLM responses\"\"\"\n",
      "\n",
      "    def __init__(self, max_size: int = 100):\n",
      "        self.cache = {}\n",
      "        self.max_size = max_size\n",
      "        self.hits = 0\n",
      "        self.misses = 0\n",
      "\n",
      "    def _generate_key(self, question: str, context_ids: List[str], model: str) -> str:\n",
      "        content = f\"{model}:{question}:{','.join(sorted(context_ids))}\"\n",
      "        return hashlib.md5(content.encode()).hexdigest()\n",
      "\n",
      "    def get(self, question: str, context_chunks: List[Dict], model: str) -> Optional[str]:\n",
      "        try:\n",
      "            context_ids = [chunk.get('id', str(i)) for i, chunk in enumerate(context_chunks)]\n",
      "            key = self._generate_key(question, context_ids, model)\n",
      "            if key in self.cache:\n",
      "                self.hits += 1\n",
      "                return self.cache[key]\n",
      "            self.misses += 1\n",
      "            return None\n",
      "        except Exception as e:\n",
      "            logger.warning(f\"Cache retrieval error: {e}\")\n",
      "            return None\n",
      "\n",
      "    def set(self, question: str, context_chunks: List[Dict], model: str, response: str):\n",
      "        try:\n",
      "            context_ids = [chunk.get('id', str(i)) for i, chunk in enumerate(context_chunks)]\n",
      "            key = self._generate_key(question, context_ids, model)\n",
      "            if len(self.cache) >= self.max_size:\n",
      "                self.cache.pop(next(iter(self.cache)))\n",
      "            self.cache[key] = response\n",
      "        except Exception as e:\n",
      "            logger.warning(f\"Cache store error: {e}\")\n",
      "\n",
      "    def get_stats(self) -> dict:\n",
      "        total = self.hits + self.misses\n",
      "        hit_rate = (self.hits / total * 100) if total > 0 else 0\n",
      "        return {\n",
      "            \"cached_items\": len(self.cache),\n",
      "            \"max_size\": self.max_size,\n",
      "            \"hits\": self.hits,\n",
      "            \"misses\": self.misses,\n",
      "            \"hit_rate_percent\": round(hit_rate, 2)\n",
      "        }\n",
      "\n",
      "    def clear(self):\n",
      "        self.cache.clear()\n",
      "        self.hits = 0\n",
      "        self.misses = 0\n",
      "\n",
      "\n",
      "\n",
      "class LLMService:\n",
      "    \"\"\"Handles LLM-based question answering\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.client = None\n",
      "        self.cache = ResponseCache()\n",
      "        self.total_tokens_used = 0\n",
      "        self.total_requests = 0\n",
      "        self._initialize_client()\n",
      "\n",
      "    def _initialize_client(self):\n",
      "        if settings.openai_api_key:\n",
      "            try:\n",
      "                self.client = OpenAI(api_key=settings.openai_api_key)\n",
      "                logger.info(\"OpenAI client initialized\")\n",
      "            except Exception as e:\n",
      "                logger.error(f\"OpenAI init error: {e}\")\n",
      "                self.client = None\n",
      "        else:\n",
      "            logger.warning(\"OpenAI API key missing\")\n",
      "\n",
      "    def generate_answer(self, question: str, context_chunks: List[Dict], model=None, temperature=None, max_tokens=None, use_cache=True) -> str:\n",
      "        if not question.strip():\n",
      "            raise ValueError(\"Question cannot be empty\")\n",
      "        if not context_chunks:\n",
      "            return \"No relevant information available. Please upload documents first.\"\n",
      "\n",
      "        model = model or settings.llm_model\n",
      "        temperature = temperature if temperature is not None else settings.llm_temperature\n",
      "        max_tokens = max_tokens or settings.max_tokens\n",
      "\n",
      "        if use_cache:\n",
      "            cached = self.cache.get(question, context_chunks, model)\n",
      "            if cached:\n",
      "                return cached\n",
      "\n",
      "        if not self.client:\n",
      "            return self.generate_fallback_answer(question, context_chunks)\n",
      "\n",
      "        system_prompt, user_prompt = PromptTemplate.create_qa_prompt(question, context_chunks)\n",
      "\n",
      "        try:\n",
      "            response = self.client.chat.completions.create(\n",
      "                model=model,\n",
      "                messages=[\n",
      "                    {\"role\": \"system\", \"content\": system_prompt},\n",
      "                    {\"role\": \"user\", \"content\": user_prompt}\n",
      "                ],\n",
      "                temperature=temperature,\n",
      "                max_tokens=max_tokens\n",
      "            )\n",
      "            answer = response.choices[0].message.content.strip()\n",
      "            self.total_requests += 1\n",
      "            if hasattr(response, \"usage\"):\n",
      "                self.total_tokens_used += response.usage.total_tokens\n",
      "            if use_cache:\n",
      "                self.cache.set(question, context_chunks, model, answer)\n",
      "            return answer\n",
      "        except Exception as e:\n",
      "            logger.error(f\"LLM error: {e}\")\n",
      "            return self.generate_fallback_answer(question, context_chunks)\n",
      "\n",
      "    def generate_fallback_answer(self, question: str, context_chunks: List[Dict]) -> str:\n",
      "        if not context_chunks:\n",
      "            return \"I couldn't find any relevant information to answer your question.\"\n",
      "        parts = [\"Based on the documents, here's what I found:\n",
      "\"]\n",
      "        for i, chunk in enumerate(context_chunks[:3], 1):\n",
      "            meta = chunk.get(\"metadata\", {})\n",
      "            doc = meta.get(\"filename\", \"Unknown\")\n",
      "            page = meta.get(\"page_number\", \"?\")\n",
      "            text = chunk[\"text\"][:400]\n",
      "            parts.append(f\"\n",
      "**From {doc} (Page {page}):**\n",
      "{text}...\")\n",
      "        parts.append(\"\n",
      "\n",
      "*Note: This is a fallback response. For better answers, configure an OpenAI API key.*\")\n",
      "        return \"\n",
      "\".join(parts)\n",
      "\n",
      "    def summarize_document(self, text: str, max_sentences: int = 5, model: Optional[str] = None) -> str:\n",
      "        if not self.client:\n",
      "            return \"Summary generation requires OpenAI API key.\"\n",
      "        model = model or settings.llm_model\n",
      "        system_prompt, user_prompt = PromptTemplate.create_summarization_prompt(text, max_sentences)\n",
      "        try:\n",
      "            response = self.client.chat.completions.create(\n",
      "                model=model,\n",
      "                messages=[\n",
      "                    {\"role\": \"system\", \"content\": system_prompt},\n",
      "                    {\"role\": \"user\", \"content\": user_prompt}\n",
      "                ],\n",
      "                temperature=0.5,\n",
      "                max_tokens=500\n",
      "            )\n",
      "            return response.choices[0].message.content.strip()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Summary error: {e}\")\n",
      "            return f\"Error generating summary: {str(e)}        except Exception as e:\n",
      "\n",
      "\n",
      "    def compare_documents(self, question: str, sources_by_doc: Dict[str, List[Dict]], model: Optional[str] = None) -> str:\n",
      "        \"\"\"\n",
      "        Generate comparative analysis across multiple documents.\n",
      "        \"\"\"\n",
      "        if not self.client:\n",
      "            return \"Comparison requires OpenAI API key.\"\n",
      "\n",
      "        model = model or settings.llm_model\n",
      "        system_prompt, user_prompt = PromptTemplate.create_comparison_prompt(question, sources_by_doc)\n",
      "\n",
      "        try:\n",
      "            response = self.client.chat.completions.create(\n",
      "                model=model,\n",
      "                messages=[\n",
      "                    {\"role\": \"system\", \"content\": system_prompt},\n",
      "                    {\"role\": \"user\", \"content\": user_prompt}\n",
      "                ],\n",
      "                temperature=0.7,\n",
      "                max_tokens=800\n",
      "            )\n",
      "            return response.choices[0].message.content.strip()\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error generating comparison: {e}\")\n",
      "            return f\"Error generating comparison: {str(e)}\"\n",
      "\n",
      "    def get_usage_stats(self) -> dict:\n",
      "        \"\"\"Get usage statistics.\"\"\"\n",
      "        cache_stats = self.cache.get_stats()\n",
      "        return {\n",
      "            \"total_requests\": self.total_requests,\n",
      "            \"total_tokens_used\": self.total_tokens_used,\n",
      "            \"average_tokens_per_request\": round(self.total_tokens_used / self.total_requests, 2) if self.total_requests > 0 else 0,\n",
      "            \"cache_stats\": cache_stats,\n",
      "            \"client_initialized\": self.client is not None\n",
      "        }\n",
      "\n",
      "    def estimate_cost(self, model: Optional[str] = None) -> dict:\n",
      "        \"\"\"Estimate API cost based on usage.\"\"\"\n",
      "        model = model or settings.llm_model\n",
      "        pricing = {\n",
      "            \"gpt-3.5-turbo\": {\"input\": 0.50, \"output\": 1.50},\n",
      "            \"gpt-4\": {\"input\": 30.00, \"output\": 60.00},\n",
      "            \"gpt-4-turbo\": {\"input\": 10.00, \"output\": 30.00},\n",
      "        }\n",
      "\n",
      "        if model not in pricing:\n",
      "            return {\"error\": f\"Pricing not available for model: {model}\"}\n",
      "\n",
      "        avg_price_per_1m = (pricing[model][\"input\"] + pricing[model][\"output\"]) / 2\n",
      "        estimated_cost = (self.total_tokens_used / 1_000_000) * avg_price_per_1m\n",
      "\n",
      "        return {\n",
      "            \"model\": model,\n",
      "            \"total_tokens\": self.total_tokens_used,\n",
      "            \"estimated_cost_usd\": round(estimated_cost, 4),\n",
      "            \"pricing_per_1m_tokens\": pricing[model]\n",
      "        }\n",
      "\n",
      "    def clear_cache(self):\n",
      "        \"\"\"Clear the response cache.\"\"\"\n",
      "        self.cache.clear()\n",
      "\n",
      "    def reset_stats(self):\n",
      "        \"\"\"Reset usage statistics.\"\"\"\n",
      "        self.total_tokens_used = 0\n",
      "        self.total_requests = 0\n",
      "        logger.info(\"Usage statistics reset\")\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# SINGLETON INSTANCE\n",
      "# ============================================================================\n",
      "\n",
      "llm_service = LLMService()\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# UTILITY FUNCTIONS\n",
      "# ============================================================================\n",
      "\n",
      "def ask_question(question: str, context_chunks: List[Dict], **kwargs) -> str:\n",
      "    \"\"\"\n",
      "    Convenience function to ask a question.\n",
      "    \"\"\"\n",
      "    return llm_service.generate_answer(question, context_chunks, **kwargs)\n",
      "\n",
      "\n",
      "def get_llm_stats() -> dict:\n",
      "    \"\"\"Get LLM service statistics.\"\"\"\n",
      "    return llm_service.get_usage_stats()\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# EXAMPLE USAGE\n",
      "# ============================================================================\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "\n",
      "    example_chunks = [\n",
      "        {\n",
      "            \"text\": \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
      "            \"metadata\": {\"filename\": \"ml_intro.pdf\", \"page_number\": 1, \"document_id\": \"doc1\"}\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Deep learning uses neural networks with multiple layers to process complex patterns.\",\n",
      "            \"metadata\": {\"filename\": \"ml_intro.pdf\", \"page_number\": 2, \"document_id\": \"doc1\"}\n",
      "        }\n",
      "    ]\n",
      "\n",
      "    print(\"\n",
      "=== Testing Question Answering ===\")\n",
      "    question = \"What is machine learning?\"\n",
      "    answer = llm_service.generate_answer(question, example_chunks)\n",
      "    print(f\"Question: {question}\")\n",
      "    print(f\"Answer: {answer}\")\n",
      "\n",
      "    print(\"\n",
      "=== Testing Cache ===\")\n",
      "    answer2 = llm_service.generate_answer(question, example_chunks)\n",
      "    print(\"Second request (should be cached):\")\n",
      "    print(answer2)\n",
      "\n",
      "\n",
      "    print(\"\n",
      "=== Usage Statistics ===\")\n",
      "    stats = llm_service.get_usage_stats()\n",
      "    for key, value in stats.items():\n",
      "        print(f\"{key}: {value}\")\n",
      "\n",
      "    print(\"\n",
      "=== Cost Estimate ===\")\n",
      "    cost = llm_service.estimate_cost()\n",
      "    for key, value in cost.items():\n",
      "        print(f\"{key}: {value}\")\n"
     ]
    }
   ],
   "source": [
    "!type backend\\app\\services\\llm_service.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597bdf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentProcessor service\n",
    "\n",
    "document_processor_code = r\"\"\"\\\"\\\"\\\"\n",
    "Document processing service for extracting and chunking text\n",
    "RAG Document Assistant - Complete Document Processor\n",
    "\\\"\\\"\\\"\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import pypdf\n",
    "from docx import Document as DocxDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from app.config import settings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CHUNKING STRATEGIES\n",
    "# ============================================================================\n",
    "\n",
    "class ChunkingStrategy:\n",
    "    @staticmethod\n",
    "    def semantic_chunking(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"]\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def fixed_chunking(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk)\n",
    "            start += chunk_size - overlap\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def paragraph_chunking(text: str, max_chunk_size: int) -> List[str]:\n",
    "        paragraphs = text.split('\\\\n\\\\n')\n",
    "        chunks, current_chunk, current_size = [], [], 0\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            if current_size + len(para) > max_chunk_size and current_chunk:\n",
    "                chunks.append('\\\\n\\\\n'.join(current_chunk))\n",
    "                current_chunk = [para]\n",
    "                current_size = len(para)\n",
    "            else:\n",
    "                current_chunk.append(para)\n",
    "                current_size += len(para)\n",
    "        if current_chunk:\n",
    "            chunks.append('\\\\n\\\\n'.join(current_chunk))\n",
    "        return chunks\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCleaner:\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        text = re.sub(r'\\\\n{3,}', '\\\\n\\\\n', text)\n",
    "        text = re.sub(r'[^\\w\\\\s\\\\.\\\\,\\\\!\\\\?\\\\;\\\\:\\\\-\\\\(\\\\)\\\n",
    "\n",
    "\\[\\\\]\n",
    "\n",
    "\\\\\"\\\\'\\\\n]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_headers_footers(text: str, patterns: Optional[List[str]] = None) -> str:\n",
    "        patterns = patterns or [\n",
    "            r'Page \\\\d+ of \\\\d+',\n",
    "            r'\\\\d+\\\\s*$',\n",
    "            r'^Copyright ©.*$',\n",
    "            r'^All rights reserved.*$'\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_whitespace(text: str) -> str:\n",
    "        text = text.replace('\\\\t', ' ').replace('\\\\r', '')\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        return '\\\\n'.join(line.strip() for line in text.split('\\\\n'))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DOCUMENT PROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=settings.chunk_size,\n",
    "            chunk_overlap=settings.chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    "        )\n",
    "        self.text_cleaner = TextCleaner()\n",
    "        self.chunking_strategy = ChunkingStrategy()\n",
    "\n",
    "    def generate_document_id(self, filename: str, content: bytes) -> str:\n",
    "        content_hash = hashlib.md5(content).hexdigest()[:8]\n",
    "        clean_name = re.sub(r'[^\\w\\\\-]', '_', Path(filename).stem)[:30]\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "        return f\"{clean_name}_{timestamp}_{content_hash}\"\n",
    "\n",
    "    def extract_text(self, file_path: str, file_type: str) -> Tuple[str, int, Dict]:\n",
    "        if file_type == \".pdf\":\n",
    "            return self.extract_text_from_pdf(file_path)\n",
    "        elif file_type == \".docx\":\n",
    "            return self.extract_text_from_docx(file_path)\n",
    "        elif file_type == \".txt\":\n",
    "            return self.extract_text_from_txt(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "    def extract_text_from_pdf(self, file_path: str) -> Tuple[str, int, Dict]:\n",
    "        try:\n",
    "            text_parts, metadata = [], {'page_sizes': []}\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = pypdf.PdfReader(file)\n",
    "                num_pages = len(reader.pages)\n",
    "                if reader.metadata:\n",
    "                    metadata.update({\n",
    "                        'title': reader.metadata.get('/Title'),\n",
    "                        'author': reader.metadata.get('/Author'),\n",
    "                        'creation_date': reader.metadata.get('/CreationDate')\n",
    "                    })\n",
    "                for i, page in enumerate(reader.pages, 1):\n",
    "                    try:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page.mediabox:\n",
    "                            metadata['page_sizes'].append({\n",
    "                                'width': float(page.mediabox.width),\n",
    "                                'height': float(page.mediabox.height)\n",
    "                            })\n",
    "                        if page_text and page_text.strip():\n",
    "                            cleaned = self.text_cleaner.normalize_whitespace(page_text)\n",
    "                            text_parts.append(f\"\\\\n[Page {i}]\\\\n{cleaned}\\\\n\")\n",
    "                        else:\n",
    "                            text_parts.append(f\"\\\\n[Page {i}]\\\\n[No extractable text]\\\\n\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Page {i} error: {e}\")\n",
    "                        text_parts.append(f\"\\\\n[Page {i}]\\\\n[Extraction error]\\\\n\")\n",
    "            full_text = ''.join(text_parts)\n",
    "            if len(full_text.strip()) < 50:\n",
    "                raise ValueError(\"PDF text too short or unreadable\")\n",
    "            return full_text, num_pages, metadata\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"PDF extraction failed: {e}\")\n",
    "\n",
    "    def extract_text_from_docx(self, file_path: str) -> Tuple[str, int, Dict]:\n",
    "        try:\n",
    "            doc = DocxDocument(file_path)\n",
    "            metadata = {\n",
    "                'title': doc.core_properties.title,\n",
    "                'author': doc.core_properties.author,\n",
    "                'created': doc.core_properties.created,\n",
    "                'modified': doc.core_properties.modified,\n",
    "                'paragraphs': len(doc.paragraphs),\n",
    "                'tables': len(doc.tables)\n",
    "            }\n",
    "            text_parts = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "            for table in doc.tables:\n",
    "                for row in table.rows:\n",
    "                    row_text = ' | '.join(cell.text.strip() for cell in row.cells)\n",
    "                    if row_text:\n",
    "                        text_parts.append(row_text)\n",
    "            full_text = self.text_cleaner.clean_text(self.text_cleaner.normalize_whitespace('\\\\n\\\\n'.join(text_parts)))\n",
    "            word_count = len(full_text.split())\n",
    "            estimated_pages = max(1, word_count // 500)\n",
    "            if len(full_text.strip()) < 50:\n",
    "                raise ValueError(\"DOCX text too short\")\n",
    "            return full_text, estimated_pages, metadata\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"DOCX extraction failed: {e}\")\n",
    "\n",
    "    def extract_text_from_txt(self, file_path: str) -> Tuple[str, int, Dict]:\n",
    "        try:\n",
    "            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "            text, encoding_used = None, None\n",
    "            for enc in encodings:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding=enc) as f:\n",
    "                        text = f.read()\n",
    "                    encoding_used = enc\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            if text is None:\n",
    "                raise ValueError(\"TXT decoding failed\")\n",
    "            text = self.text_cleaner.normalize_whitespace(text)\n",
    "            word_count = len(text.split())\n",
    "            estimated_pages = max(1, word_count // 500)\n",
    "            metadata = {\n",
    "                'encoding': encoding_used,\n",
    "                'size_bytes': os.path.getsize(file_path),\n",
    "                'lines': len(text.split('\\\\n')),\n",
    "                'words': word_count\n",
    "            }\n",
    "            if len(text.strip()) < 10:\n",
    "                raise ValueError(\"TXT file too short\")\n",
    "            return text, estimated_pages, metadata\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"TXT extraction failed: {e}\")\n",
    "\n",
    "    def chunk_text(self, text: str, metadata: Dict, strategy: str = \"semantic\") -> List[Dict]:\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            raise ValueError(\"Text too short to chunk\")\n",
    "        if strategy == \"semantic\":\n",
    "            chunks = self.chunking_strategy.semantic_chunking(text, settings.chunk_size, settings.chunk_overlap)\n",
    "        elif strategy\n",
    "                elif strategy == \"fixed\":\n",
    "            chunks = self.chunking_strategy.fixed_chunking(text, settings.chunk_size, settings.chunk_overlap)\n",
    "        elif strategy == \"paragraph\":\n",
    "            chunks = self.chunking_strategy.paragraph_chunking(text, settings.chunk_size)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown chunking strategy: {strategy}\")\n",
    "\n",
    "        result = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if not chunk_text.strip():\n",
    "                continue\n",
    "            page_num = self._extract_page_number(chunk_text)\n",
    "            result.append({\n",
    "                \"text\": chunk_text.strip(),\n",
    "                \"metadata\": {\n",
    "                    **metadata,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"page_number\": page_num,\n",
    "                    \"chunk_size\": len(chunk_text),\n",
    "                    \"chunk_strategy\": strategy\n",
    "                }\n",
    "            })\n",
    "\n",
    "        if not result:\n",
    "            raise ValueError(\"No valid chunks created from text\")\n",
    "\n",
    "        logger.info(f\"Created {len(result)} chunks using {strategy} strategy\")\n",
    "        return result\n",
    "\n",
    "    def _extract_page_number(self, text: str) -> int:\n",
    "        match = re.search(r'\n",
    "\n",
    "\\[Page (\\d+)\\]\n",
    "\n",
    "', text)\n",
    "        return int(match.group(1)) if match else 1\n",
    "\n",
    "    def process_document(self, file_path: str, filename: str, file_type: str, chunking_strategy: str = \"semantic\", clean_text: bool = True) -> Dict:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            text, num_pages, extraction_metadata = self.extract_text(file_path, file_type)\n",
    "\n",
    "            if clean_text:\n",
    "                text = self.text_cleaner.remove_headers_footers(text)\n",
    "                text = self.text_cleaner.clean_text(text)\n",
    "\n",
    "            if len(text.strip()) < 50:\n",
    "                raise ValueError(\"Extracted text is too short or empty\")\n",
    "\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                content = f.read()\n",
    "            document_id = self.generate_document_id(filename, content)\n",
    "\n",
    "            metadata = {\n",
    "                \"document_id\": document_id,\n",
    "                \"filename\": filename,\n",
    "                \"file_type\": file_type,\n",
    "                \"pages\": num_pages,\n",
    "                \"source\": file_path,\n",
    "                \"processed_at\": datetime.now().isoformat(),\n",
    "                \"file_size\": len(content),\n",
    "                \"text_length\": len(text),\n",
    "                \"word_count\": len(text.split())\n",
    "            }\n",
    "\n",
    "            chunks = self.chunk_text(text, metadata, strategy=chunking_strategy)\n",
    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "            return {\n",
    "                \"document_id\": document_id,\n",
    "                \"text\": text,\n",
    "                \"chunks\": chunks,\n",
    "                \"pages\": num_pages,\n",
    "                \"metadata\": metadata,\n",
    "                \"extraction_metadata\": extraction_metadata,\n",
    "                \"processing_time\": elapsed,\n",
    "                \"chunk_count\": len(chunks)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing document {filename}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def validate_file(self, filename: str, file_size: int) -> None:\n",
    "        ext = Path(filename).suffix.lower()\n",
    "        if ext not in settings.allowed_extensions:\n",
    "            raise ValueError(f\"File type '{ext}' not allowed. Supported formats: {', '.join(settings.allowed_extensions)}\")\n",
    "        if file_size > settings.max_file_size:\n",
    "            raise ValueError(f\"File too large ({file_size / 1024 / 1024:.1f}MB). Max size: {settings.max_file_size / 1024 / 1024:.1f}MB\")\n",
    "        if file_size < 100:\n",
    "            raise ValueError(\"File is too small or empty\")\n",
    "        logger.info(f\"File validation passed: {filename} ({file_size} bytes)\")\n",
    "\n",
    "    def get_supported_formats(self) -> List[str]:\n",
    "        return list(settings.allowed_extensions)\n",
    "\n",
    "    def estimate_processing_time(self, file_size: int, file_type: str) -> float:\n",
    "        rates = {\n",
    "            \".pdf\": 500_000,\n",
    "            \".docx\": 1_000_000,\n",
    "            \".txt\": 2_000_000\n",
    "        }\n",
    "        rate = rates.get(file_type, 500_000)\n",
    "        return file_size / rate\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SINGLETON INSTANCE\n",
    "# ============================================================================\n",
    "\n",
    "document_processor = DocumentProcessor()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def process_file(file_path: str, filename: str) -> Dict:\n",
    "    file_type = Path(filename).suffix.lower()\n",
    "    return document_processor.process_document(file_path, filename, file_type)\n",
    "\n",
    "def validate_upload(filename: str, file_size: int):\n",
    "    document_processor.validate_file(filename, file_size)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    print(\"\\n=== Document Processor Test ===\\n\")\n",
    "\n",
    "    try:\n",
    "        document_processor.validate_file(\"test.pdf\", 1024000)\n",
    "        print(\"✓ Validation passed for test.pdf\")\n",
    "    except ValueError as e:\n",
    "        print(f\"✗ Validation failed: {e}\")\n",
    "\n",
    "    print(f\"\\nSupported formats: {document_processor.get_supported_formats()}\")\n",
    "    est_time = document_processor.estimate_processing_time(2048000, \".pdf\")\n",
    "    print(f\"\\nEstimated processing time for 2MB PDF: {est_time:.2f}s\")\n",
    "    print(\"\\n=== Test Complete ===\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"backend/app/services/document_processor.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(document_processor_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5798c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\"\\\"\\\"\n",
      "Document processing service for extracting and chunking text\n",
      "RAG Document Assistant - Complete Document Processor\n",
      "\\\"\\\"\\\"\n",
      "import os\n",
      "import re\n",
      "import hashlib\n",
      "from pathlib import Path\n",
      "from typing import List, Dict, Tuple, Optional\n",
      "from datetime import datetime\n",
      "import logging\n",
      "\n",
      "import pypdf\n",
      "from docx import Document as DocxDocument\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "\n",
      "from app.config import settings\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# CHUNKING STRATEGIES\n",
      "# ============================================================================\n",
      "\n",
      "class ChunkingStrategy:\n",
      "    @staticmethod\n",
      "    def semantic_chunking(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
      "        splitter = RecursiveCharacterTextSplitter(\n",
      "            chunk_size=chunk_size,\n",
      "            chunk_overlap=overlap,\n",
      "            length_function=len,\n",
      "            separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"]\n",
      "        )\n",
      "        return splitter.split_text(text)\n",
      "\n",
      "    @staticmethod\n",
      "    def fixed_chunking(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
      "        chunks = []\n",
      "        start = 0\n",
      "        while start < len(text):\n",
      "            end = start + chunk_size\n",
      "            chunk = text[start:end]\n",
      "            if chunk.strip():\n",
      "                chunks.append(chunk)\n",
      "            start += chunk_size - overlap\n",
      "        return chunks\n",
      "\n",
      "    @staticmethod\n",
      "    def paragraph_chunking(text: str, max_chunk_size: int) -> List[str]:\n",
      "        paragraphs = text.split('\\\\n\\\\n')\n",
      "        chunks, current_chunk, current_size = [], [], 0\n",
      "        for para in paragraphs:\n",
      "            para = para.strip()\n",
      "            if not para:\n",
      "                continue\n",
      "            if current_size + len(para) > max_chunk_size and current_chunk:\n",
      "                chunks.append('\\\\n\\\\n'.join(current_chunk))\n",
      "                current_chunk = [para]\n",
      "                current_size = len(para)\n",
      "            else:\n",
      "                current_chunk.append(para)\n",
      "                current_size += len(para)\n",
      "        if current_chunk:\n",
      "            chunks.append('\\\\n\\\\n'.join(current_chunk))\n",
      "        return chunks\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# TEXT CLEANING\n",
      "# ============================================================================\n",
      "\n",
      "class TextCleaner:\n",
      "    @staticmethod\n",
      "    def clean_text(text: str) -> str:\n",
      "        text = re.sub(r' +', ' ', text)\n",
      "        text = re.sub(r'\\\\n{3,}', '\\\\n\\\\n', text)\n",
      "        text = re.sub(r'[^\\w\\\\s\\\\.\\\\,\\\\!\\\\?\\\\;\\\\:\\\\-\\\\(\\\\)\\\n",
      "\n",
      "\\[\\\\]\n",
      "\n",
      "\\\\\"\\\\'\\\\n]', '', text)\n",
      "        return text.strip()\n",
      "\n",
      "    @staticmethod\n",
      "    def remove_headers_footers(text: str, patterns: Optional[List[str]] = None) -> str:\n",
      "        patterns = patterns or [\n",
      "            r'Page \\\\d+ of \\\\d+',\n",
      "            r'\\\\d+\\\\s*$',\n",
      "            r'^Copyright ©.*$',\n",
      "            r'^All rights reserved.*$'\n",
      "        ]\n",
      "        for pattern in patterns:\n",
      "            text = re.sub(pattern, '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
      "        return text\n",
      "\n",
      "    @staticmethod\n",
      "    def normalize_whitespace(text: str) -> str:\n",
      "        text = text.replace('\\\\t', ' ').replace('\\\\r', '')\n",
      "        text = re.sub(r' +', ' ', text)\n",
      "        return '\\\\n'.join(line.strip() for line in text.split('\\\\n'))\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# DOCUMENT PROCESSOR\n",
      "# ============================================================================\n",
      "\n",
      "class DocumentProcessor:\n",
      "    def __init__(self):\n",
      "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
      "            chunk_size=settings.chunk_size,\n",
      "            chunk_overlap=settings.chunk_overlap,\n",
      "            length_function=len,\n",
      "            separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
      "        )\n",
      "        self.text_cleaner = TextCleaner()\n",
      "        self.chunking_strategy = ChunkingStrategy()\n",
      "\n",
      "    def generate_document_id(self, filename: str, content: bytes) -> str:\n",
      "        content_hash = hashlib.md5(content).hexdigest()[:8]\n",
      "        clean_name = re.sub(r'[^\\w\\\\-]', '_', Path(filename).stem)[:30]\n",
      "        timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
      "        return f\"{clean_name}_{timestamp}_{content_hash}\"\n",
      "\n",
      "    def extract_text(self, file_path: str, file_type: str) -> Tuple[str, int, Dict]:\n",
      "        if file_type == \".pdf\":\n",
      "            return self.extract_text_from_pdf(file_path)\n",
      "        elif file_type == \".docx\":\n",
      "            return self.extract_text_from_docx(file_path)\n",
      "        elif file_type == \".txt\":\n",
      "            return self.extract_text_from_txt(file_path)\n",
      "        else:\n",
      "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
      "\n",
      "    def extract_text_from_pdf(self, file_path: str) -> Tuple[str, int, Dict]:\n",
      "        try:\n",
      "            text_parts, metadata = [], {'page_sizes': []}\n",
      "            with open(file_path, 'rb') as file:\n",
      "                reader = pypdf.PdfReader(file)\n",
      "                num_pages = len(reader.pages)\n",
      "                if reader.metadata:\n",
      "                    metadata.update({\n",
      "                        'title': reader.metadata.get('/Title'),\n",
      "                        'author': reader.metadata.get('/Author'),\n",
      "                        'creation_date': reader.metadata.get('/CreationDate')\n",
      "                    })\n",
      "                for i, page in enumerate(reader.pages, 1):\n",
      "                    try:\n",
      "                        page_text = page.extract_text()\n",
      "                        if page.mediabox:\n",
      "                            metadata['page_sizes'].append({\n",
      "                                'width': float(page.mediabox.width),\n",
      "                                'height': float(page.mediabox.height)\n",
      "                            })\n",
      "                        if page_text and page_text.strip():\n",
      "                            cleaned = self.text_cleaner.normalize_whitespace(page_text)\n",
      "                            text_parts.append(f\"\\\\n[Page {i}]\\\\n{cleaned}\\\\n\")\n",
      "                        else:\n",
      "                            text_parts.append(f\"\\\\n[Page {i}]\\\\n[No extractable text]\\\\n\")\n",
      "                    except Exception as e:\n",
      "                        logger.warning(f\"Page {i} error: {e}\")\n",
      "                        text_parts.append(f\"\\\\n[Page {i}]\\\\n[Extraction error]\\\\n\")\n",
      "            full_text = ''.join(text_parts)\n",
      "            if len(full_text.strip()) < 50:\n",
      "                raise ValueError(\"PDF text too short or unreadable\")\n",
      "            return full_text, num_pages, metadata\n",
      "        except Exception as e:\n",
      "            raise ValueError(f\"PDF extraction failed: {e}\")\n",
      "\n",
      "    def extract_text_from_docx(self, file_path: str) -> Tuple[str, int, Dict]:\n",
      "        try:\n",
      "            doc = DocxDocument(file_path)\n",
      "            metadata = {\n",
      "                'title': doc.core_properties.title,\n",
      "                'author': doc.core_properties.author,\n",
      "                'created': doc.core_properties.created,\n",
      "                'modified': doc.core_properties.modified,\n",
      "                'paragraphs': len(doc.paragraphs),\n",
      "                'tables': len(doc.tables)\n",
      "            }\n",
      "            text_parts = [p.text for p in doc.paragraphs if p.text.strip()]\n",
      "            for table in doc.tables:\n",
      "                for row in table.rows:\n",
      "                    row_text = ' | '.join(cell.text.strip() for cell in row.cells)\n",
      "                    if row_text:\n",
      "                        text_parts.append(row_text)\n",
      "            full_text = self.text_cleaner.clean_text(self.text_cleaner.normalize_whitespace('\\\\n\\\\n'.join(text_parts)))\n",
      "            word_count = len(full_text.split())\n",
      "            estimated_pages = max(1, word_count // 500)\n",
      "            if len(full_text.strip()) < 50:\n",
      "                raise ValueError(\"DOCX text too short\")\n",
      "            return full_text, estimated_pages, metadata\n",
      "        except Exception as e:\n",
      "            raise ValueError(f\"DOCX extraction failed: {e}\")\n",
      "\n",
      "    def extract_text_from_txt(self, file_path: str) -> Tuple[str, int, Dict]:\n",
      "        try:\n",
      "            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
      "            text, encoding_used = None, None\n",
      "            for enc in encodings:\n",
      "                try:\n",
      "                    with open(file_path, 'r', encoding=enc) as f:\n",
      "                        text = f.read()\n",
      "                    encoding_used = enc\n",
      "                    break\n",
      "                except UnicodeDecodeError:\n",
      "                    continue\n",
      "            if text is None:\n",
      "                raise ValueError(\"TXT decoding failed\")\n",
      "            text = self.text_cleaner.normalize_whitespace(text)\n",
      "            word_count = len(text.split())\n",
      "            estimated_pages = max(1, word_count // 500)\n",
      "            metadata = {\n",
      "                'encoding': encoding_used,\n",
      "                'size_bytes': os.path.getsize(file_path),\n",
      "                'lines': len(text.split('\\\\n')),\n",
      "                'words': word_count\n",
      "            }\n",
      "            if len(text.strip()) < 10:\n",
      "                raise ValueError(\"TXT file too short\")\n",
      "            return text, estimated_pages, metadata\n",
      "        except Exception as e:\n",
      "            raise ValueError(f\"TXT extraction failed: {e}\")\n",
      "\n",
      "    def chunk_text(self, text: str, metadata: Dict, strategy: str = \"semantic\") -> List[Dict]:\n",
      "        if not text or len(text.strip()) < 10:\n",
      "            raise ValueError(\"Text too short to chunk\")\n",
      "        if strategy == \"semantic\":\n",
      "            chunks = self.chunking_strategy.semantic_chunking(text, settings.chunk_size, settings.chunk_overlap)\n",
      "        elif strategy\n",
      "                elif strategy == \"fixed\":\n",
      "            chunks = self.chunking_strategy.fixed_chunking(text, settings.chunk_size, settings.chunk_overlap)\n",
      "        elif strategy == \"paragraph\":\n",
      "            chunks = self.chunking_strategy.paragraph_chunking(text, settings.chunk_size)\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown chunking strategy: {strategy}\")\n",
      "\n",
      "        result = []\n",
      "        for i, chunk_text in enumerate(chunks):\n",
      "            if not chunk_text.strip():\n",
      "                continue\n",
      "            page_num = self._extract_page_number(chunk_text)\n",
      "            result.append({\n",
      "                \"text\": chunk_text.strip(),\n",
      "                \"metadata\": {\n",
      "                    **metadata,\n",
      "                    \"chunk_id\": i,\n",
      "                    \"chunk_index\": i,\n",
      "                    \"page_number\": page_num,\n",
      "                    \"chunk_size\": len(chunk_text),\n",
      "                    \"chunk_strategy\": strategy\n",
      "                }\n",
      "            })\n",
      "\n",
      "        if not result:\n",
      "            raise ValueError(\"No valid chunks created from text\")\n",
      "\n",
      "        logger.info(f\"Created {len(result)} chunks using {strategy} strategy\")\n",
      "        return result\n",
      "\n",
      "    def _extract_page_number(self, text: str) -> int:\n",
      "        match = re.search(r'\n",
      "\n",
      "\\[Page (\\d+)\\]\n",
      "\n",
      "', text)\n",
      "        return int(match.group(1)) if match else 1\n",
      "\n",
      "    def process_document(self, file_path: str, filename: str, file_type: str, chunking_strategy: str = \"semantic\", clean_text: bool = True) -> Dict:\n",
      "        try:\n",
      "            start_time = datetime.now()\n",
      "            text, num_pages, extraction_metadata = self.extract_text(file_path, file_type)\n",
      "\n",
      "            if clean_text:\n",
      "                text = self.text_cleaner.remove_headers_footers(text)\n",
      "                text = self.text_cleaner.clean_text(text)\n",
      "\n",
      "            if len(text.strip()) < 50:\n",
      "                raise ValueError(\"Extracted text is too short or empty\")\n",
      "\n",
      "            with open(file_path, \"rb\") as f:\n",
      "                content = f.read()\n",
      "            document_id = self.generate_document_id(filename, content)\n",
      "\n",
      "            metadata = {\n",
      "                \"document_id\": document_id,\n",
      "                \"filename\": filename,\n",
      "                \"file_type\": file_type,\n",
      "                \"pages\": num_pages,\n",
      "                \"source\": file_path,\n",
      "                \"processed_at\": datetime.now().isoformat(),\n",
      "                \"file_size\": len(content),\n",
      "                \"text_length\": len(text),\n",
      "                \"word_count\": len(text.split())\n",
      "            }\n",
      "\n",
      "            chunks = self.chunk_text(text, metadata, strategy=chunking_strategy)\n",
      "            elapsed = (datetime.now() - start_time).total_seconds()\n",
      "\n",
      "            return {\n",
      "                \"document_id\": document_id,\n",
      "                \"text\": text,\n",
      "                \"chunks\": chunks,\n",
      "                \"pages\": num_pages,\n",
      "                \"metadata\": metadata,\n",
      "                \"extraction_metadata\": extraction_metadata,\n",
      "                \"processing_time\": elapsed,\n",
      "                \"chunk_count\": len(chunks)\n",
      "            }\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error processing document {filename}: {e}\")\n",
      "            raise\n",
      "\n",
      "    def validate_file(self, filename: str, file_size: int) -> None:\n",
      "        ext = Path(filename).suffix.lower()\n",
      "        if ext not in settings.allowed_extensions:\n",
      "            raise ValueError(f\"File type '{ext}' not allowed. Supported formats: {', '.join(settings.allowed_extensions)}\")\n",
      "        if file_size > settings.max_file_size:\n",
      "            raise ValueError(f\"File too large ({file_size / 1024 / 1024:.1f}MB). Max size: {settings.max_file_size / 1024 / 1024:.1f}MB\")\n",
      "        if file_size < 100:\n",
      "            raise ValueError(\"File is too small or empty\")\n",
      "        logger.info(f\"File validation passed: {filename} ({file_size} bytes)\")\n",
      "\n",
      "    def get_supported_formats(self) -> List[str]:\n",
      "        return list(settings.allowed_extensions)\n",
      "\n",
      "    def estimate_processing_time(self, file_size: int, file_type: str) -> float:\n",
      "        rates = {\n",
      "            \".pdf\": 500_000,\n",
      "            \".docx\": 1_000_000,\n",
      "            \".txt\": 2_000_000\n",
      "        }\n",
      "        rate = rates.get(file_type, 500_000)\n",
      "        return file_size / rate\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# SINGLETON INSTANCE\n",
      "# ============================================================================\n",
      "\n",
      "document_processor = DocumentProcessor()\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# UTILITY FUNCTIONS\n",
      "# ============================================================================\n",
      "\n",
      "def process_file(file_path: str, filename: str) -> Dict:\n",
      "    file_type = Path(filename).suffix.lower()\n",
      "    return document_processor.process_document(file_path, filename, file_type)\n",
      "\n",
      "def validate_upload(filename: str, file_size: int):\n",
      "    document_processor.validate_file(filename, file_size)\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# EXAMPLE USAGE\n",
      "# ============================================================================\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    print(\"\\n=== Document Processor Test ===\\n\")\n",
      "\n",
      "    try:\n",
      "        document_processor.validate_file(\"test.pdf\", 1024000)\n",
      "        print(\"✓ Validation passed for test.pdf\")\n",
      "    except ValueError as e:\n",
      "        print(f\"✗ Validation failed: {e}\")\n",
      "\n",
      "    print(f\"\\nSupported formats: {document_processor.get_supported_formats()}\")\n",
      "    est_time = document_processor.estimate_processing_time(2048000, \".pdf\")\n",
      "    print(f\"\\nEstimated processing time for 2MB PDF: {est_time:.2f}s\")\n",
      "    print(\"\\n=== Test Complete ===\\n\")\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!type backend\\app\\services\\document_processor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40b7c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "backend_main = '''\n",
    "\"\"\"\n",
    "FastAPI main application\n",
    "RAG Document Assistant - Complete Backend API\n",
    "\"\"\"\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "from app.config import settings\n",
    "from app.models import (\n",
    "    DocumentUploadResponse,\n",
    "    QuestionRequest,\n",
    "    QuestionResponse,\n",
    "    Source,\n",
    "    DocumentListResponse,\n",
    "    DocumentInfo,\n",
    "    HealthResponse,\n",
    "    ErrorResponse,\n",
    "    SearchRequest,\n",
    "    SearchResponse,\n",
    "    SearchResult,\n",
    "    SystemStatistics,\n",
    "    FeedbackRequest,\n",
    "    FeedbackResponse\n",
    ")\n",
    "from app.services.document_processor import document_processor\n",
    "from app.services.vector_store import vector_store\n",
    "from app.services.llm_service import llm_service\n",
    "from app.services.embeddings import embedding_service\n",
    "\n",
    "# ============================================================================\n",
    "# FASTAPI APP INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "app = FastAPI(\n",
    "    title=settings.app_name,\n",
    "    description=\"\"\"\n",
    "    RAG-based Document Question Answering System\n",
    "    \n",
    "    ## Features\n",
    "    * Upload documents (PDF, DOCX, TXT)\n",
    "    * Semantic search across documents\n",
    "    * Question answering with source citations\n",
    "    * Document management\n",
    "    * Real-time statistics\n",
    "    \n",
    "    ## Endpoints\n",
    "    * `/upload` - Upload and process documents\n",
    "    * `/ask` - Ask questions about documents\n",
    "    * `/search` - Semantic search\n",
    "    * `/documents` - List and manage documents\n",
    "    * `/health` - System health check\n",
    "    \"\"\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\"\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO if not settings.debug else logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('logs/app.log') if Path('logs').exists() else logging.NullHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MIDDLEWARE\n",
    "# ============================================================================\n",
    "\n",
    "# CORS Middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production, specify exact origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Request logging middleware\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    \"\"\"Log all incoming requests\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Log request\n",
    "    logger.info(f\"Request: {request.method} {request.url.path}\")\n",
    "    \n",
    "    try:\n",
    "        response = await call_next(request)\n",
    "        \n",
    "        # Log response\n",
    "        duration = time.time() - start_time\n",
    "        logger.info(\n",
    "            f\"Response: {request.method} {request.url.path} - \"\n",
    "            f\"Status: {response.status_code} - Duration: {duration:.2f}s\"\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Request failed: {request.method} {request.url.path} - Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Exception handler\n",
    "@app.exception_handler(Exception)\n",
    "async def global_exception_handler(request: Request, exc: Exception):\n",
    "    \"\"\"Handle all unhandled exceptions\"\"\"\n",
    "    logger.error(f\"Unhandled exception: {exc}\", exc_info=True)\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\n",
    "            \"error\": \"Internal Server Error\",\n",
    "            \"detail\": str(exc) if settings.debug else \"An unexpected error occurred\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# STARTUP & SHUTDOWN EVENTS\n",
    "# ============================================================================\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Initialize services on startup\"\"\"\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"Starting {settings.app_name}\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    \n",
    "    # Create necessary directories\n",
    "    Path(settings.upload_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(settings.vector_db_path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"logs\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Log configuration\n",
    "    logger.info(f\"Upload directory: {settings.upload_dir}\")\n",
    "    logger.info(f\"Vector DB path: {settings.vector_db_path}\")\n",
    "    logger.info(f\"Max file size: {settings.max_file_size / (1024*1024):.1f}MB\")\n",
    "    logger.info(f\"Allowed extensions: {settings.allowed_extensions}\")\n",
    "    logger.info(f\"LLM Model: {settings.llm_model}\")\n",
    "    logger.info(f\"Embedding Model: {settings.embedding_model}\")\n",
    "    \n",
    "    # Check services\n",
    "    doc_count = vector_store.get_document_count()\n",
    "    logger.info(f\"Documents indexed: {doc_count}\")\n",
    "    \n",
    "    logger.info(\"Application started successfully!\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    \"\"\"Cleanup on shutdown\"\"\"\n",
    "    logger.info(\"Shutting down application...\")\n",
    "    \n",
    "    # Log final statistics\n",
    "    stats = llm_service.get_usage_stats()\n",
    "    logger.info(f\"Total requests processed: {stats['total_requests']}\")\n",
    "    logger.info(f\"Total tokens used: {stats['total_tokens_used']}\")\n",
    "    \n",
    "    logger.info(\"Application shut down successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# ROOT & INFO ENDPOINTS\n",
    "# ============================================================================\n",
    "\n",
    "@app.get(\"/\", response_model=dict, tags=[\"Info\"])\n",
    "async def root():\n",
    "    \"\"\"Root endpoint with API information\"\"\"\n",
    "    return {\n",
    "        \"name\": settings.app_name,\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"description\": \"RAG-based Document Question Answering System\",\n",
    "        \"status\": \"running\",\n",
    "        \"endpoints\": {\n",
    "            \"health\": \"/health\",\n",
    "            \"upload\": \"/upload\",\n",
    "            \"ask\": \"/ask\",\n",
    "            \"search\": \"/search\",\n",
    "            \"documents\": \"/documents\",\n",
    "            \"statistics\": \"/statistics\",\n",
    "            \"documentation\": \"/docs\"\n",
    "        },\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse, tags=[\"Health\"])\n",
    "async def health_check():\n",
    "    \"\"\"\n",
    "    Health check endpoint\n",
    "    \n",
    "    Returns system health status and basic metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check vector store\n",
    "        doc_count = vector_store.get_document_count()\n",
    "        vector_status = \"connected\"\n",
    "        \n",
    "        # Check embedding service\n",
    "        try:\n",
    "            embedding_info = embedding_service.get_model_info()\n",
    "            embedding_status = \"loaded\"\n",
    "        except:\n",
    "            embedding_status = \"error\"\n",
    "        \n",
    "        # Check LLM service\n",
    "        llm_stats = llm_service.get_usage_stats()\n",
    "        \n",
    "        return HealthResponse(\n",
    "            status=\"healthy\",\n",
    "            version=\"1.0.0\",\n",
    "            vector_store_status=vector_status,\n",
    "            documents_indexed=doc_count,\n",
    "            uptime_seconds=time.time()  # Simplified - track actual uptime in production\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Health check failed: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=503, \n",
    "            detail=\"Service unhealthy\"\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# DOCUMENT UPLOAD ENDPOINT\n",
    "# ============================================================================\n",
    "\n",
    "@app.post(\n",
    "    \"/upload\", \n",
    "    response_model=DocumentUploadResponse,\n",
    "    tags=[\"Documents\"],\n",
    "    summary=\"Upload and process a document\"\n",
    ")\n",
    "async def upload_document(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Upload and process a document\n",
    "    \n",
    "    - **file**: PDF, DOCX, or TXT file (max 10MB)\n",
    "    \n",
    "    Returns document metadata and processing status\n",
    "    \"\"\"\n",
    "    upload_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Received file upload: {file.filename}\")\n",
    "        \n",
    "        # Validate filename\n",
    "        if not file.filename:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"Filename is required\"\n",
    "            )\n",
    "        \n",
    "        # Read file content\n",
    "        content = await file.read()\n",
    "        file_size = len(content)\n",
    "        file_type = Path(file.filename).suffix.lower()\n",
    "        \n",
    "        logger.info(f\"File size: {file_size / (1024*1024):.2f}MB\")\n",
    "        \n",
    "        # Validate file\n",
    "        try:\n",
    "            document_processor.validate_file(file.filename, file_size)\n",
    "        except ValueError as e:\n",
    "            raise HTTPException(status_code=400, detail=str(e))\n",
    "        \n",
    "        # Save file temporarily\n",
    "        file_path = Path(settings.upload_dir) / file.filename\n",
    "        \n",
    "        # Handle duplicate filenames\n",
    "        if file_path.exists():\n",
    "            base = file_path.stem\n",
    "            ext = file_path.suffix\n",
    "            counter = 1\n",
    "            while file_path.exists():\n",
    "                file_path = Path(settings.upload_dir) / f\"{base}_{counter}{ext}\"\n",
    "                counter += 1\n",
    "        \n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        logger.info(f\"File saved to: {file_path}\")\n",
    "        \n",
    "        # Process document\n",
    "        try:\n",
    "            result = document_processor.process_document(\n",
    "                str(file_path),\n",
    "                file.filename,\n",
    "                file_type\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Clean up file if processing fails\n",
    "            if file_path.exists():\n",
    "                file_path.unlink()\n",
    "            raise HTTPException(\n",
    "                status_code=422,\n",
    "                detail=f\"Document processing failed: {str(e)}\"\n",
    "            )\n",
    "        \n",
    "        # Add to vector store\n",
    "        try:\n",
    "            num_chunks = vector_store.add_documents(\n",
    "                result[\"chunks\"],\n",
    "                result[\"document_id\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Clean up file if vector store fails\n",
    "            if file_path.exists():\n",
    "                file_path.unlink()\n",
    "            raise HTTPException(\n",
    "                status_code=500,\n",
    "                detail=f\"Failed to index document: {str(e)}\"\n",
    "            )\n",
    "        \n",
    "        # Calculate total time\n",
    "        total_time = time.time() - upload_start\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Document processed successfully: {result['document_id']} - \"\n",
    "            f\"Time: {total_time:.2f}s\"\n",
    "        )\n",
    "        \n",
    "        return DocumentUploadResponse(\n",
    "            document_id=result[\"document_id\"],\n",
    "            filename=file.filename,\n",
    "            pages=result[\"pages\"],\n",
    "            chunks=num_chunks,\n",
    "            status=\"success\",\n",
    "            message=f\"Document uploaded and indexed successfully in {total_time:.2f}s\",\n",
    "            file_size=file_size\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading document: {e}\", exc_info=True)\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to process document: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# QUESTION ANSWERING ENDPOINT\n",
    "# ============================================================================\n",
    "\n",
    "@app.post(\n",
    "    \"/ask\", \n",
    "    response_model=QuestionResponse,\n",
    "    tags=[\"Question Answering\"],\n",
    "    summary=\"Ask a question about documents\"\n",
    ")\n",
    "async def ask_question(request: QuestionRequest):\n",
    "    \"\"\"\n",
    "    Ask a question about uploaded documents\n",
    "    \n",
    "    - **question**: Your question (3-500 characters)\n",
    "    - **document_ids**: Optional list of specific documents to search\n",
    "    - **top_k**: Number of relevant chunks to retrieve (1-10)\n",
    "    - **include_sources**: Whether to include source citations\n",
    "    - **temperature**: LLM temperature (0.0-2.0)\n",
    "    \n",
    "    Returns an answer with source citations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Received question: {request.question[:100]}...\")\n",
    "        \n",
    "        # Check if any documents exist\n",
    "        doc_count = vector_store.get_document_count()\n",
    "        if doc_count == 0:\n",
    "            return QuestionResponse(\n",
    "                answer=\"No documents have been uploaded yet. Please upload some documents first before asking questions.\",\n",
    "                sources=[],\n",
    "                question=request.question,\n",
    "                model_used=\"none\",\n",
    "                processing_time=time.time() - start_time,\n",
    "                tokens_used=0\n",
    "            )\n",
    "        \n",
    "        # Search for relevant chunks\n",
    "        try:\n",
    "            search_results = vector_store.search(\n",
    "                query=request.question,\n",
    "                top_k=request.top_k,\n",
    "                document_ids=request.document_ids\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search failed: {e}\")\n",
    "            raise HTTPException(\n",
    "                status_code=500,\n",
    "                detail=f\"Search failed: {str(e)}\"\n",
    "            )\n",
    "        \n",
    "        if not search_results:\n",
    "            return QuestionResponse(\n",
    "                answer=\"I couldn't find any relevant information to answer your question. Please try rephrasing or check if the relevant documents are uploaded.\",\n",
    "                sources=[],\n",
    "                question=request.question,\n",
    "                model_used=\"none\",\n",
    "                processing_time=time.time() - start_time,\n",
    "                tokens_used=0\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Found {len(search_results)} relevant chunks\")\n",
    "        \n",
    "        # Generate answer\n",
    "        try:\n",
    "            answer = llm_service.generate_answer(\n",
    "                request.question,\n",
    "                search_results,\n",
    "                temperature=request.temperature,\n",
    "                use_cache=True\n",
    "            )\n",
    "            model_used = settings.llm_model\n",
    "            tokens_used = None  # Would track from LLM response\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"LLM generation failed, using fallback: {e}\")\n",
    "            answer = llm_service.generate_fallback_answer(\n",
    "                request.question,\n",
    "                search_results\n",
    "            )\n",
    "            model_used = \"fallback\"\n",
    "            tokens_used = 0\n",
    "        \n",
    "        # Format sources\n",
    "        sources = []\n",
    "        if request.include_sources:\n",
    "            for result in search_results:\n",
    "                metadata = result['metadata']\n",
    "                source = Source(\n",
    "                    document_id=metadata['document_id'],\n",
    "                    document_name=metadata.get('filename', 'Unknown'),\n",
    "                    page_number=metadata.get('page_number'),\n",
    "                    chunk_text=result['text'][:300] + \"...\" if len(result['text']) > 300 else result['text'],\n",
    "                    relevance_score=round(result['score'], 3),\n",
    "                    chunk_id=metadata.get('chunk_id')\n",
    "                )\n",
    "                sources.append(source)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"Question answered in {processing_time:.2f}s\")\n",
    "        \n",
    "        return QuestionResponse(\n",
    "            answer=answer,\n",
    "            sources=sources,\n",
    "            question=request.question,\n",
    "            model_used=model_used,\n",
    "            processing_time=processing_time,\n",
    "            tokens_used=tokens_used\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error answering question: {e}\", exc_info=True)\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to answer question: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# SEMANTIC SEARCH ENDPOINT\n",
    "# ============================================================================\n",
    "\n",
    "@app.post(\n",
    "    \"/search\",\n",
    "    response_model=SearchResponse,\n",
    "    tags=[\"Search\"],\n",
    "    summary=\"Semantic search across documents\"\n",
    ")\n",
    "async def search_documents(request: SearchRequest):\n",
    "    \"\"\"\n",
    "    Perform semantic search across documents without generating an answer\n",
    "    \n",
    "    - **query**: Search query\n",
    "    - **document_ids**: Optional document filter\n",
    "    - **top_k**: Number of results (1-50)\n",
    "    - **similarity_threshold**: Minimum similarity score\n",
    "    \n",
    "    Returns relevant document chunks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Search query: {request.query}\")\n",
    "        \n",
    "        # Perform search\n",
    "        search_results = vector_store.search(\n",
    "            query=request.query,\n",
    "            top_k=request.top_k,\n",
    "            document_ids=request.document_ids\n",
    "        )\n",
    "        \n",
    "        # Filter by similarity threshold if specified\n",
    "        if request.similarity_threshold:\n",
    "            search_results = [\n",
    "                r for r in search_results \n",
    "                if r['score'] >= request.similarity_threshold\n",
    "            ]\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for result in search_results:\n",
    "            metadata = result['metadata']\n",
    "            search_result = SearchResult(\n",
    "                document_id=metadata['document_id'],\n",
    "                document_name=metadata.get('filename', 'Unknown'),\n",
    "                chunk_text=result['text'],\n",
    "                page_number=metadata.get('page_number'),\n",
    "                similarity_score=round(result['score'], 4),\n",
    "                metadata=metadata\n",
    "            )\n",
    "            results.append(search_result)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return SearchResponse(\n",
    "            results=results,\n",
    "            query=request.query,\n",
    "            total_results=len(results),\n",
    "            processing_time=processing_time\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search error: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Search failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# DOCUMENT MANAGEMENT ENDPOINTS\n",
    "# ============================================================================\n",
    "\n",
    "@app.get(\n",
    "    \"/documents\", \n",
    "    response_model=DocumentListResponse,\n",
    "    tags=[\"Documents\"],\n",
    "    summary=\"List all documents\"\n",
    ")\n",
    "async def list_documents():\n",
    "    \"\"\"\n",
    "    List all uploaded documents\n",
    "    \n",
    "    Returns metadata for all indexed documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        docs = vector_store.get_all_documents()\n",
    "        \n",
    "        document_infos = []\n",
    "        for doc in docs:\n",
    "            doc_info = DocumentInfo(\n",
    "                document_id=doc['document_id'],\n",
    "                filename=doc['filename'],\n",
    "                upload_date=None,  # Could add to metadata\n",
    "                pages=doc['pages'],\n",
    "                chunks=doc['chunks'],\n",
    "                file_size=0,  # Could add to metadata\n",
    "                file_type=doc['file_type']\n",
    "            )\n",
    "            document_infos.append(doc_info)\n",
    "        \n",
    "        return DocumentListResponse(\n",
    "            documents=document_infos,\n",
    "            total_count=len(document_infos)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error listing documents: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to list documents: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\n",
    "    \"/documents/{document_id}\",\n",
    "    response_model=DocumentInfo,\n",
    "    tags=[\"Documents\"],\n",
    "    summary=\"Get document details\"\n",
    ")\n",
    "async def get_document(document_id: str):\n",
    "    \"\"\"Get detailed information about a specific document\"\"\"\n",
    "    try:\n",
    "        docs = vector_store.get_all_documents()\n",
    "        doc = next((d for d in docs if d['document_id'] == document_id), None)\n",
    "        \n",
    "        if not doc:\n",
    "            raise HTTPException(\n",
    "                status_code=404,\n",
    "                detail=f\"Document {document_id} not found\"\n",
    "            )\n",
    "        \n",
    "        return DocumentInfo(\n",
    "            document_id=doc['document_id'],\n",
    "            filename=doc['filename'],\n",
    "            upload_date=None,\n",
    "            pages=doc['pages'],\n",
    "            chunks=doc['chunks'],\n",
    "            file_size=0,\n",
    "            file_type=doc['file_type']\n",
    "        )\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting document: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.delete(\n",
    "    \"/documents/{document_id}\",\n",
    "    tags=[\"Documents\"],\n",
    "    summary=\"Delete a document\"\n",
    ")\n",
    "async def delete_document(document_id: str):\n",
    "    \"\"\"\n",
    "    Delete a document and all its chunks\n",
    "    \n",
    "    - **document_id**: ID of document to delete\n",
    "    \n",
    "    Returns deletion confirmation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Deleting document: {document_id}\")\n",
    "        \n",
    "        deleted_count = vector_store.delete_document(document_id)\n",
    "        \n",
    "        if deleted_count == 0:\n",
    "            raise HTTPException(\n",
    "                status_code=404,\n",
    "                detail=f\"Document {document_id} not found\"\n",
    "            )\n",
    "        \n",
    "        # Try to delete the physical file\n",
    "        try:\n",
    "            for file_path in Path(settings.upload_dir).glob(f\"*{document_id}*\"):\n",
    "                file_path.unlink()\n",
    "                logger.info(f\"Deleted file: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not delete physical file: {e}\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"Document {document_id} deleted successfully\",\n",
    "            \"chunks_deleted\": deleted_count,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error deleting document: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to delete document: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICS ENDPOINT\n",
    "# ============================================================================\n",
    "\n",
    "@app.get(\n",
    "    \"/statistics\",\n",
    "    response_model=SystemStatistics,\n",
    "    tags=[\"Statistics\"],\n",
    "    summary=\"Get system statistics\"\n",
    ")\n",
    "async def get_statistics():\n",
    "    \"\"\"\n",
    "    Get overall system statistics\n",
    "    \n",
    "    Returns metrics about documents, queries, and performance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get document stats\n",
    "        all_docs = vector_store.get_all_documents()\n",
    "        total_docs = len(all_docs)\n",
    "        total_chunks = sum(doc['chunks'] for doc in all_docs)\n",
    "        \n",
    "        # Get LLM stats\n",
    "        llm_stats = llm_service.get_usage_stats()\n",
    "        \n",
    "        # Get embedding stats\n",
    "        embedding_info = embedding_service.get_model_info()\n",
    "        \n",
    "        return SystemStatistics(\n",
    "            total_documents=total_docs,\n",
    "            total_chunks=total_chunks,\n",
    "            total_queries=llm_stats['total_requests'],\n",
    "            average_query_time=None,  # Would calculate from logs\n",
    "            average_answer_length=None,\n",
    "            most_queried_documents=None\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting statistics: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ============================================================================\n",
    "# FEEDBACK ENDPOINT\n",
    "# ============================================================================\n",
    "\n",
    "@app.post(\n",
    "    \"/feedback\",\n",
    "    response_model=FeedbackResponse,\n",
    "    tags=[\"Feedback\"],\n",
    "    summary=\"Submit feedback on an answer\"\n",
    ")\n",
    "async def submit_feedback(feedback: FeedbackRequest):\n",
    "    \"\"\"\n",
    "    Submit feedback on a question/answer\n",
    "    \n",
    "    Helps improve the system over time\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # In production, store this in a database\n",
    "        logger.info(\n",
    "            f\"Feedback received - Rating: {feedback.rating}, \"\n",
    "            f\"Helpful: {feedback.helpful}, Accurate: {feedback.accurate}\"\n",
    "        )\n",
    "        \n",
    "        feedback_id = f\"fb_{int(time.time())}\"\n",
    "        \n",
    "        return FeedbackResponse(\n",
    "            message=\"Thank you for your feedback!\",\n",
    "            feedback_id=feedback_id\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error submitting feedback: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY ENDPOINTS\n",
    "# ============================================================================\n",
    "\n",
    "@app.post(\n",
    "    \"/clear-cache\",\n",
    "    tags=[\"Utilities\"],\n",
    "    summary=\"Clear all caches\"\n",
    ")\n",
    "async def clear_caches():\n",
    "    \"\"\"Clear LLM and embedding caches\"\"\"\n",
    "    try:\n",
    "        llm_service.clear_cache()\n",
    "        embedding_service.clear_cache()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"All caches cleared successfully\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error clearing caches: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\n",
    "    \"/system-info\",\n",
    "    tags=[\"Info\"],\n",
    "    summary=\"Get system information\"\n",
    ")\n",
    "async def get_system_info():\n",
    "    \"\"\"Get detailed system information\"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            \"app_name\": settings.app_name,\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"llm_model\": settings.llm_model,\n",
    "            \"embedding_model\": settings.embedding_model,\n",
    "            \"embedding_dimension\": embedding_service.get_embedding_dimension(),\n",
    "            \"max_file_size_mb\": settings.max_file_size / (1024 * 1024),\n",
    "            \"supported_formats\": list(settings.allowed_extensions),\n",
    "            \"chunk_size\": settings.chunk_size,\n",
    "            \"chunk_overlap\": settings.chunk_overlap,\n",
    "            \"vector_store\": \"ChromaDB\",\n",
    "            \"documents_indexed\": vector_store.get_document_count(),\n",
    "            \"llm_stats\": llm_service.get_usage_stats(),\n",
    "            \"cache_stats\": embedding_service.cache.get_cache_stats()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting system info: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ============================================================================\n",
    "# RUN APPLICATION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    \n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\",\n",
    "        reload=settings.debug\n",
    "    )\n",
    "'''\n",
    "\n",
    "# Save to file \n",
    "with open(\"backend/app/main.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(backend_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32ee0f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\"\"\n",
      "FastAPI main application\n",
      "RAG Document Assistant - Complete Backend API\n",
      "\"\"\"\n",
      "from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, Request\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "from fastapi.responses import JSONResponse, StreamingResponse\n",
      "from pathlib import Path\n",
      "import shutil\n",
      "import time\n",
      "from typing import List, Optional\n",
      "import logging\n",
      "import os\n",
      "import asyncio\n",
      "from datetime import datetime\n",
      "\n",
      "from app.config import settings\n",
      "from app.models import (\n",
      "    DocumentUploadResponse,\n",
      "    QuestionRequest,\n",
      "    QuestionResponse,\n",
      "    Source,\n",
      "    DocumentListResponse,\n",
      "    DocumentInfo,\n",
      "    HealthResponse,\n",
      "    ErrorResponse,\n",
      "    SearchRequest,\n",
      "    SearchResponse,\n",
      "    SearchResult,\n",
      "    SystemStatistics,\n",
      "    FeedbackRequest,\n",
      "    FeedbackResponse\n",
      ")\n",
      "from app.services.document_processor import document_processor\n",
      "from app.services.vector_store import vector_store\n",
      "from app.services.llm_service import llm_service\n",
      "from app.services.embeddings import embedding_service\n",
      "\n",
      "# ============================================================================\n",
      "# FASTAPI APP INITIALIZATION\n",
      "# ============================================================================\n",
      "\n",
      "app = FastAPI(\n",
      "    title=settings.app_name,\n",
      "    description=\"\"\"\n",
      "    RAG-based Document Question Answering System\n",
      "\n",
      "    ## Features\n",
      "    * Upload documents (PDF, DOCX, TXT)\n",
      "    * Semantic search across documents\n",
      "    * Question answering with source citations\n",
      "    * Document management\n",
      "    * Real-time statistics\n",
      "\n",
      "    ## Endpoints\n",
      "    * `/upload` - Upload and process documents\n",
      "    * `/ask` - Ask questions about documents\n",
      "    * `/search` - Semantic search\n",
      "    * `/documents` - List and manage documents\n",
      "    * `/health` - System health check\n",
      "    \"\"\",\n",
      "    version=\"1.0.0\",\n",
      "    docs_url=\"/docs\",\n",
      "    redoc_url=\"/redoc\"\n",
      ")\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# LOGGING CONFIGURATION\n",
      "# ============================================================================\n",
      "\n",
      "logging.basicConfig(\n",
      "    level=logging.INFO if not settings.debug else logging.DEBUG,\n",
      "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
      "    handlers=[\n",
      "        logging.StreamHandler(),\n",
      "        logging.FileHandler('logs/app.log') if Path('logs').exists() else logging.NullHandler()\n",
      "    ]\n",
      ")\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# MIDDLEWARE\n",
      "# ============================================================================\n",
      "\n",
      "# CORS Middleware\n",
      "app.add_middleware(\n",
      "    CORSMiddleware,\n",
      "    allow_origins=[\"*\"],  # In production, specify exact origins\n",
      "    allow_credentials=True,\n",
      "    allow_methods=[\"*\"],\n",
      "    allow_headers=[\"*\"],\n",
      ")\n",
      "\n",
      "# Request logging middleware\n",
      "@app.middleware(\"http\")\n",
      "async def log_requests(request: Request, call_next):\n",
      "    \"\"\"Log all incoming requests\"\"\"\n",
      "    start_time = time.time()\n",
      "\n",
      "    # Log request\n",
      "    logger.info(f\"Request: {request.method} {request.url.path}\")\n",
      "\n",
      "    try:\n",
      "        response = await call_next(request)\n",
      "\n",
      "        # Log response\n",
      "        duration = time.time() - start_time\n",
      "        logger.info(\n",
      "            f\"Response: {request.method} {request.url.path} - \"\n",
      "            f\"Status: {response.status_code} - Duration: {duration:.2f}s\"\n",
      "        )\n",
      "\n",
      "        return response\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Request failed: {request.method} {request.url.path} - Error: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "# Exception handler\n",
      "@app.exception_handler(Exception)\n",
      "async def global_exception_handler(request: Request, exc: Exception):\n",
      "    \"\"\"Handle all unhandled exceptions\"\"\"\n",
      "    logger.error(f\"Unhandled exception: {exc}\", exc_info=True)\n",
      "    return JSONResponse(\n",
      "        status_code=500,\n",
      "        content={\n",
      "            \"error\": \"Internal Server Error\",\n",
      "            \"detail\": str(exc) if settings.debug else \"An unexpected error occurred\",\n",
      "            \"timestamp\": datetime.now().isoformat()\n",
      "        }\n",
      "    )\n",
      "\n",
      "# ============================================================================\n",
      "# STARTUP & SHUTDOWN EVENTS\n",
      "# ============================================================================\n",
      "\n",
      "@app.on_event(\"startup\")\n",
      "async def startup_event():\n",
      "    \"\"\"Initialize services on startup\"\"\"\n",
      "    logger.info(\"=\" * 50)\n",
      "    logger.info(f\"Starting {settings.app_name}\")\n",
      "    logger.info(\"=\" * 50)\n",
      "\n",
      "    # Create necessary directories\n",
      "    Path(settings.upload_dir).mkdir(parents=True, exist_ok=True)\n",
      "    Path(settings.vector_db_path).mkdir(parents=True, exist_ok=True)\n",
      "    Path(\"logs\").mkdir(exist_ok=True)\n",
      "\n",
      "    # Log configuration\n",
      "    logger.info(f\"Upload directory: {settings.upload_dir}\")\n",
      "    logger.info(f\"Vector DB path: {settings.vector_db_path}\")\n",
      "    logger.info(f\"Max file size: {settings.max_file_size / (1024*1024):.1f}MB\")\n",
      "    logger.info(f\"Allowed extensions: {settings.allowed_extensions}\")\n",
      "    logger.info(f\"LLM Model: {settings.llm_model}\")\n",
      "    logger.info(f\"Embedding Model: {settings.embedding_model}\")\n",
      "\n",
      "    # Check services\n",
      "    doc_count = vector_store.get_document_count()\n",
      "    logger.info(f\"Documents indexed: {doc_count}\")\n",
      "\n",
      "    logger.info(\"Application started successfully!\")\n",
      "    logger.info(\"=\" * 50)\n",
      "\n",
      "@app.on_event(\"shutdown\")\n",
      "async def shutdown_event():\n",
      "    \"\"\"Cleanup on shutdown\"\"\"\n",
      "    logger.info(\"Shutting down application...\")\n",
      "\n",
      "    # Log final statistics\n",
      "    stats = llm_service.get_usage_stats()\n",
      "    logger.info(f\"Total requests processed: {stats['total_requests']}\")\n",
      "    logger.info(f\"Total tokens used: {stats['total_tokens_used']}\")\n",
      "\n",
      "    logger.info(\"Application shut down successfully\")\n",
      "\n",
      "# ============================================================================\n",
      "# ROOT & INFO ENDPOINTS\n",
      "# ============================================================================\n",
      "\n",
      "@app.get(\"/\", response_model=dict, tags=[\"Info\"])\n",
      "async def root():\n",
      "    \"\"\"Root endpoint with API information\"\"\"\n",
      "    return {\n",
      "        \"name\": settings.app_name,\n",
      "        \"version\": \"1.0.0\",\n",
      "        \"description\": \"RAG-based Document Question Answering System\",\n",
      "        \"status\": \"running\",\n",
      "        \"endpoints\": {\n",
      "            \"health\": \"/health\",\n",
      "            \"upload\": \"/upload\",\n",
      "            \"ask\": \"/ask\",\n",
      "            \"search\": \"/search\",\n",
      "            \"documents\": \"/documents\",\n",
      "            \"statistics\": \"/statistics\",\n",
      "            \"documentation\": \"/docs\"\n",
      "        },\n",
      "        \"timestamp\": datetime.now().isoformat()\n",
      "    }\n",
      "\n",
      "@app.get(\"/health\", response_model=HealthResponse, tags=[\"Health\"])\n",
      "async def health_check():\n",
      "    \"\"\"\n",
      "    Health check endpoint\n",
      "\n",
      "    Returns system health status and basic metrics\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Check vector store\n",
      "        doc_count = vector_store.get_document_count()\n",
      "        vector_status = \"connected\"\n",
      "\n",
      "        # Check embedding service\n",
      "        try:\n",
      "            embedding_info = embedding_service.get_model_info()\n",
      "            embedding_status = \"loaded\"\n",
      "        except:\n",
      "            embedding_status = \"error\"\n",
      "\n",
      "        # Check LLM service\n",
      "        llm_stats = llm_service.get_usage_stats()\n",
      "\n",
      "        return HealthResponse(\n",
      "            status=\"healthy\",\n",
      "            version=\"1.0.0\",\n",
      "            vector_store_status=vector_status,\n",
      "            documents_indexed=doc_count,\n",
      "            uptime_seconds=time.time()  # Simplified - track actual uptime in production\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Health check failed: {e}\")\n",
      "        raise HTTPException(\n",
      "            status_code=503, \n",
      "            detail=\"Service unhealthy\"\n",
      "        )\n",
      "\n",
      "# ============================================================================\n",
      "# DOCUMENT UPLOAD ENDPOINT\n",
      "# ============================================================================\n",
      "\n",
      "@app.post(\n",
      "    \"/upload\", \n",
      "    response_model=DocumentUploadResponse,\n",
      "    tags=[\"Documents\"],\n",
      "    summary=\"Upload and process a document\"\n",
      ")\n",
      "async def upload_document(file: UploadFile = File(...)):\n",
      "    \"\"\"\n",
      "    Upload and process a document\n",
      "\n",
      "    - **file**: PDF, DOCX, or TXT file (max 10MB)\n",
      "\n",
      "    Returns document metadata and processing status\n",
      "    \"\"\"\n",
      "    upload_start = time.time()\n",
      "\n",
      "    try:\n",
      "        logger.info(f\"Received file upload: {file.filename}\")\n",
      "\n",
      "        # Validate filename\n",
      "        if not file.filename:\n",
      "            raise HTTPException(\n",
      "                status_code=400,\n",
      "                detail=\"Filename is required\"\n",
      "            )\n",
      "\n",
      "        # Read file content\n",
      "        content = await file.read()\n",
      "        file_size = len(content)\n",
      "        file_type = Path(file.filename).suffix.lower()\n",
      "\n",
      "        logger.info(f\"File size: {file_size / (1024*1024):.2f}MB\")\n",
      "\n",
      "        # Validate file\n",
      "        try:\n",
      "            document_processor.validate_file(file.filename, file_size)\n",
      "        except ValueError as e:\n",
      "            raise HTTPException(status_code=400, detail=str(e))\n",
      "\n",
      "        # Save file temporarily\n",
      "        file_path = Path(settings.upload_dir) / file.filename\n",
      "\n",
      "        # Handle duplicate filenames\n",
      "        if file_path.exists():\n",
      "            base = file_path.stem\n",
      "            ext = file_path.suffix\n",
      "            counter = 1\n",
      "            while file_path.exists():\n",
      "                file_path = Path(settings.upload_dir) / f\"{base}_{counter}{ext}\"\n",
      "                counter += 1\n",
      "\n",
      "        with open(file_path, \"wb\") as f:\n",
      "            f.write(content)\n",
      "\n",
      "        logger.info(f\"File saved to: {file_path}\")\n",
      "\n",
      "        # Process document\n",
      "        try:\n",
      "            result = document_processor.process_document(\n",
      "                str(file_path),\n",
      "                file.filename,\n",
      "                file_type\n",
      "            )\n",
      "        except Exception as e:\n",
      "            # Clean up file if processing fails\n",
      "            if file_path.exists():\n",
      "                file_path.unlink()\n",
      "            raise HTTPException(\n",
      "                status_code=422,\n",
      "                detail=f\"Document processing failed: {str(e)}\"\n",
      "            )\n",
      "\n",
      "        # Add to vector store\n",
      "        try:\n",
      "            num_chunks = vector_store.add_documents(\n",
      "                result[\"chunks\"],\n",
      "                result[\"document_id\"]\n",
      "            )\n",
      "        except Exception as e:\n",
      "            # Clean up file if vector store fails\n",
      "            if file_path.exists():\n",
      "                file_path.unlink()\n",
      "            raise HTTPException(\n",
      "                status_code=500,\n",
      "                detail=f\"Failed to index document: {str(e)}\"\n",
      "            )\n",
      "\n",
      "        # Calculate total time\n",
      "        total_time = time.time() - upload_start\n",
      "\n",
      "        logger.info(\n",
      "            f\"Document processed successfully: {result['document_id']} - \"\n",
      "            f\"Time: {total_time:.2f}s\"\n",
      "        )\n",
      "\n",
      "        return DocumentUploadResponse(\n",
      "            document_id=result[\"document_id\"],\n",
      "            filename=file.filename,\n",
      "            pages=result[\"pages\"],\n",
      "            chunks=num_chunks,\n",
      "            status=\"success\",\n",
      "            message=f\"Document uploaded and indexed successfully in {total_time:.2f}s\",\n",
      "            file_size=file_size\n",
      "        )\n",
      "\n",
      "    except HTTPException:\n",
      "        raise\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error uploading document: {e}\", exc_info=True)\n",
      "        raise HTTPException(\n",
      "            status_code=500,\n",
      "            detail=f\"Failed to process document: {str(e)}\"\n",
      "        )\n",
      "\n",
      "# ============================================================================\n",
      "# QUESTION ANSWERING ENDPOINT\n",
      "# ============================================================================\n",
      "\n",
      "@app.post(\n",
      "    \"/ask\", \n",
      "    response_model=QuestionResponse,\n",
      "    tags=[\"Question Answering\"],\n",
      "    summary=\"Ask a question about documents\"\n",
      ")\n",
      "async def ask_question(request: QuestionRequest):\n",
      "    \"\"\"\n",
      "    Ask a question about uploaded documents\n",
      "\n",
      "    - **question**: Your question (3-500 characters)\n",
      "    - **document_ids**: Optional list of specific documents to search\n",
      "    - **top_k**: Number of relevant chunks to retrieve (1-10)\n",
      "    - **include_sources**: Whether to include source citations\n",
      "    - **temperature**: LLM temperature (0.0-2.0)\n",
      "\n",
      "    Returns an answer with source citations\n",
      "    \"\"\"\n",
      "    try:\n",
      "        start_time = time.time()\n",
      "        logger.info(f\"Received question: {request.question[:100]}...\")\n",
      "\n",
      "        # Check if any documents exist\n",
      "        doc_count = vector_store.get_document_count()\n",
      "        if doc_count == 0:\n",
      "            return QuestionResponse(\n",
      "                answer=\"No documents have been uploaded yet. Please upload some documents first before asking questions.\",\n",
      "                sources=[],\n",
      "                question=request.question,\n",
      "                model_used=\"none\",\n",
      "                processing_time=time.time() - start_time,\n",
      "                tokens_used=0\n",
      "            )\n",
      "\n",
      "        # Search for relevant chunks\n",
      "        try:\n",
      "            search_results = vector_store.search(\n",
      "                query=request.question,\n",
      "                top_k=request.top_k,\n",
      "                document_ids=request.document_ids\n",
      "            )\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Search failed: {e}\")\n",
      "            raise HTTPException(\n",
      "                status_code=500,\n",
      "                detail=f\"Search failed: {str(e)}\"\n",
      "            )\n",
      "\n",
      "        if not search_results:\n",
      "            return QuestionResponse(\n",
      "                answer=\"I couldn't find any relevant information to answer your question. Please try rephrasing or check if the relevant documents are uploaded.\",\n",
      "                sources=[],\n",
      "                question=request.question,\n",
      "                model_used=\"none\",\n",
      "                processing_time=time.time() - start_time,\n",
      "                tokens_used=0\n",
      "            )\n",
      "\n",
      "        logger.info(f\"Found {len(search_results)} relevant chunks\")\n",
      "\n",
      "        # Generate answer\n",
      "        try:\n",
      "            answer = llm_service.generate_answer(\n",
      "                request.question,\n",
      "                search_results,\n",
      "                temperature=request.temperature,\n",
      "                use_cache=True\n",
      "            )\n",
      "            model_used = settings.llm_model\n",
      "            tokens_used = None  # Would track from LLM response\n",
      "        except Exception as e:\n",
      "            logger.warning(f\"LLM generation failed, using fallback: {e}\")\n",
      "            answer = llm_service.generate_fallback_answer(\n",
      "                request.question,\n",
      "                search_results\n",
      "            )\n",
      "            model_used = \"fallback\"\n",
      "            tokens_used = 0\n",
      "\n",
      "        # Format sources\n",
      "        sources = []\n",
      "        if request.include_sources:\n",
      "            for result in search_results:\n",
      "                metadata = result['metadata']\n",
      "                source = Source(\n",
      "                    document_id=metadata['document_id'],\n",
      "                    document_name=metadata.get('filename', 'Unknown'),\n",
      "                    page_number=metadata.get('page_number'),\n",
      "                    chunk_text=result['text'][:300] + \"...\" if len(result['text']) > 300 else result['text'],\n",
      "                    relevance_score=round(result['score'], 3),\n",
      "                    chunk_id=metadata.get('chunk_id')\n",
      "                )\n",
      "                sources.append(source)\n",
      "\n",
      "        processing_time = time.time() - start_time\n",
      "        logger.info(f\"Question answered in {processing_time:.2f}s\")\n",
      "\n",
      "        return QuestionResponse(\n",
      "            answer=answer,\n",
      "            sources=sources,\n",
      "            question=request.question,\n",
      "            model_used=model_used,\n",
      "            processing_time=processing_time,\n",
      "            tokens_used=tokens_used\n",
      "        )\n",
      "\n",
      "    except HTTPException:\n",
      "        raise\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error answering question: {e}\", exc_info=True)\n",
      "        raise HTTPException(\n",
      "            status_code=500,\n",
      "            detail=f\"Failed to answer question: {str(e)}\"\n",
      "        )\n",
      "\n",
      "# ============================================================================\n",
      "# SEMANTIC SEARCH ENDPOINT\n",
      "# ============================================================================\n",
      "\n",
      "@app.post(\n",
      "    \"/search\",\n",
      "    response_model=SearchResponse,\n",
      "    tags=[\"Search\"],\n",
      "    summary=\"Semantic search across documents\"\n",
      ")\n",
      "async def search_documents(request: SearchRequest):\n",
      "    \"\"\"\n",
      "    Perform semantic search across documents without generating an answer\n",
      "\n",
      "    - **query**: Search query\n",
      "    - **document_ids**: Optional document filter\n",
      "    - **top_k**: Number of results (1-50)\n",
      "    - **similarity_threshold**: Minimum similarity score\n",
      "\n",
      "    Returns relevant document chunks\n",
      "    \"\"\"\n",
      "    try:\n",
      "        start_time = time.time()\n",
      "        logger.info(f\"Search query: {request.query}\")\n",
      "\n",
      "        # Perform search\n",
      "        search_results = vector_store.search(\n",
      "            query=request.query,\n",
      "            top_k=request.top_k,\n",
      "            document_ids=request.document_ids\n",
      "        )\n",
      "\n",
      "        # Filter by similarity threshold if specified\n",
      "        if request.similarity_threshold:\n",
      "            search_results = [\n",
      "                r for r in search_results \n",
      "                if r['score'] >= request.similarity_threshold\n",
      "            ]\n",
      "\n",
      "        # Format results\n",
      "        results = []\n",
      "        for result in search_results:\n",
      "            metadata = result['metadata']\n",
      "            search_result = SearchResult(\n",
      "                document_id=metadata['document_id'],\n",
      "                document_name=metadata.get('filename', 'Unknown'),\n",
      "                chunk_text=result['text'],\n",
      "                page_number=metadata.get('page_number'),\n",
      "                similarity_score=round(result['score'], 4),\n",
      "                metadata=metadata\n",
      "            )\n",
      "            results.append(search_result)\n",
      "\n",
      "        processing_time = time.time() - start_time\n",
      "\n",
      "        return SearchResponse(\n",
      "            results=results,\n",
      "            query=request.query,\n",
      "            total_results=len(results),\n",
      "            processing_time=processing_time\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Search error: {e}\")\n",
      "        raise HTTPException(\n",
      "            status_code=500,\n",
      "            detail=f\"Search failed: {str(e)}\"\n",
      "        )\n",
      "\n",
      "# ============================================================================\n",
      "# DOCUMENT MANAGEMENT ENDPOINTS\n",
      "# ============================================================================\n",
      "\n",
      "@app.get(\n",
      "    \"/documents\", \n",
      "    response_model=DocumentListResponse,\n",
      "    tags=[\"Documents\"],\n",
      "    summary=\"List all documents\"\n",
      ")\n",
      "async def list_documents():\n",
      "    \"\"\"\n",
      "    List all uploaded documents\n",
      "\n",
      "    Returns metadata for all indexed documents\n",
      "    \"\"\"\n",
      "    try:\n",
      "        docs = vector_store.get_all_documents()\n",
      "\n",
      "        document_infos = []\n",
      "        for doc in docs:\n",
      "            doc_info = DocumentInfo(\n",
      "                document_id=doc['document_id'],\n",
      "                filename=doc['filename'],\n",
      "                upload_date=None,  # Could add to metadata\n",
      "                pages=doc['pages'],\n",
      "                chunks=doc['chunks'],\n",
      "                file_size=0,  # Could add to metadata\n",
      "                file_type=doc['file_type']\n",
      "            )\n",
      "            document_infos.append(doc_info)\n",
      "\n",
      "        return DocumentListResponse(\n",
      "            documents=document_infos,\n",
      "            total_count=len(document_infos)\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error listing documents: {e}\")\n",
      "        raise HTTPException(\n",
      "            status_code=500,\n",
      "            detail=f\"Failed to list documents: {str(e)}\"\n",
      "        )\n",
      "\n",
      "@app.get(\n",
      "    \"/documents/{document_id}\",\n",
      "    response_model=DocumentInfo,\n",
      "    tags=[\"Documents\"],\n",
      "    summary=\"Get document details\"\n",
      ")\n",
      "async def get_document(document_id: str):\n",
      "    \"\"\"Get detailed information about a specific document\"\"\"\n",
      "    try:\n",
      "        docs = vector_store.get_all_documents()\n",
      "        doc = next((d for d in docs if d['document_id'] == document_id), None)\n",
      "\n",
      "        if not doc:\n",
      "            raise HTTPException(\n",
      "                status_code=404,\n",
      "                detail=f\"Document {document_id} not found\"\n",
      "            )\n",
      "\n",
      "        return DocumentInfo(\n",
      "            document_id=doc['document_id'],\n",
      "            filename=doc['filename'],\n",
      "            upload_date=None,\n",
      "            pages=doc['pages'],\n",
      "            chunks=doc['chunks'],\n",
      "            file_size=0,\n",
      "            file_type=doc['file_type']\n",
      "        )\n",
      "\n",
      "    except HTTPException:\n",
      "        raise\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error getting document: {e}\")\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "\n",
      "@app.delete(\n",
      "    \"/documents/{document_id}\",\n",
      "    tags=[\"Documents\"],\n",
      "    summary=\"Delete a document\"\n",
      ")\n",
      "async def delete_document(document_id: str):\n",
      "    \"\"\"\n",
      "    Delete a document and all its chunks\n",
      "\n",
      "    - **document_id**: ID of document to delete\n",
      "\n",
      "    Returns deletion confirmation\n",
      "    \"\"\"\n",
      "    try:\n",
      "        logger.info(f\"Deleting document: {document_id}\")\n",
      "\n",
      "        deleted_count = vector_store.delete_document(document_id)\n",
      "\n",
      "        if deleted_count == 0:\n",
      "            raise HTTPException(\n",
      "                status_code=404,\n",
      "                detail=f\"Document {document_id} not found\"\n",
      "            )\n",
      "\n",
      "        # Try to delete the physical file\n",
      "        try:\n",
      "            for file_path in Path(settings.upload_dir).glob(f\"*{document_id}*\"):\n",
      "                file_path.unlink()\n",
      "                logger.info(f\"Deleted file: {file_path}\")\n",
      "        except Exception as e:\n",
      "            logger.warning(f\"Could not delete physical file: {e}\")\n",
      "\n",
      "        return {\n",
      "            \"success\": True,\n",
      "            \"message\": f\"Document {document_id} deleted successfully\",\n",
      "            \"chunks_deleted\": deleted_count,\n",
      "            \"timestamp\": datetime.now().isoformat()\n",
      "        }\n",
      "\n",
      "    except HTTPException:\n",
      "        raise\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error deleting document: {e}\")\n",
      "        raise HTTPException(\n",
      "            status_code=500,\n",
      "            detail=f\"Failed to delete document: {str(e)}\"\n",
      "        )\n",
      "\n",
      "# ============================================================================\n",
      "# STATISTICS ENDPOINT\n",
      "# ============================================================================\n",
      "\n",
      "@app.get(\n",
      "    \"/statistics\",\n",
      "    response_model=SystemStatistics,\n",
      "    tags=[\"Statistics\"],\n",
      "    summary=\"Get system statistics\"\n",
      ")\n",
      "async def get_statistics():\n",
      "    \"\"\"\n",
      "    Get overall system statistics\n",
      "\n",
      "    Returns metrics about documents, queries, and performance\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Get document stats\n",
      "        all_docs = vector_store.get_all_documents()\n",
      "        total_docs = len(all_docs)\n",
      "        total_chunks = sum(doc['chunks'] for doc in all_docs)\n",
      "\n",
      "        # Get LLM stats\n",
      "        llm_stats = llm_service.get_usage_stats()\n",
      "\n",
      "        # Get embedding stats\n",
      "        embedding_info = embedding_service.get_model_info()\n",
      "\n",
      "        return SystemStatistics(\n",
      "            total_documents=total_docs,\n",
      "            total_chunks=total_chunks,\n",
      "            total_queries=llm_stats['total_requests'],\n",
      "            average_query_time=None,  # Would calculate from logs\n",
      "            average_answer_length=None,\n",
      "            most_queried_documents=None\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error getting statistics: {e}\")\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "\n",
      "# ============================================================================\n",
      "# FEEDBACK ENDPOINT\n",
      "# ============================================================================\n",
      "\n",
      "@app.post(\n",
      "    \"/feedback\",\n",
      "    response_model=FeedbackResponse,\n",
      "    tags=[\"Feedback\"],\n",
      "    summary=\"Submit feedback on an answer\"\n",
      ")\n",
      "async def submit_feedback(feedback: FeedbackRequest):\n",
      "    \"\"\"\n",
      "    Submit feedback on a question/answer\n",
      "\n",
      "    Helps improve the system over time\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # In production, store this in a database\n",
      "        logger.info(\n",
      "            f\"Feedback received - Rating: {feedback.rating}, \"\n",
      "            f\"Helpful: {feedback.helpful}, Accurate: {feedback.accurate}\"\n",
      "        )\n",
      "\n",
      "        feedback_id = f\"fb_{int(time.time())}\"\n",
      "\n",
      "        return FeedbackResponse(\n",
      "            message=\"Thank you for your feedback!\",\n",
      "            feedback_id=feedback_id\n",
      "        )\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error submitting feedback: {e}\")\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "\n",
      "# ============================================================================\n",
      "# UTILITY ENDPOINTS\n",
      "# ============================================================================\n",
      "\n",
      "@app.post(\n",
      "    \"/clear-cache\",\n",
      "    tags=[\"Utilities\"],\n",
      "    summary=\"Clear all caches\"\n",
      ")\n",
      "async def clear_caches():\n",
      "    \"\"\"Clear LLM and embedding caches\"\"\"\n",
      "    try:\n",
      "        llm_service.clear_cache()\n",
      "        embedding_service.clear_cache()\n",
      "\n",
      "        return {\n",
      "            \"success\": True,\n",
      "            \"message\": \"All caches cleared successfully\",\n",
      "            \"timestamp\": datetime.now().isoformat()\n",
      "        }\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error clearing caches: {e}\")\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "\n",
      "@app.get(\n",
      "    \"/system-info\",\n",
      "    tags=[\"Info\"],\n",
      "    summary=\"Get system information\"\n",
      ")\n",
      "async def get_system_info():\n",
      "    \"\"\"Get detailed system information\"\"\"\n",
      "    try:\n",
      "        return {\n",
      "            \"app_name\": settings.app_name,\n",
      "            \"version\": \"1.0.0\",\n",
      "            \"llm_model\": settings.llm_model,\n",
      "            \"embedding_model\": settings.embedding_model,\n",
      "            \"embedding_dimension\": embedding_service.get_embedding_dimension(),\n",
      "            \"max_file_size_mb\": settings.max_file_size / (1024 * 1024),\n",
      "            \"supported_formats\": list(settings.allowed_extensions),\n",
      "            \"chunk_size\": settings.chunk_size,\n",
      "            \"chunk_overlap\": settings.chunk_overlap,\n",
      "            \"vector_store\": \"ChromaDB\",\n",
      "            \"documents_indexed\": vector_store.get_document_count(),\n",
      "            \"llm_stats\": llm_service.get_usage_stats(),\n",
      "            \"cache_stats\": embedding_service.cache.get_cache_stats()\n",
      "        }\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error getting system info: {e}\")\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "\n",
      "# ============================================================================\n",
      "# RUN APPLICATION\n",
      "# ============================================================================\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import uvicorn\n",
      "\n",
      "    uvicorn.run(\n",
      "        app,\n",
      "        host=\"0.0.0.0\",\n",
      "        port=8000,\n",
      "        log_level=\"info\",\n",
      "        reload=settings.debug\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "!type backend\\app\\main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07e7dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "model_file = '''\n",
    "\"\"\"\n",
    "Pydantic models for request/response validation\n",
    "RAG Document Assistant - Complete Models File\n",
    "\"\"\"\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENUMS\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentStatus(str, Enum):\n",
    "    \"\"\"Document processing status\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    PROCESSING = \"processing\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "\n",
    "class FileType(str, Enum):\n",
    "    \"\"\"Supported file types\"\"\"\n",
    "    PDF = \".pdf\"\n",
    "    DOCX = \".docx\"\n",
    "    TXT = \".txt\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DOCUMENT MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentUploadResponse(BaseModel):\n",
    "    \"\"\"Response after uploading a document\"\"\"\n",
    "    document_id: str = Field(..., description=\"Unique document identifier\")\n",
    "    filename: str = Field(..., description=\"Original filename\")\n",
    "    pages: int = Field(..., ge=0, description=\"Number of pages\")\n",
    "    chunks: int = Field(..., ge=0, description=\"Number of text chunks created\")\n",
    "    status: str = Field(..., description=\"Processing status\")\n",
    "    message: str = Field(..., description=\"Status message\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "    file_size: Optional[int] = Field(None, description=\"File size in bytes\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"document_id\": \"research_paper_a1b2c3d4\",\n",
    "                \"filename\": \"research_paper.pdf\",\n",
    "                \"pages\": 15,\n",
    "                \"chunks\": 32,\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Document uploaded and indexed successfully\",\n",
    "                \"file_size\": 2048576\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class DocumentInfo(BaseModel):\n",
    "    \"\"\"Document metadata information\"\"\"\n",
    "    document_id: str\n",
    "    filename: str\n",
    "    upload_date: Optional[datetime] = None\n",
    "    pages: int = Field(..., ge=0)\n",
    "    chunks: int = Field(..., ge=0)\n",
    "    file_size: int = Field(..., ge=0)\n",
    "    file_type: str\n",
    "    status: Optional[str] = \"completed\"\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"document_id\": \"research_paper_a1b2c3d4\",\n",
    "                \"filename\": \"research_paper.pdf\",\n",
    "                \"upload_date\": \"2025-01-15T10:30:00\",\n",
    "                \"pages\": 15,\n",
    "                \"chunks\": 32,\n",
    "                \"file_size\": 2048576,\n",
    "                \"file_type\": \".pdf\",\n",
    "                \"status\": \"completed\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class DocumentListResponse(BaseModel):\n",
    "    \"\"\"Response for listing documents\"\"\"\n",
    "    documents: List[DocumentInfo]\n",
    "    total_count: int = Field(..., ge=0)\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"documents\": [\n",
    "                    {\n",
    "                        \"document_id\": \"doc1\",\n",
    "                        \"filename\": \"file1.pdf\",\n",
    "                        \"pages\": 10,\n",
    "                        \"chunks\": 25,\n",
    "                        \"file_size\": 1024000,\n",
    "                        \"file_type\": \".pdf\"\n",
    "                    }\n",
    "                ],\n",
    "                \"total_count\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class DocumentDeleteResponse(BaseModel):\n",
    "    \"\"\"Response after deleting a document\"\"\"\n",
    "    document_id: str\n",
    "    message: str\n",
    "    chunks_deleted: int = Field(..., ge=0)\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUESTION ANSWERING MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class QuestionRequest(BaseModel):\n",
    "    \"\"\"Request model for asking questions\"\"\"\n",
    "    question: str = Field(\n",
    "        ..., \n",
    "        min_length=3, \n",
    "        max_length=500,\n",
    "        description=\"Question to ask about the documents\"\n",
    "    )\n",
    "    document_ids: Optional[List[str]] = Field(\n",
    "        None,\n",
    "        description=\"Optional list of document IDs to search. If None, searches all documents\"\n",
    "    )\n",
    "    top_k: int = Field(\n",
    "        default=4, \n",
    "        ge=1, \n",
    "        le=10,\n",
    "        description=\"Number of relevant chunks to retrieve\"\n",
    "    )\n",
    "    include_sources: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Whether to include source citations in response\"\n",
    "    )\n",
    "    temperature: Optional[float] = Field(\n",
    "        None,\n",
    "        ge=0.0,\n",
    "        le=2.0,\n",
    "        description=\"LLM temperature for response generation\"\n",
    "    )\n",
    "    \n",
    "    @validator('question')\n",
    "    def question_not_empty(cls, v):\n",
    "        \"\"\"Validate question is not empty or whitespace\"\"\"\n",
    "        if not v or not v.strip():\n",
    "            raise ValueError('Question cannot be empty')\n",
    "        return v.strip()\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"question\": \"What are the main findings of this research?\",\n",
    "                \"document_ids\": [\"research_paper_a1b2c3d4\"],\n",
    "                \"top_k\": 4,\n",
    "                \"include_sources\": True,\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class Source(BaseModel):\n",
    "    \"\"\"Source citation information\"\"\"\n",
    "    document_id: str = Field(..., description=\"Document identifier\")\n",
    "    document_name: str = Field(..., description=\"Document filename\")\n",
    "    page_number: Optional[int] = Field(None, ge=1, description=\"Page number in document\")\n",
    "    chunk_text: str = Field(..., description=\"Relevant text excerpt\")\n",
    "    relevance_score: float = Field(..., ge=0.0, le=1.0, description=\"Relevance score (0-1)\")\n",
    "    chunk_id: Optional[int] = Field(None, description=\"Chunk identifier within document\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"document_id\": \"research_paper_a1b2c3d4\",\n",
    "                \"document_name\": \"research_paper.pdf\",\n",
    "                \"page_number\": 5,\n",
    "                \"chunk_text\": \"The study found that...\",\n",
    "                \"relevance_score\": 0.87,\n",
    "                \"chunk_id\": 12\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class QuestionResponse(BaseModel):\n",
    "    \"\"\"Response model for question answering\"\"\"\n",
    "    answer: str = Field(..., description=\"Generated answer to the question\")\n",
    "    sources: List[Source] = Field(default_factory=list, description=\"Source citations\")\n",
    "    question: str = Field(..., description=\"Original question asked\")\n",
    "    model_used: str = Field(..., description=\"Model used for generation\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "    processing_time: float = Field(..., ge=0.0, description=\"Processing time in seconds\")\n",
    "    tokens_used: Optional[int] = Field(None, description=\"Number of tokens used (if available)\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"answer\": \"Based on the research, the main findings are...\",\n",
    "                \"sources\": [\n",
    "                    {\n",
    "                        \"document_id\": \"doc1\",\n",
    "                        \"document_name\": \"research.pdf\",\n",
    "                        \"page_number\": 5,\n",
    "                        \"chunk_text\": \"The study found...\",\n",
    "                        \"relevance_score\": 0.87\n",
    "                    }\n",
    "                ],\n",
    "                \"question\": \"What are the main findings?\",\n",
    "                \"model_used\": \"gpt-3.5-turbo\",\n",
    "                \"processing_time\": 2.34,\n",
    "                \"tokens_used\": 450\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONVERSATION MODELS (Optional - for chat history)\n",
    "# ============================================================================\n",
    "\n",
    "class ConversationMessage(BaseModel):\n",
    "    \"\"\"Single message in a conversation\"\"\"\n",
    "    role: str = Field(..., description=\"Message role: 'user' or 'assistant'\")\n",
    "    content: str = Field(..., description=\"Message content\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "    sources: Optional[List[Source]] = Field(None, description=\"Sources for assistant messages\")\n",
    "    \n",
    "    @validator('role')\n",
    "    def validate_role(cls, v):\n",
    "        \"\"\"Validate role is either user or assistant\"\"\"\n",
    "        if v not in ['user', 'assistant', 'system']:\n",
    "            raise ValueError('Role must be \"user\", \"assistant\", or \"system\"')\n",
    "        return v\n",
    "\n",
    "\n",
    "class ConversationHistory(BaseModel):\n",
    "    \"\"\"Conversation history\"\"\"\n",
    "    session_id: str = Field(..., description=\"Unique session identifier\")\n",
    "    messages: List[ConversationMessage] = Field(default_factory=list)\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    updated_at: datetime = Field(default_factory=datetime.now)\n",
    "    document_ids: Optional[List[str]] = Field(None, description=\"Documents in this conversation\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response\"\"\"\n",
    "    status: str = Field(..., description=\"System status: 'healthy' or 'unhealthy'\")\n",
    "    version: str = Field(..., description=\"Application version\")\n",
    "    vector_store_status: str = Field(..., description=\"Vector store connection status\")\n",
    "    documents_indexed: int = Field(..., ge=0, description=\"Number of documents in index\")\n",
    "    uptime_seconds: Optional[float] = Field(None, description=\"System uptime in seconds\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"status\": \"healthy\",\n",
    "                \"version\": \"1.0.0\",\n",
    "                \"vector_store_status\": \"connected\",\n",
    "                \"documents_indexed\": 42,\n",
    "                \"uptime_seconds\": 3600.5\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class ErrorResponse(BaseModel):\n",
    "    \"\"\"Error response model\"\"\"\n",
    "    error: str = Field(..., description=\"Error type or category\")\n",
    "    detail: Optional[str] = Field(None, description=\"Detailed error message\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "    request_id: Optional[str] = Field(None, description=\"Request identifier for tracking\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"error\": \"ValidationError\",\n",
    "                \"detail\": \"Question must be at least 3 characters long\",\n",
    "                \"timestamp\": \"2025-01-15T10:30:00\",\n",
    "                \"request_id\": \"req_123abc\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SEARCH & RETRIEVAL MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    \"\"\"Request for semantic search without answer generation\"\"\"\n",
    "    query: str = Field(..., min_length=3, max_length=500)\n",
    "    document_ids: Optional[List[str]] = None\n",
    "    top_k: int = Field(default=10, ge=1, le=50)\n",
    "    similarity_threshold: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    \"\"\"Single search result\"\"\"\n",
    "    document_id: str\n",
    "    document_name: str\n",
    "    chunk_text: str\n",
    "    page_number: Optional[int] = None\n",
    "    similarity_score: float = Field(..., ge=0.0, le=1.0)\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class SearchResponse(BaseModel):\n",
    "    \"\"\"Response for search requests\"\"\"\n",
    "    results: List[SearchResult]\n",
    "    query: str\n",
    "    total_results: int\n",
    "    processing_time: float\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICS & ANALYTICS MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentStatistics(BaseModel):\n",
    "    \"\"\"Statistics for a single document\"\"\"\n",
    "    document_id: str\n",
    "    filename: str\n",
    "    total_queries: int = Field(default=0, ge=0)\n",
    "    average_relevance_score: Optional[float] = None\n",
    "    last_accessed: Optional[datetime] = None\n",
    "\n",
    "\n",
    "class SystemStatistics(BaseModel):\n",
    "    \"\"\"Overall system statistics\"\"\"\n",
    "    total_documents: int = Field(..., ge=0)\n",
    "    total_chunks: int = Field(..., ge=0)\n",
    "    total_queries: int = Field(..., ge=0)\n",
    "    average_query_time: Optional[float] = None\n",
    "    average_answer_length: Optional[int] = None\n",
    "    most_queried_documents: Optional[List[str]] = None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH OPERATIONS MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class BatchQuestionRequest(BaseModel):\n",
    "    \"\"\"Request for batch question processing\"\"\"\n",
    "    questions: List[str] = Field(..., min_items=1, max_items=50)\n",
    "    document_ids: Optional[List[str]] = None\n",
    "    top_k: int = Field(default=4, ge=1, le=10)\n",
    "\n",
    "\n",
    "class BatchQuestionResponse(BaseModel):\n",
    "    \"\"\"Response for batch questions\"\"\"\n",
    "    results: List[QuestionResponse]\n",
    "    total_questions: int\n",
    "    successful: int\n",
    "    failed: int\n",
    "    total_processing_time: float\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FEEDBACK MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class FeedbackRequest(BaseModel):\n",
    "    \"\"\"User feedback on an answer\"\"\"\n",
    "    question_id: Optional[str] = Field(None, description=\"Identifier of the question\")\n",
    "    rating: int = Field(..., ge=1, le=5, description=\"Rating from 1-5\")\n",
    "    comment: Optional[str] = Field(None, max_length=1000, description=\"Optional feedback comment\")\n",
    "    helpful: Optional[bool] = Field(None, description=\"Was the answer helpful?\")\n",
    "    accurate: Optional[bool] = Field(None, description=\"Was the answer accurate?\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"question_id\": \"q_123abc\",\n",
    "                \"rating\": 5,\n",
    "                \"comment\": \"Very helpful and accurate answer with good sources\",\n",
    "                \"helpful\": True,\n",
    "                \"accurate\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class FeedbackResponse(BaseModel):\n",
    "    \"\"\"Response after submitting feedback\"\"\"\n",
    "    message: str\n",
    "    feedback_id: str\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class ExportFormat(str, Enum):\n",
    "    \"\"\"Supported export formats\"\"\"\n",
    "    PDF = \"pdf\"\n",
    "    MARKDOWN = \"markdown\"\n",
    "    JSON = \"json\"\n",
    "    TXT = \"txt\"\n",
    "\n",
    "\n",
    "class ExportRequest(BaseModel):\n",
    "    \"\"\"Request to export conversation or results\"\"\"\n",
    "    session_id: Optional[str] = None\n",
    "    format: ExportFormat = Field(default=ExportFormat.PDF)\n",
    "    include_sources: bool = Field(default=True)\n",
    "    include_metadata: bool = Field(default=False)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class ChunkingConfig(BaseModel):\n",
    "    \"\"\"Configuration for document chunking\"\"\"\n",
    "    chunk_size: int = Field(default=1000, ge=100, le=5000)\n",
    "    chunk_overlap: int = Field(default=200, ge=0, le=1000)\n",
    "    separators: List[str] = Field(default=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "\n",
    "\n",
    "class RetrievalConfig(BaseModel):\n",
    "    \"\"\"Configuration for retrieval\"\"\"\n",
    "    top_k: int = Field(default=4, ge=1, le=20)\n",
    "    similarity_threshold: float = Field(default=0.7, ge=0.0, le=1.0)\n",
    "    reranking_enabled: bool = Field(default=False)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class SuccessResponse(BaseModel):\n",
    "    \"\"\"Generic success response\"\"\"\n",
    "    success: bool = True\n",
    "    message: str\n",
    "    data: Optional[Dict[str, Any]] = None\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "\n",
    "\n",
    "class PaginationParams(BaseModel):\n",
    "    \"\"\"Pagination parameters\"\"\"\n",
    "    page: int = Field(default=1, ge=1)\n",
    "    page_size: int = Field(default=10, ge=1, le=100)\n",
    "\n",
    "\n",
    "class PaginatedResponse(BaseModel):\n",
    "    \"\"\"Paginated response wrapper\"\"\"\n",
    "    items: List[Any]\n",
    "    total: int\n",
    "    page: int\n",
    "    page_size: int\n",
    "    total_pages: int\n",
    "'''\n",
    "# Save file \n",
    "with open(\"backend/app/models.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bed4602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\"\"\n",
      "Pydantic models for request/response validation\n",
      "RAG Document Assistant - Complete Models File\n",
      "\"\"\"\n",
      "from pydantic import BaseModel, Field, validator\n",
      "from typing import List, Optional, Dict, Any\n",
      "from datetime import datetime\n",
      "from enum import Enum\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# ENUMS\n",
      "# ============================================================================\n",
      "\n",
      "class DocumentStatus(str, Enum):\n",
      "    \"\"\"Document processing status\"\"\"\n",
      "    PENDING = \"pending\"\n",
      "    PROCESSING = \"processing\"\n",
      "    COMPLETED = \"completed\"\n",
      "    FAILED = \"failed\"\n",
      "\n",
      "\n",
      "class FileType(str, Enum):\n",
      "    \"\"\"Supported file types\"\"\"\n",
      "    PDF = \".pdf\"\n",
      "    DOCX = \".docx\"\n",
      "    TXT = \".txt\"\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# DOCUMENT MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class DocumentUploadResponse(BaseModel):\n",
      "    \"\"\"Response after uploading a document\"\"\"\n",
      "    document_id: str = Field(..., description=\"Unique document identifier\")\n",
      "    filename: str = Field(..., description=\"Original filename\")\n",
      "    pages: int = Field(..., ge=0, description=\"Number of pages\")\n",
      "    chunks: int = Field(..., ge=0, description=\"Number of text chunks created\")\n",
      "    status: str = Field(..., description=\"Processing status\")\n",
      "    message: str = Field(..., description=\"Status message\")\n",
      "    timestamp: datetime = Field(default_factory=datetime.now)\n",
      "    file_size: Optional[int] = Field(None, description=\"File size in bytes\")\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"document_id\": \"research_paper_a1b2c3d4\",\n",
      "                \"filename\": \"research_paper.pdf\",\n",
      "                \"pages\": 15,\n",
      "                \"chunks\": 32,\n",
      "                \"status\": \"success\",\n",
      "                \"message\": \"Document uploaded and indexed successfully\",\n",
      "                \"file_size\": 2048576\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "class DocumentInfo(BaseModel):\n",
      "    \"\"\"Document metadata information\"\"\"\n",
      "    document_id: str\n",
      "    filename: str\n",
      "    upload_date: Optional[datetime] = None\n",
      "    pages: int = Field(..., ge=0)\n",
      "    chunks: int = Field(..., ge=0)\n",
      "    file_size: int = Field(..., ge=0)\n",
      "    file_type: str\n",
      "    status: Optional[str] = \"completed\"\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"document_id\": \"research_paper_a1b2c3d4\",\n",
      "                \"filename\": \"research_paper.pdf\",\n",
      "                \"upload_date\": \"2025-01-15T10:30:00\",\n",
      "                \"pages\": 15,\n",
      "                \"chunks\": 32,\n",
      "                \"file_size\": 2048576,\n",
      "                \"file_type\": \".pdf\",\n",
      "                \"status\": \"completed\"\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "class DocumentListResponse(BaseModel):\n",
      "    \"\"\"Response for listing documents\"\"\"\n",
      "    documents: List[DocumentInfo]\n",
      "    total_count: int = Field(..., ge=0)\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"documents\": [\n",
      "                    {\n",
      "                        \"document_id\": \"doc1\",\n",
      "                        \"filename\": \"file1.pdf\",\n",
      "                        \"pages\": 10,\n",
      "                        \"chunks\": 25,\n",
      "                        \"file_size\": 1024000,\n",
      "                        \"file_type\": \".pdf\"\n",
      "                    }\n",
      "                ],\n",
      "                \"total_count\": 1\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "class DocumentDeleteResponse(BaseModel):\n",
      "    \"\"\"Response after deleting a document\"\"\"\n",
      "    document_id: str\n",
      "    message: str\n",
      "    chunks_deleted: int = Field(..., ge=0)\n",
      "    timestamp: datetime = Field(default_factory=datetime.now)\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# QUESTION ANSWERING MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class QuestionRequest(BaseModel):\n",
      "    \"\"\"Request model for asking questions\"\"\"\n",
      "    question: str = Field(\n",
      "        ..., \n",
      "        min_length=3, \n",
      "        max_length=500,\n",
      "        description=\"Question to ask about the documents\"\n",
      "    )\n",
      "    document_ids: Optional[List[str]] = Field(\n",
      "        None,\n",
      "        description=\"Optional list of document IDs to search. If None, searches all documents\"\n",
      "    )\n",
      "    top_k: int = Field(\n",
      "        default=4, \n",
      "        ge=1, \n",
      "        le=10,\n",
      "        description=\"Number of relevant chunks to retrieve\"\n",
      "    )\n",
      "    include_sources: bool = Field(\n",
      "        default=True,\n",
      "        description=\"Whether to include source citations in response\"\n",
      "    )\n",
      "    temperature: Optional[float] = Field(\n",
      "        None,\n",
      "        ge=0.0,\n",
      "        le=2.0,\n",
      "        description=\"LLM temperature for response generation\"\n",
      "    )\n",
      "\n",
      "    @validator('question')\n",
      "    def question_not_empty(cls, v):\n",
      "        \"\"\"Validate question is not empty or whitespace\"\"\"\n",
      "        if not v or not v.strip():\n",
      "            raise ValueError('Question cannot be empty')\n",
      "        return v.strip()\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"question\": \"What are the main findings of this research?\",\n",
      "                \"document_ids\": [\"research_paper_a1b2c3d4\"],\n",
      "                \"top_k\": 4,\n",
      "                \"include_sources\": True,\n",
      "                \"temperature\": 0.7\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "class Source(BaseModel):\n",
      "    \"\"\"Source citation information\"\"\"\n",
      "    document_id: str = Field(..., description=\"Document identifier\")\n",
      "    document_name: str = Field(..., description=\"Document filename\")\n",
      "    page_number: Optional[int] = Field(None, ge=1, description=\"Page number in document\")\n",
      "    chunk_text: str = Field(..., description=\"Relevant text excerpt\")\n",
      "    relevance_score: float = Field(..., ge=0.0, le=1.0, description=\"Relevance score (0-1)\")\n",
      "    chunk_id: Optional[int] = Field(None, description=\"Chunk identifier within document\")\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"document_id\": \"research_paper_a1b2c3d4\",\n",
      "                \"document_name\": \"research_paper.pdf\",\n",
      "                \"page_number\": 5,\n",
      "                \"chunk_text\": \"The study found that...\",\n",
      "                \"relevance_score\": 0.87,\n",
      "                \"chunk_id\": 12\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "class QuestionResponse(BaseModel):\n",
      "    \"\"\"Response model for question answering\"\"\"\n",
      "    answer: str = Field(..., description=\"Generated answer to the question\")\n",
      "    sources: List[Source] = Field(default_factory=list, description=\"Source citations\")\n",
      "    question: str = Field(..., description=\"Original question asked\")\n",
      "    model_used: str = Field(..., description=\"Model used for generation\")\n",
      "    timestamp: datetime = Field(default_factory=datetime.now)\n",
      "    processing_time: float = Field(..., ge=0.0, description=\"Processing time in seconds\")\n",
      "    tokens_used: Optional[int] = Field(None, description=\"Number of tokens used (if available)\")\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"answer\": \"Based on the research, the main findings are...\",\n",
      "                \"sources\": [\n",
      "                    {\n",
      "                        \"document_id\": \"doc1\",\n",
      "                        \"document_name\": \"research.pdf\",\n",
      "                        \"page_number\": 5,\n",
      "                        \"chunk_text\": \"The study found...\",\n",
      "                        \"relevance_score\": 0.87\n",
      "                    }\n",
      "                ],\n",
      "                \"question\": \"What are the main findings?\",\n",
      "                \"model_used\": \"gpt-3.5-turbo\",\n",
      "                \"processing_time\": 2.34,\n",
      "                \"tokens_used\": 450\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# CONVERSATION MODELS (Optional - for chat history)\n",
      "# ============================================================================\n",
      "\n",
      "class ConversationMessage(BaseModel):\n",
      "    \"\"\"Single message in a conversation\"\"\"\n",
      "    role: str = Field(..., description=\"Message role: 'user' or 'assistant'\")\n",
      "    content: str = Field(..., description=\"Message content\")\n",
      "    timestamp: datetime = Field(default_factory=datetime.now)\n",
      "    sources: Optional[List[Source]] = Field(None, description=\"Sources for assistant messages\")\n",
      "\n",
      "    @validator('role')\n",
      "    def validate_role(cls, v):\n",
      "        \"\"\"Validate role is either user or assistant\"\"\"\n",
      "        if v not in ['user', 'assistant', 'system']:\n",
      "            raise ValueError('Role must be \"user\", \"assistant\", or \"system\"')\n",
      "        return v\n",
      "\n",
      "\n",
      "class ConversationHistory(BaseModel):\n",
      "    \"\"\"Conversation history\"\"\"\n",
      "    session_id: str = Field(..., description=\"Unique session identifier\")\n",
      "    messages: List[ConversationMessage] = Field(default_factory=list)\n",
      "    created_at: datetime = Field(default_factory=datetime.now)\n",
      "    updated_at: datetime = Field(default_factory=datetime.now)\n",
      "    document_ids: Optional[List[str]] = Field(None, description=\"Documents in this conversation\")\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# SYSTEM MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class HealthResponse(BaseModel):\n",
      "    \"\"\"Health check response\"\"\"\n",
      "    status: str = Field(..., description=\"System status: 'healthy' or 'unhealthy'\")\n",
      "    version: str = Field(..., description=\"Application version\")\n",
      "    vector_store_status: str = Field(..., description=\"Vector store connection status\")\n",
      "    documents_indexed: int = Field(..., ge=0, description=\"Number of documents in index\")\n",
      "    uptime_seconds: Optional[float] = Field(None, description=\"System uptime in seconds\")\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"status\": \"healthy\",\n",
      "                \"version\": \"1.0.0\",\n",
      "                \"vector_store_status\": \"connected\",\n",
      "                \"documents_indexed\": 42,\n",
      "                \"uptime_seconds\": 3600.5\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "class ErrorResponse(BaseModel):\n",
      "    \"\"\"Error response model\"\"\"\n",
      "    error: str = Field(..., description=\"Error type or category\")\n",
      "    detail: Optional[str] = Field(None, description=\"Detailed error message\")\n",
      "    timestamp: datetime = Field(default_factory=datetime.now)\n",
      "    request_id: Optional[str] = Field(None, description=\"Request identifier for tracking\")\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"error\": \"ValidationError\",\n",
      "                \"detail\": \"Question must be at least 3 characters long\",\n",
      "                \"timestamp\": \"2025-01-15T10:30:00\",\n",
      "                \"request_id\": \"req_123abc\"\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# SEARCH & RETRIEVAL MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class SearchRequest(BaseModel):\n",
      "    \"\"\"Request for semantic search without answer generation\"\"\"\n",
      "    query: str = Field(..., min_length=3, max_length=500)\n",
      "    document_ids: Optional[List[str]] = None\n",
      "    top_k: int = Field(default=10, ge=1, le=50)\n",
      "    similarity_threshold: Optional[float] = Field(None, ge=0.0, le=1.0)\n",
      "\n",
      "\n",
      "class SearchResult(BaseModel):\n",
      "    \"\"\"Single search result\"\"\"\n",
      "    document_id: str\n",
      "    document_name: str\n",
      "    chunk_text: str\n",
      "    page_number: Optional[int] = None\n",
      "    similarity_score: float = Field(..., ge=0.0, le=1.0)\n",
      "    metadata: Optional[Dict[str, Any]] = None\n",
      "\n",
      "\n",
      "class SearchResponse(BaseModel):\n",
      "    \"\"\"Response for search requests\"\"\"\n",
      "    results: List[SearchResult]\n",
      "    query: str\n",
      "    total_results: int\n",
      "    processing_time: float\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# STATISTICS & ANALYTICS MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class DocumentStatistics(BaseModel):\n",
      "    \"\"\"Statistics for a single document\"\"\"\n",
      "    document_id: str\n",
      "    filename: str\n",
      "    total_queries: int = Field(default=0, ge=0)\n",
      "    average_relevance_score: Optional[float] = None\n",
      "    last_accessed: Optional[datetime] = None\n",
      "\n",
      "\n",
      "class SystemStatistics(BaseModel):\n",
      "    \"\"\"Overall system statistics\"\"\"\n",
      "    total_documents: int = Field(..., ge=0)\n",
      "    total_chunks: int = Field(..., ge=0)\n",
      "    total_queries: int = Field(..., ge=0)\n",
      "    average_query_time: Optional[float] = None\n",
      "    average_answer_length: Optional[int] = None\n",
      "    most_queried_documents: Optional[List[str]] = None\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# BATCH OPERATIONS MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class BatchQuestionRequest(BaseModel):\n",
      "    \"\"\"Request for batch question processing\"\"\"\n",
      "    questions: List[str] = Field(..., min_items=1, max_items=50)\n",
      "    document_ids: Optional[List[str]] = None\n",
      "    top_k: int = Field(default=4, ge=1, le=10)\n",
      "\n",
      "\n",
      "class BatchQuestionResponse(BaseModel):\n",
      "    \"\"\"Response for batch questions\"\"\"\n",
      "    results: List[QuestionResponse]\n",
      "    total_questions: int\n",
      "    successful: int\n",
      "    failed: int\n",
      "    total_processing_time: float\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# FEEDBACK MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class FeedbackRequest(BaseModel):\n",
      "    \"\"\"User feedback on an answer\"\"\"\n",
      "    question_id: Optional[str] = Field(None, description=\"Identifier of the question\")\n",
      "    rating: int = Field(..., ge=1, le=5, description=\"Rating from 1-5\")\n",
      "    comment: Optional[str] = Field(None, max_length=1000, description=\"Optional feedback comment\")\n",
      "    helpful: Optional[bool] = Field(None, description=\"Was the answer helpful?\")\n",
      "    accurate: Optional[bool] = Field(None, description=\"Was the answer accurate?\")\n",
      "\n",
      "    class Config:\n",
      "        json_schema_extra = {\n",
      "            \"example\": {\n",
      "                \"question_id\": \"q_123abc\",\n",
      "                \"rating\": 5,\n",
      "                \"comment\": \"Very helpful and accurate answer with good sources\",\n",
      "                \"helpful\": True,\n",
      "                \"accurate\": True\n",
      "            }\n",
      "        }\n",
      "\n",
      "\n",
      "class FeedbackResponse(BaseModel):\n",
      "    \"\"\"Response after submitting feedback\"\"\"\n",
      "    message: str\n",
      "    feedback_id: str\n",
      "    timestamp: datetime = Field(default_factory=datetime.now)\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# EXPORT MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class ExportFormat(str, Enum):\n",
      "    \"\"\"Supported export formats\"\"\"\n",
      "    PDF = \"pdf\"\n",
      "    MARKDOWN = \"markdown\"\n",
      "    JSON = \"json\"\n",
      "    TXT = \"txt\"\n",
      "\n",
      "\n",
      "class ExportRequest(BaseModel):\n",
      "    \"\"\"Request to export conversation or results\"\"\"\n",
      "    session_id: Optional[str] = None\n",
      "    format: ExportFormat = Field(default=ExportFormat.PDF)\n",
      "    include_sources: bool = Field(default=True)\n",
      "    include_metadata: bool = Field(default=False)\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# CONFIGURATION MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class ChunkingConfig(BaseModel):\n",
      "    \"\"\"Configuration for document chunking\"\"\"\n",
      "    chunk_size: int = Field(default=1000, ge=100, le=5000)\n",
      "    chunk_overlap: int = Field(default=200, ge=0, le=1000)\n",
      "    separators: List[str] = Field(default=[\"\n",
      "\n",
      "\", \"\n",
      "\", \". \", \" \", \"\"])\n",
      "\n",
      "\n",
      "class RetrievalConfig(BaseModel):\n",
      "    \"\"\"Configuration for retrieval\"\"\"\n",
      "    top_k: int = Field(default=4, ge=1, le=20)\n",
      "    similarity_threshold: float = Field(default=0.7, ge=0.0, le=1.0)\n",
      "    reranking_enabled: bool = Field(default=False)\n",
      "\n",
      "\n",
      "# ============================================================================\n",
      "# UTILITY MODELS\n",
      "# ============================================================================\n",
      "\n",
      "class SuccessResponse(BaseModel):\n",
      "    \"\"\"Generic success response\"\"\"\n",
      "    success: bool = True\n",
      "    message: str\n",
      "    data: Optional[Dict[str, Any]] = None\n",
      "    timestamp: datetime = Field(default_factory=datetime.now)\n",
      "\n",
      "\n",
      "class PaginationParams(BaseModel):\n",
      "    \"\"\"Pagination parameters\"\"\"\n",
      "    page: int = Field(default=1, ge=1)\n",
      "    page_size: int = Field(default=10, ge=1, le=100)\n",
      "\n",
      "\n",
      "class PaginatedResponse(BaseModel):\n",
      "    \"\"\"Paginated response wrapper\"\"\"\n",
      "    items: List[Any]\n",
      "    total: int\n",
      "    page: int\n",
      "    page_size: int\n",
      "    total_pages: int\n"
     ]
    }
   ],
   "source": [
    "!type backend\\app\\models.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
